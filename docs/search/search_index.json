{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Monitoring Hub","text":"<p>The definitive Software Factory for Prometheus Exporters.</p> <p> </p>"},{"location":"#project-goal","title":"\ud83c\udfaf Project Goal","text":"<p>Monitoring Hub is an automated Software Factory that transforms simple YAML manifests into production-ready monitoring tools. It focuses on Enterprise Standards, Multi-Architecture support, and Full Automation.</p>"},{"location":"#key-features","title":"\ud83d\ude80 Key Features","text":"<ul> <li>Native Multi-Arch: Every tool is built for <code>x86_64</code> and <code>aarch64</code> (ARM64)</li> <li>Multi-Format Packages: RPM (RHEL/CentOS/Rocky/Alma), DEB (Ubuntu/Debian), and OCI containers</li> <li>GPG-Signed Packages: All RPM and DEB packages are cryptographically signed for integrity</li> <li>Hardened Security: All Docker images use Red Hat UBI 9 Minimal with Trivy scanning</li> <li>Linux Standard (FHS): Packages include system users, standard paths, and systemd integration</li> <li>Zero-Click Updates: Automated watcher opens PRs and merges when tests pass</li> <li>Always Up-to-Date: Never worry about upstream releases again</li> </ul>"},{"location":"#whats-included","title":"\ud83d\udce6 What's Included","text":"<p>The factory currently builds and maintains 30+ Prometheus exporters, including:</p> <ul> <li>System Monitoring: node_exporter, process_exporter, systemd_exporter</li> <li>Database: postgres_exporter, mysql_exporter, mongodb_exporter, redis_exporter</li> <li>Web Services: nginx_exporter, apache_exporter, blackbox_exporter</li> <li>Storage: ceph_exporter, ipmi_exporter, smartctl_exporter</li> <li>Messaging: kafka_exporter, rabbitmq_exporter, nats_exporter</li> <li>And many more...</li> </ul> <p>Browse the complete catalog \u2192</p>"},{"location":"#quick-links","title":"\ud83d\udcda Quick Links","text":"<ul> <li> <p> Quick Start</p> <p>Get up and running in minutes</p> </li> <li> <p> User Guide</p> <p>Learn how to add new exporters</p> </li> <li> <p> Architecture</p> <p>Understand how it works</p> </li> <li> <p> API Reference</p> <p>Explore the codebase</p> </li> </ul>"},{"location":"#how-it-works","title":"\ud83c\udfed How It Works","text":"<pre><code>graph LR\n    A[YAML Manifest] --&gt; B[Builder]\n    B --&gt; C[RPM Packages]\n    B --&gt; D[Docker Images]\n    C --&gt; E[YUM Repository]\n    D --&gt; F[GHCR Registry]\n    E --&gt; G[Production]\n    F --&gt; G</code></pre> <ol> <li>Define: Create a simple YAML manifest describing the exporter</li> <li>Build: Automated CI builds RPM and Docker artifacts</li> <li>Distribute: Packages are published to YUM repo and GHCR</li> <li>Update: Watcher monitors upstream and auto-updates</li> </ol>"},{"location":"#distribution","title":"\ud83d\udccb Distribution","text":""},{"location":"#yum-repository-rpm","title":"YUM Repository (RPM)","text":"<pre><code># Add GPG key\nsudo rpm --import https://sckyzo.github.io/monitoring-hub/RPM-GPG-KEY-monitoring-hub\n\n# Configure repository\nsudo dnf config-manager --add-repo https://sckyzo.github.io/monitoring-hub/el9/$(arch)/\n\n# Install any exporter\nsudo dnf install node_exporter\n\n# Enable and start\nsudo systemctl enable --now node_exporter\n</code></pre>"},{"location":"#apt-repository-deb","title":"APT Repository (DEB)","text":"<pre><code># Add GPG key\ncurl -fsSL https://sckyzo.github.io/monitoring-hub/apt/monitoring-hub.asc | \\\n  sudo gpg --dearmor -o /usr/share/keyrings/monitoring-hub.gpg\n\n# Add repository (Ubuntu 22.04 \"jammy\" example)\necho \"deb [signed-by=/usr/share/keyrings/monitoring-hub.gpg] \\\n  https://sckyzo.github.io/monitoring-hub/apt jammy main\" | \\\n  sudo tee /etc/apt/sources.list.d/monitoring-hub.list\n\n# Install any exporter\nsudo apt update &amp;&amp; sudo apt install node-exporter\n\n# Enable and start\nsudo systemctl enable --now node_exporter\n</code></pre>"},{"location":"#container-registry-docker","title":"Container Registry (Docker)","text":"<pre><code># Pull any exporter\ndocker pull ghcr.io/sckyzo/monitoring-hub/node_exporter:latest\n\n# Run\ndocker run -d -p 9100:9100 ghcr.io/sckyzo/monitoring-hub/node_exporter:latest\n</code></pre>"},{"location":"#contributing","title":"\ud83e\udd1d Contributing","text":"<p>We welcome contributions! Check out our Contributing Guide to get started.</p>"},{"location":"#license","title":"\ud83d\udcc4 License","text":"<p>Distributed under the MIT License. See LICENSE for more information.</p>"},{"location":"api-reference/builder/","title":"Builder API","text":""},{"location":"api-reference/builder/#core.engine.builder","title":"<code>core.engine.builder</code>","text":""},{"location":"api-reference/builder/#core.engine.builder-functions","title":"Functions","text":""},{"location":"api-reference/builder/#core.engine.builder.copy_local_binary","title":"<code>copy_local_binary(data, output_dir, manifest_dir)</code>","text":"<p>Copy local binary or archive to output directory. Mirrors the \"Smart Copy Logic\" pattern used for extra_files.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <p>Manifest data</p> required <code>output_dir</code> <p>Build output directory</p> required <code>manifest_dir</code> <p>Directory containing the manifest (for relative paths)</p> required Source code in <code>core/engine/builder.py</code> <pre><code>def copy_local_binary(data, output_dir, manifest_dir):\n    \"\"\"\n    Copy local binary or archive to output directory.\n    Mirrors the \"Smart Copy Logic\" pattern used for extra_files.\n\n    Args:\n        data: Manifest data\n        output_dir: Build output directory\n        manifest_dir: Directory containing the manifest (for relative paths)\n    \"\"\"\n    binary_name = data[\"build\"][\"binary_name\"]\n    local_binary = data[\"upstream\"].get(\"local_binary\")\n    local_archive = data[\"upstream\"].get(\"local_archive\")\n    binaries_to_find = [binary_name] + data[\"build\"].get(\"extra_binaries\", [])\n    found_binaries = []\n\n    if local_binary:\n        # Case 1: Direct binary file\n        source_path = os.path.join(manifest_dir, local_binary)\n\n        if not os.path.exists(source_path):\n            abs_path = os.path.abspath(source_path)\n            click.echo(\n                f\"Error: Local binary not found: {source_path}\",\n                err=True,\n            )\n            click.echo(f\"Absolute path checked: {abs_path}\", err=True)\n            click.echo(\n                \"Hint: Verify the 'upstream.local_binary' path in your manifest.yaml is relative to the exporter directory.\",\n                err=True,\n            )\n            raise click.Abort()\n\n        if not os.path.isfile(source_path):\n            click.echo(\n                f\"Error: Path is not a file: {source_path} (is it a directory?)\",\n                err=True,\n            )\n            raise click.Abort()\n\n        click.echo(f\"Copying local binary: {source_path}\")\n        dest_path = os.path.join(output_dir, binary_name)\n        shutil.copy(source_path, dest_path)\n        os.chmod(dest_path, 0o755)  # nosec B103 - Executable binary requires execute permissions\n        found_binaries.append(binary_name)\n        click.echo(f\"Binary ready: {dest_path}\")\n\n    elif local_archive:\n        # Case 2: Archive (.tar.gz or .gz)\n        source_path = os.path.join(manifest_dir, local_archive)\n\n        if not os.path.exists(source_path):\n            click.echo(f\"Error: Local archive not found: {source_path}\", err=True)\n            raise click.Abort()\n\n        click.echo(f\"Extracting local archive: {source_path}\")\n\n        if source_path.endswith(\".gz\") and not source_path.endswith(\".tar.gz\"):\n            # Simple .gz file (single binary)\n            click.echo(\"Decompressing single binary...\")\n            final_path = os.path.join(output_dir, binary_name)\n            with gzip.open(source_path, \"rb\") as f_in, open(final_path, \"wb\") as f_out:\n                shutil.copyfileobj(f_in, f_out)\n            os.chmod(final_path, 0o755)  # nosec B103 - Executable binary requires execute permissions\n            found_binaries.append(binary_name)\n            click.echo(f\"Binary ready: {final_path}\")\n\n        elif source_path.endswith(\".tar.gz\"):\n            # .tar.gz archive - reuse existing logic from download_and_extract\n            click.echo(f\"Extracting binaries {binaries_to_find}...\")\n            extracted_dirs = set()\n            with tarfile.open(source_path, \"r:gz\") as tar:\n                members = tar.getmembers()\n                for b_name in binaries_to_find:\n                    member_to_extract = None\n                    for member in members:\n                        if member.name.endswith(f\"/{b_name}\") or member.name == b_name:\n                            member_to_extract = member\n                            break\n\n                    if member_to_extract:\n                        tar.extract(member_to_extract, path=output_dir, filter=\"data\")\n                        extracted_path = os.path.join(\n                            output_dir, member_to_extract.name\n                        )\n                        final_path = os.path.join(output_dir, b_name)\n\n                        if extracted_path != final_path:\n                            shutil.move(extracted_path, final_path)\n\n                        parts = member_to_extract.name.split(\"/\")\n                        if len(parts) &gt; 1:\n                            extracted_dirs.add(parts[0])\n\n                        os.chmod(final_path, 0o755)  # nosec B103 - Executable binary requires execute permissions\n                        found_binaries.append(b_name)\n                        click.echo(f\"Binary ready: {final_path}\")\n                    else:\n                        click.echo(f\"Warning: Binary '{b_name}' not found in archive.\")\n\n            # Cleanup extracted directories\n            for d in extracted_dirs:\n                dir_to_remove = os.path.join(output_dir, d)\n                if os.path.isdir(dir_to_remove):\n                    shutil.rmtree(dir_to_remove, ignore_errors=True)\n        else:\n            click.echo(f\"Error: Unsupported archive format: {source_path}\", err=True)\n            raise click.Abort()\n\n    if not found_binaries:\n        click.echo(\"Error: No binaries found.\", err=True)\n        raise click.Abort()\n</code></pre>"},{"location":"api-reference/builder/#core.engine.builder.download_and_extract","title":"<code>download_and_extract(data, output_dir, arch)</code>","text":"<p>Downloads the upstream binary release and extracts it.</p> <p>This function handles the complexity of GitHub release naming conventions: 1. Some projects use 'v' prefixes in tags but not in filenames. 2. Some projects use dashes, others dots. 3. We support custom 'archive_name' patterns to handle any edge case. 4. Supports .tar.gz archives and simple .gz compressed binaries.</p> Source code in <code>core/engine/builder.py</code> <pre><code>@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=2, max=10),\n    retry=retry_if_exception_type((requests.exceptions.RequestException, OSError)),\n    reraise=True,\n)\ndef download_and_extract(data, output_dir, arch):\n    \"\"\"\n    Downloads the upstream binary release and extracts it.\n\n    This function handles the complexity of GitHub release naming conventions:\n    1. Some projects use 'v' prefixes in tags but not in filenames.\n    2. Some projects use dashes, others dots.\n    3. We support custom 'archive_name' patterns to handle any edge case.\n    4. Supports .tar.gz archives and simple .gz compressed binaries.\n    \"\"\"\n    name = data[\"name\"]\n    version = data[\"version\"]\n    repo = data[\"upstream\"][\"repo\"]\n    binary_name = data[\"build\"][\"binary_name\"]\n    archive_pattern = data[\"upstream\"].get(\"archive_name\")\n\n    # We strip the 'v' prefix for filename construction because Go projects\n    # typically tag 'v1.0.0' but release 'project-1.0.0.tar.gz'.\n    clean_version = version.lstrip(\"v\") if version.startswith(\"v\") else version\n\n    # Construct the download URL\n    if archive_pattern:\n        # Support two formats:\n        # 1. String pattern: \"project-{version}-{arch}.tar.gz\"\n        # 2. Dict per arch: { amd64: \"project-x86.tar.gz\", arm64: \"project-arm.tar.gz\" }\n        if isinstance(archive_pattern, dict):\n            # Dict format: lookup the pattern for this specific architecture\n            if arch not in archive_pattern:\n                click.echo(\n                    f\"Error: No archive_name defined for architecture '{arch}' in manifest\",\n                    err=True,\n                )\n                raise click.Abort()\n            pattern = archive_pattern[arch]\n        else:\n            # String format: use the pattern with variable substitution\n            pattern = archive_pattern\n\n        # Apply variable substitution\n        filename = pattern.format(\n            name=name,\n            version=version,\n            clean_version=clean_version,\n            arch=arch,\n            rpm_arch=\"x86_64\" if arch == \"amd64\" else \"aarch64\",\n            deb_arch=arch,  # DEB uses standard names (amd64, arm64)\n            upstream_linux_arch=\"x86_64\"\n            if arch == \"amd64\"\n            else \"arm64\",  # Mixed convention (x86_64, arm64)\n        )\n    else:\n        # Default standard Prometheus naming convention\n        upstream_arch = f\"linux-{arch}\"\n        filename = f\"{name}-{clean_version}.{upstream_arch}.tar.gz\"\n\n    url = f\"https://github.com/{repo}/releases/download/{version}/{filename}\"\n\n    click.echo(f\"Downloading {url}...\")\n    local_file = os.path.join(output_dir, filename)\n\n    # We look for the main binary AND any extra binaries (like promtool)\n    binaries_to_find = [binary_name] + data[\"build\"].get(\"extra_binaries\", [])\n    found_binaries = []\n\n    try:\n        with requests.get(url, stream=True, timeout=30) as r:\n            r.raise_for_status()\n            with open(local_file, \"wb\") as f:\n                for chunk in r.iter_content(chunk_size=8192):\n                    f.write(chunk)\n\n        # Case 1: Simple .gz file (single binary)\n        if filename.endswith(\".gz\") and not filename.endswith(\".tar.gz\"):\n            click.echo(f\"Decompressing single binary {filename}...\")\n            final_path = os.path.join(output_dir, binary_name)\n            with gzip.open(local_file, \"rb\") as f_in, open(final_path, \"wb\") as f_out:\n                shutil.copyfileobj(f_in, f_out)\n            os.chmod(final_path, 0o755)  # nosec B103 - Executable binary requires execute permissions\n            found_binaries.append(binary_name)\n            click.echo(f\"Binary ready: {final_path}\")\n\n        # Case 2: .tar.gz archive\n        else:\n            click.echo(f\"Extracting binaries {binaries_to_find}...\")\n            # We need to track extracted dirs to clean them up later\n            extracted_dirs = set()\n            with tarfile.open(local_file, \"r:gz\") as tar:\n                members = tar.getmembers()\n                for b_name in binaries_to_find:\n                    member_to_extract = None\n                    # Search for the binary, even if nested in a subfolder\n                    for member in members:\n                        if member.name.endswith(f\"/{b_name}\") or member.name == b_name:\n                            member_to_extract = member\n                            break\n\n                    if member_to_extract:\n                        # Flatten: we extract everything to the root of output_dir\n                        tar.extract(member_to_extract, path=output_dir, filter=\"data\")\n                        extracted_path = os.path.join(\n                            output_dir, member_to_extract.name\n                        )\n                        final_path = os.path.join(output_dir, b_name)\n\n                        if extracted_path != final_path:\n                            shutil.move(extracted_path, final_path)\n\n                        parts = member_to_extract.name.split(\"/\")\n                        if len(parts) &gt; 1:\n                            extracted_dirs.add(parts[0])\n\n                        os.chmod(final_path, 0o755)  # nosec B103 - Executable binary requires execute permissions\n                        found_binaries.append(b_name)\n                        click.echo(f\"Binary ready: {final_path}\")\n                    else:\n                        click.echo(f\"Warning: Binary '{b_name}' not found.\")\n\n            # Clean up the folder structure from the tarball (we keep only binaries)\n            for d in extracted_dirs:\n                dir_to_remove = os.path.join(output_dir, d)\n                if os.path.isdir(dir_to_remove):\n                    shutil.rmtree(dir_to_remove, ignore_errors=True)\n\n        if not found_binaries:\n            click.echo(\n                f\"Error: No binaries found in archive. Expected binaries: {', '.join(binaries_to_find)}\",\n                err=True,\n            )\n            click.echo(\n                \"Hint: Check that the 'binary_name' in your manifest matches the actual binary name in the upstream release.\",\n                err=True,\n            )\n            raise click.Abort()\n\n    except Exception as e:\n        click.echo(f\"Failed to process artifact: {e}\", err=True)\n        raise e\n    finally:\n        if os.path.exists(local_file):\n            os.remove(local_file)\n</code></pre>"},{"location":"api-reference/builder/#core.engine.builder.download_extra_sources","title":"<code>download_extra_sources(data, output_dir)</code>","text":"<p>Download additional files (like config examples) that are not in the release tarball.</p> Source code in <code>core/engine/builder.py</code> <pre><code>@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=2, max=10),\n    retry=retry_if_exception_type(requests.exceptions.RequestException),\n    reraise=True,\n)\ndef download_extra_sources(data, output_dir):\n    \"\"\"\n    Download additional files (like config examples) that are not in the release tarball.\n    \"\"\"\n    extra_sources = data.get(\"build\", {}).get(\"extra_sources\", [])\n    for source in extra_sources:\n        url = source[\"url\"]\n        filename = source[\"filename\"]\n        click.echo(f\"Downloading extra source: {url}...\")\n        try:\n            r = requests.get(url, timeout=30)\n            r.raise_for_status()\n            with open(os.path.join(output_dir, filename), \"wb\") as f:\n                f.write(r.content)\n            click.echo(f\"Extra source saved as {filename}\")\n        except Exception as e:\n            click.echo(f\"Warning: Failed to download extra source {url}: {e}\")\n</code></pre>"},{"location":"api-reference/builder/#core.engine.builder.get_upstream_license","title":"<code>get_upstream_license(repo_slug)</code>","text":"<p>Fetch license information from GitHub API. Returns SPDX ID (e.g. 'MIT', 'Apache-2.0') or None.</p> Source code in <code>core/engine/builder.py</code> <pre><code>def get_upstream_license(repo_slug):\n    \"\"\"\n    Fetch license information from GitHub API.\n    Returns SPDX ID (e.g. 'MIT', 'Apache-2.0') or None.\n    \"\"\"\n    try:\n        token = os.environ.get(\"GITHUB_TOKEN\")\n        headers = {\"Accept\": \"application/vnd.github.v3+json\"}\n        if token:\n            headers[\"Authorization\"] = f\"token {token}\"\n\n        url = f\"https://api.github.com/repos/{repo_slug}/license\"\n        r = requests.get(url, headers=headers, timeout=5)\n        if r.status_code == 200:\n            data = r.json()\n            return data.get(\"license\", {}).get(\"spdx_id\")\n    except Exception as e:\n        click.echo(f\"Warning: Could not fetch license for {repo_slug}: {e}\")\n    return None\n</code></pre>"},{"location":"api-reference/builder/#core.engine.builder.load_manifest","title":"<code>load_manifest(path)</code>","text":"<p>Loads and validates the manifest YAML file against the strict schema. This ensures we fail early if the user input is invalid.</p> Source code in <code>core/engine/builder.py</code> <pre><code>def load_manifest(path):\n    \"\"\"\n    Loads and validates the manifest YAML file against the strict schema.\n    This ensures we fail early if the user input is invalid.\n    \"\"\"\n    with open(path) as f:\n        data = yaml.safe_load(f)\n\n    schema = ManifestSchema()\n    try:\n        return schema.load(data)\n    except ValidationError as err:\n        click.echo(f\"Validation error in {path}: {err.messages}\", err=True)\n        raise click.Abort() from err\n</code></pre>"},{"location":"api-reference/builder/#core.engine.builder.render_deb_templates","title":"<code>render_deb_templates(data, output_dir, arch, env, manifest_dir)</code>","text":"<p>Generate the debian/ directory with all required DEB packaging files: - control (package metadata) - rules (build rules) - changelog (version history) - compat (debhelper compatibility level) - .service (systemd unit if enabled) Source code in <code>core/engine/builder.py</code> <pre><code>def render_deb_templates(data, output_dir, arch, env, manifest_dir):\n    \"\"\"\n    Generate the debian/ directory with all required DEB packaging files:\n    - control (package metadata)\n    - rules (build rules)\n    - changelog (version history)\n    - compat (debhelper compatibility level)\n    - &lt;name&gt;.service (systemd unit if enabled)\n    \"\"\"\n    from datetime import datetime\n\n    debian_dir = os.path.join(output_dir, \"debian\")\n    os.makedirs(debian_dir, exist_ok=True)\n    click.echo(f\"Creating debian/ directory at {debian_dir}\")\n\n    # Add build date for changelog (RFC 2822 format required by Debian)\n    from datetime import timezone\n\n    dt = datetime.now(timezone.utc)\n    data[\"build_date\"] = dt.strftime(\"%a, %d %b %Y %H:%M:%S %z\")\n\n    # Add binary_name to root level for easier access in templates\n    data[\"binary_name\"] = data.get(\"build\", {}).get(\"binary_name\", data[\"name\"])\n\n    # 0. Handle extra files FIRST (copy and set build_source before rendering templates)\n    artifacts = data.get(\"artifacts\", {})\n    deb_config = artifacts.get(\"deb\", {})\n    for extra_file in deb_config.get(\"extra_files\", []):\n        source_path = extra_file[\"source\"]\n        local_src = os.path.join(manifest_dir, source_path)\n        downloaded_src = os.path.join(output_dir, source_path)\n\n        dst_name = os.path.basename(source_path)\n        dst_path = os.path.join(output_dir, dst_name)\n\n        if os.path.exists(local_src):\n            shutil.copy(local_src, dst_path)\n            click.echo(f\"  Copied extra file: {dst_name}\")\n        elif os.path.exists(downloaded_src):\n            if downloaded_src != dst_path:\n                shutil.copy(downloaded_src, dst_path)\n                click.echo(f\"  Copied extra file: {dst_name}\")\n        else:\n            click.echo(f\"  Warning: Extra file {source_path} not found\")\n\n        extra_file[\"build_source\"] = dst_name\n\n    # 1. Generate debian/control\n    template = env.get_template(\"debian_control.j2\")\n    control_content = template.render(data)\n    with open(os.path.join(debian_dir, \"control\"), \"w\") as f:\n        f.write(control_content)\n    click.echo(\"  Created debian/control\")\n\n    # 2. Generate debian/rules\n    template = env.get_template(\"debian_rules.j2\")\n    rules_content = template.render(data)\n    rules_path = os.path.join(debian_dir, \"rules\")\n    with open(rules_path, \"w\") as f:\n        f.write(rules_content)\n    os.chmod(rules_path, 0o755)  # nosec B103 - debian/rules must be executable\n    click.echo(\"  Created debian/rules\")\n\n    # 3. Generate debian/changelog\n    template = env.get_template(\"debian_changelog.j2\")\n    changelog_content = template.render(data)\n    with open(os.path.join(debian_dir, \"changelog\"), \"w\") as f:\n        f.write(changelog_content)\n    click.echo(\"  Created debian/changelog\")\n\n    # 4. Create debian/compat (debhelper compatibility level)\n    with open(os.path.join(debian_dir, \"compat\"), \"w\") as f:\n        f.write(\"10\\n\")\n    click.echo(\"  Created debian/compat\")\n\n    # 5. Generate systemd service if enabled\n    if data.get(\"artifacts\", {}).get(\"deb\", {}).get(\"systemd\", {}).get(\"enabled\"):\n        template = env.get_template(\"debian_service.j2\")\n        service_content = template.render(data)\n        # Debian package names use dashes instead of underscores\n        deb_name = data[\"name\"].replace(\"_\", \"-\")\n        service_file = os.path.join(debian_dir, f\"{deb_name}.service\")\n        with open(service_file, \"w\") as f:\n            f.write(service_content)\n        click.echo(f\"  Created debian/{deb_name}.service\")\n\n    click.echo(\"\u2713 DEB packaging files generated successfully\")\n</code></pre>"},{"location":"api-reference/catalog-v3/","title":"V3 Catalog Architecture","text":""},{"location":"api-reference/catalog-v3/#overview","title":"Overview","text":"<p>The V3 catalog architecture introduces granular artifact files with atomic writes to eliminate race conditions in parallel GitHub Actions builds. Each build job writes exactly one JSON file, ensuring consistency and reliability.</p>"},{"location":"api-reference/catalog-v3/#directory-structure","title":"Directory Structure","text":"<pre><code>catalog/\n\u251c\u2500\u2500 &lt;exporter_name&gt;/\n\u2502   \u251c\u2500\u2500 rpm_&lt;arch&gt;_&lt;dist&gt;.json       # RPM artifact metadata\n\u2502   \u251c\u2500\u2500 deb_&lt;arch&gt;_&lt;dist&gt;.json       # DEB artifact metadata\n\u2502   \u251c\u2500\u2500 docker.json                  # Docker image metadata\n\u2502   \u2514\u2500\u2500 metadata.json                # Aggregated metadata (generated on-demand)\n\u2514\u2500\u2500 catalog.json                     # Global index (backward compatible)\n</code></pre>"},{"location":"api-reference/catalog-v3/#example-node_exporter","title":"Example: node_exporter","text":"<pre><code>catalog/node_exporter/\n\u251c\u2500\u2500 rpm_amd64_el9.json\n\u251c\u2500\u2500 rpm_amd64_el10.json\n\u251c\u2500\u2500 rpm_arm64_el9.json\n\u251c\u2500\u2500 rpm_arm64_el10.json\n\u251c\u2500\u2500 deb_amd64_ubuntu-22.04.json\n\u251c\u2500\u2500 deb_amd64_ubuntu-24.04.json\n\u251c\u2500\u2500 deb_amd64_debian-12.json\n\u251c\u2500\u2500 deb_amd64_debian-13.json\n\u251c\u2500\u2500 deb_arm64_ubuntu-22.04.json\n\u251c\u2500\u2500 deb_arm64_ubuntu-24.04.json\n\u251c\u2500\u2500 deb_arm64_debian-12.json\n\u251c\u2500\u2500 deb_arm64_debian-13.json\n\u251c\u2500\u2500 docker.json\n\u2514\u2500\u2500 metadata.json\n</code></pre>"},{"location":"api-reference/catalog-v3/#artifact-file-formats","title":"Artifact File Formats","text":""},{"location":"api-reference/catalog-v3/#rpm-artifact-rpm_arch_distjson","title":"RPM Artifact (<code>rpm_&lt;arch&gt;_&lt;dist&gt;.json</code>)","text":"<pre><code>{\n  \"format_version\": \"3.0\",\n  \"artifact_type\": \"rpm\",\n  \"exporter\": \"node_exporter\",\n  \"version\": \"1.10.2\",\n  \"arch\": \"amd64\",\n  \"dist\": \"el9\",\n  \"build_date\": \"2024-02-13T10:30:00Z\",\n  \"status\": \"success\",\n  \"package\": {\n    \"filename\": \"node_exporter-1.10.2-1.el9.x86_64.rpm\",\n    \"url\": \"https://sckyzo.github.io/monitoring-hub/el9/x86_64/node_exporter-1.10.2-1.el9.x86_64.rpm\",\n    \"sha256\": \"abc123def456...\",\n    \"size_bytes\": 12345678\n  },\n  \"rpm_metadata\": {\n    \"name\": \"node_exporter\",\n    \"version\": \"1.10.2\",\n    \"release\": \"1.el9\",\n    \"arch\": \"x86_64\",\n    \"summary\": \"Prometheus exporter for hardware and OS metrics\",\n    \"license\": \"Apache-2.0\",\n    \"vendor\": \"Monitoring Hub\"\n  }\n}\n</code></pre> <p>Fields: - <code>format_version</code>: Always \"3.0\" for V3 artifacts - <code>artifact_type</code>: \"rpm\", \"deb\", or \"docker\" - <code>exporter</code>: Exporter name from manifest - <code>version</code>: Version without 'v' prefix - <code>arch</code>: Architecture (amd64, arm64) - <code>dist</code>: Distribution (el8, el9, el10) - <code>build_date</code>: ISO 8601 timestamp - <code>status</code>: \"success\", \"failed\", \"pending\", or \"na\" - <code>package</code>: Package file details - <code>rpm_metadata</code>: Extracted RPM metadata (optional, requires rpm tools)</p>"},{"location":"api-reference/catalog-v3/#deb-artifact-deb_arch_distjson","title":"DEB Artifact (<code>deb_&lt;arch&gt;_&lt;dist&gt;.json</code>)","text":"<pre><code>{\n  \"format_version\": \"3.0\",\n  \"artifact_type\": \"deb\",\n  \"exporter\": \"node_exporter\",\n  \"version\": \"1.10.2\",\n  \"arch\": \"amd64\",\n  \"dist\": \"ubuntu-22.04\",\n  \"build_date\": \"2024-02-13T10:30:00Z\",\n  \"status\": \"success\",\n  \"package\": {\n    \"filename\": \"node-exporter_1.10.2-1_amd64.deb\",\n    \"url\": \"https://sckyzo.github.io/monitoring-hub/apt/pool/main/n/node-exporter/node-exporter_1.10.2-1_amd64.deb\",\n    \"sha256\": \"def789ghi012...\",\n    \"size_bytes\": 10234567\n  },\n  \"deb_metadata\": {\n    \"package\": \"node-exporter\",\n    \"version\": \"1.10.2-1\",\n    \"architecture\": \"amd64\",\n    \"section\": \"net\",\n    \"priority\": \"optional\",\n    \"description\": \"Prometheus exporter for hardware and OS metrics\"\n  }\n}\n</code></pre> <p>Fields: - Same as RPM artifact, but with <code>deb_metadata</code> instead - <code>dist</code>: Distribution (ubuntu-22.04, ubuntu-24.04, debian-12, debian-13) - <code>deb_metadata</code>: Extracted DEB control file metadata (optional, requires dpkg tools)</p>"},{"location":"api-reference/catalog-v3/#docker-artifact-dockerjson","title":"Docker Artifact (<code>docker.json</code>)","text":"<pre><code>{\n  \"format_version\": \"3.0\",\n  \"artifact_type\": \"docker\",\n  \"exporter\": \"node_exporter\",\n  \"version\": \"1.10.2\",\n  \"build_date\": \"2024-02-13T10:30:00Z\",\n  \"status\": \"success\",\n  \"image\": {\n    \"registry\": \"ghcr.io\",\n    \"repository\": \"sckyzo/monitoring-hub/node-exporter\",\n    \"tag\": \"1.10.2\",\n    \"digest\": \"sha256:abc123def456...\",\n    \"platforms\": [\"linux/amd64\", \"linux/arm64\"]\n  },\n  \"manifest\": {\n    \"schemaVersion\": 2,\n    \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n    \"config\": {\n      \"digest\": \"sha256:config123...\",\n      \"size\": 1234\n    },\n    \"layers\": [\n      {\n        \"digest\": \"sha256:layer1...\",\n        \"size\": 12345678\n      }\n    ]\n  }\n}\n</code></pre> <p>Fields: - <code>image</code>: Container image details - <code>manifest</code>: OCI manifest (optional, extracted from registry)</p>"},{"location":"api-reference/catalog-v3/#aggregated-metadata-metadatajson","title":"Aggregated Metadata (<code>metadata.json</code>)","text":"<p>Generated on-demand by <code>aggregate_catalog_metadata.py</code> from granular artifacts:</p> <pre><code>{\n  \"format_version\": \"3.0\",\n  \"exporter\": \"node_exporter\",\n  \"version\": \"1.10.2\",\n  \"category\": \"System\",\n  \"description\": \"Hardware and OS metrics\",\n  \"last_updated\": \"2024-02-13T10:30:00Z\",\n  \"artifacts\": {\n    \"rpm\": {\n      \"el9\": {\n        \"amd64\": {\n          \"status\": \"success\",\n          \"url\": \"https://sckyzo.github.io/monitoring-hub/el9/x86_64/node_exporter-1.10.2-1.el9.x86_64.rpm\",\n          \"size_bytes\": 12345678,\n          \"sha256\": \"abc123...\"\n        },\n        \"arm64\": {\n          \"status\": \"success\",\n          \"url\": \"https://sckyzo.github.io/monitoring-hub/el9/aarch64/node_exporter-1.10.2-1.el9.aarch64.rpm\",\n          \"size_bytes\": 12123456,\n          \"sha256\": \"def456...\"\n        }\n      },\n      \"el10\": { ... }\n    },\n    \"deb\": {\n      \"ubuntu-22.04\": {\n        \"amd64\": {\n          \"status\": \"success\",\n          \"url\": \"https://sckyzo.github.io/monitoring-hub/apt/pool/main/n/node-exporter/...\",\n          \"size_bytes\": 10234567,\n          \"sha256\": \"ghi789...\"\n        },\n        \"arm64\": { ... }\n      },\n      \"debian-12\": { ... }\n    },\n    \"docker\": {\n      \"status\": \"success\",\n      \"registry\": \"ghcr.io\",\n      \"repository\": \"sckyzo/monitoring-hub/node-exporter\",\n      \"tag\": \"1.10.2\",\n      \"digest\": \"sha256:abc123...\",\n      \"platforms\": [\"linux/amd64\", \"linux/arm64\"]\n    }\n  },\n  \"status\": {\n    \"rpm\": \"success\",\n    \"deb\": \"success\",\n    \"docker\": \"success\"\n  }\n}\n</code></pre> <p>Aggregation Logic:</p> <ol> <li>Load granular artifacts: Read all <code>rpm_*.json</code>, <code>deb_*.json</code>, <code>docker.json</code></li> <li>Group by type: Organize RPM by dist/arch, DEB by dist/arch</li> <li>Compute aggregate status:</li> <li><code>success</code>: At least one artifact succeeded</li> <li><code>failed</code>: All enabled artifacts failed</li> <li><code>pending</code>: Some artifacts still building</li> <li><code>na</code>: Artifact type disabled in manifest</li> <li>Extract latest timestamp: Use most recent <code>build_date</code> from all artifacts</li> </ol>"},{"location":"api-reference/catalog-v3/#global-catalog-catalogjson","title":"Global Catalog (<code>catalog.json</code>)","text":"<p>Backward-compatible index of all exporters:</p> <pre><code>{\n  \"format_version\": \"3.0\",\n  \"generated_at\": \"2024-02-13T12:00:00Z\",\n  \"exporters\": [\n    {\n      \"name\": \"node_exporter\",\n      \"version\": \"1.10.2\",\n      \"category\": \"System\",\n      \"description\": \"Hardware and OS metrics\",\n      \"last_updated\": \"2024-02-13T10:30:00Z\",\n      \"rpm_status\": \"success\",\n      \"deb_status\": \"success\",\n      \"docker_status\": \"success\",\n      \"metadata_url\": \"https://sckyzo.github.io/monitoring-hub/catalog/node_exporter/metadata.json\"\n    },\n    ...\n  ]\n}\n</code></pre>"},{"location":"api-reference/catalog-v3/#v3-scripts","title":"V3 Scripts","text":""},{"location":"api-reference/catalog-v3/#generate_artifact_metadatapy","title":"generate_artifact_metadata.py","text":"<p>Generates atomic artifact JSON files from build outputs.</p> <p>Usage: <pre><code>python3 core/scripts/generate_artifact_metadata.py \\\n  --type rpm \\\n  --exporter node_exporter \\\n  --version 1.10.2 \\\n  --arch amd64 \\\n  --dist el9 \\\n  --filename node_exporter-1.10.2-1.el9.x86_64.rpm \\\n  --url https://sckyzo.github.io/monitoring-hub/el9/x86_64/node_exporter-1.10.2-1.el9.x86_64.rpm \\\n  --sha256 abc123... \\\n  --size 12345678 \\\n  --status success \\\n  --output catalog/node_exporter/rpm_amd64_el9.json\n</code></pre></p> <p>Options: - <code>--type</code>: Artifact type (rpm, deb, docker) - <code>--exporter</code>: Exporter name - <code>--version</code>: Version (without 'v' prefix) - <code>--arch</code>: Architecture (amd64, arm64) - <code>--dist</code>: Distribution (el9, ubuntu-22.04, etc.) - <code>--filename</code>: Package filename - <code>--url</code>: Download URL - <code>--sha256</code>: SHA256 checksum - <code>--size</code>: File size in bytes - <code>--status</code>: Build status (success, failed, pending, na) - <code>--output</code>: Output JSON file path - <code>--extract-metadata</code>: Extract RPM/DEB metadata (requires rpm/dpkg tools)</p>"},{"location":"api-reference/catalog-v3/#aggregate_catalog_metadatapy","title":"aggregate_catalog_metadata.py","text":"<p>Aggregates granular artifact files into <code>metadata.json</code>.</p> <p>Usage: <pre><code>python3 core/scripts/aggregate_catalog_metadata.py \\\n  --exporter node_exporter \\\n  --catalog-dir catalog \\\n  --manifest-path exporters/node_exporter/manifest.yaml \\\n  --output catalog/node_exporter/metadata.json\n</code></pre></p> <p>Process: 1. Load all <code>rpm_*.json</code>, <code>deb_*.json</code>, <code>docker.json</code> from exporter directory 2. Validate format_version and artifact_type 3. Group artifacts by type and platform 4. Compute aggregate status for each type 5. Extract exporter metadata from manifest (category, description, readme) 6. Generate metadata.json with V3.0 format</p>"},{"location":"api-reference/catalog-v3/#publish_artifact_metadatash","title":"publish_artifact_metadata.sh","text":"<p>Publishes atomic artifact files to gh-pages with git operations.</p> <p>Usage: <pre><code>bash core/scripts/publish_artifact_metadata.sh \\\n  catalog/node_exporter/rpm_amd64_el9.json\n</code></pre></p> <p>Process: 1. Clone gh-pages branch (if not already cloned) 2. Copy artifact JSON to catalog directory 3. Git add the single file 4. Git commit with descriptive message 5. Git push (with retry on conflict)</p>"},{"location":"api-reference/catalog-v3/#key-v3-principles","title":"Key V3 Principles","text":""},{"location":"api-reference/catalog-v3/#1-atomic-writes","title":"1. Atomic Writes","text":"<p>Each GitHub Actions job writes exactly 1 file:</p> <pre><code>- name: \ud83d\udce6 Publish RPM metadata (amd64/el9)\n  run: |\n    python3 core/scripts/generate_artifact_metadata.py \\\n      --type rpm --arch amd64 --dist el9 \\\n      --output catalog/node_exporter/rpm_amd64_el9.json\n\n    bash core/scripts/publish_artifact_metadata.sh \\\n      catalog/node_exporter/rpm_amd64_el9.json\n</code></pre> <p>No race conditions: Jobs can run in parallel without conflicts.</p>"},{"location":"api-reference/catalog-v3/#2-format-versioning","title":"2. Format Versioning","text":"<p>All files include <code>\"format_version\": \"3.0\"</code> for future compatibility:</p> <pre><code>if artifact[\"format_version\"] == \"3.0\":\n    # Use V3 schema\nelif artifact[\"format_version\"] == \"4.0\":\n    # Use V4 schema (future)\n</code></pre>"},{"location":"api-reference/catalog-v3/#3-on-demand-aggregation","title":"3. On-Demand Aggregation","text":"<p>Granular files are the source of truth. Aggregation happens: - At read-time by portal (site_generator_v2.py) - On-demand by aggregate_catalog_metadata.py - Never during build (eliminates concurrent writes)</p>"},{"location":"api-reference/catalog-v3/#4-backward-compatibility","title":"4. Backward Compatibility","text":"<p><code>catalog.json</code> maintains legacy format for existing consumers:</p> <pre><code># Legacy consumers still work\ncatalog = requests.get(\"https://sckyzo.github.io/monitoring-hub/catalog.json\").json()\nexporters = catalog[\"exporters\"]\n\n# V3-aware consumers can use granular files\nrpm_el9 = requests.get(\n    \"https://sckyzo.github.io/monitoring-hub/catalog/node_exporter/rpm_amd64_el9.json\"\n).json()\n</code></pre>"},{"location":"api-reference/catalog-v3/#migration-guide","title":"Migration Guide","text":""},{"location":"api-reference/catalog-v3/#for-portal-consumers","title":"For Portal Consumers","text":"<p>Before V3: <pre><code># Read catalog.json\ncatalog = load_catalog()\nfor exporter in catalog[\"exporters\"]:\n    rpm_status = exporter[\"rpm_status\"]\n</code></pre></p> <p>After V3: <pre><code># Option 1: Use aggregated metadata.json (recommended)\nmetadata = load_metadata(\"catalog/node_exporter/metadata.json\")\nrpm_status = metadata[\"status\"][\"rpm\"]\n\n# Option 2: Read granular artifacts directly\nrpm_el9 = load_artifact(\"catalog/node_exporter/rpm_amd64_el9.json\")\nif rpm_el9[\"status\"] == \"success\":\n    download_url = rpm_el9[\"package\"][\"url\"]\n</code></pre></p>"},{"location":"api-reference/catalog-v3/#for-ci-workflows","title":"For CI Workflows","text":"<p>Before V3: <pre><code>- name: Upload catalog\n  run: |\n    # Danger: Multiple jobs write to catalog.json (race condition!)\n    jq '.exporters += [{\"name\": \"...\"}]' catalog.json &gt; tmp &amp;&amp; mv tmp catalog.json\n    git add catalog.json\n    git commit -m \"Update catalog\"\n</code></pre></p> <p>After V3: <pre><code>- name: Publish artifact metadata\n  run: |\n    # Safe: Each job writes 1 file (no race condition)\n    python3 core/scripts/generate_artifact_metadata.py \\\n      --type rpm --arch amd64 --dist el9 \\\n      --output catalog/node_exporter/rpm_amd64_el9.json\n\n    bash core/scripts/publish_artifact_metadata.sh \\\n      catalog/node_exporter/rpm_amd64_el9.json\n</code></pre></p>"},{"location":"api-reference/catalog-v3/#testing","title":"Testing","text":"<p>V3 artifacts are validated by comprehensive test suite:</p> <ul> <li>test_artifact_schemas.py (330 lines) - Schema validation for all artifact types</li> <li>test_aggregation.py (420 lines) - Aggregation logic and status computation</li> <li>test_site_generator.py (180 lines) - Portal generation and V3\u2192legacy conversion</li> </ul> <p>Run tests: <pre><code>./devctl test\n./devctl test-cov  # With coverage report\n</code></pre></p>"},{"location":"api-reference/catalog-v3/#see-also","title":"See Also","text":"<ul> <li>Architecture Overview</li> <li>Refactoring V2 Plan</li> <li>State Manager</li> <li>Builder</li> </ul>"},{"location":"api-reference/schema/","title":"Schema API","text":""},{"location":"api-reference/schema/#core.engine.schema","title":"<code>core.engine.schema</code>","text":""},{"location":"api-reference/schema/#core.engine.schema-classes","title":"Classes","text":""},{"location":"api-reference/schema/#core.engine.schema.UpstreamSchema","title":"<code>UpstreamSchema</code>","text":"<p>               Bases: <code>Schema</code></p>"},{"location":"api-reference/schema/#core.engine.schema.UpstreamSchema-functions","title":"Functions","text":""},{"location":"api-reference/schema/#core.engine.schema.UpstreamSchema.validate_upstream","title":"<code>validate_upstream(data, **_kwargs)</code>","text":"<p>Validate type-specific requirements.</p> Source code in <code>core/engine/schema.py</code> <pre><code>@validates_schema\ndef validate_upstream(self, data, **_kwargs):\n    \"\"\"Validate type-specific requirements.\"\"\"\n    if data.get(\"type\") == \"github\":\n        if not data.get(\"repo\"):\n            raise ValidationError(\"'repo' is required for upstream type 'github'\")\n\n        # Validate archive_name format if provided\n        archive_name = data.get(\"archive_name\")\n        if archive_name is not None:\n            if isinstance(archive_name, dict):\n                # Dict format: must have keys for all architectures\n                if not archive_name:\n                    raise ValidationError(\"archive_name dict cannot be empty\")\n                # Validate that all values are strings\n                for arch, pattern in archive_name.items():\n                    if not isinstance(pattern, str):\n                        raise ValidationError(\n                            f\"archive_name[{arch}] must be a string, got {type(pattern)}\"\n                        )\n            elif not isinstance(archive_name, str):\n                raise ValidationError(\n                    \"archive_name must be either a string pattern or a dict mapping architectures to patterns\"\n                )\n\n    elif data.get(\"type\") == \"local\":\n        if not data.get(\"local_binary\") and not data.get(\"local_archive\"):\n            raise ValidationError(\n                \"'local_binary' or 'local_archive' required for upstream type 'local'\"\n            )\n        if data.get(\"local_binary\") and data.get(\"local_archive\"):\n            raise ValidationError(\n                \"Only one of 'local_binary' or 'local_archive' allowed\"\n            )\n</code></pre>"},{"location":"api-reference/state-manager/","title":"State Manager API","text":""},{"location":"api-reference/state-manager/#core.engine.state_manager","title":"<code>core.engine.state_manager</code>","text":""},{"location":"api-reference/state-manager/#core.engine.state_manager-functions","title":"Functions","text":""},{"location":"api-reference/state-manager/#core.engine.state_manager.get_local_state","title":"<code>get_local_state(exporters_dir=EXPORTERS_DIR)</code>","text":"<p>Reads all local manifest.yaml files to build the current desired state.</p> Source code in <code>core/engine/state_manager.py</code> <pre><code>def get_local_state(exporters_dir=EXPORTERS_DIR):\n    \"\"\"\n    Reads all local manifest.yaml files to build the current desired state.\n    \"\"\"\n    local_state = {}\n    if not os.path.isdir(exporters_dir):\n        return {}\n\n    for exporter_name in os.listdir(exporters_dir):\n        manifest_path = os.path.join(exporters_dir, exporter_name, \"manifest.yaml\")\n        if os.path.exists(manifest_path):\n            try:\n                with open(manifest_path) as f:\n                    data = yaml.safe_load(f)\n                    # Normalize version (strip 'v' prefix if present to match catalog standard)\n                    version = data[\"version\"].lstrip(\"v\")\n                    local_state[exporter_name] = version\n            except Exception as e:\n                print(f\"Error reading {manifest_path}: {e}\", file=sys.stderr)\n    return local_state\n</code></pre>"},{"location":"api-reference/state-manager/#core.engine.state_manager.get_remote_catalog","title":"<code>get_remote_catalog(catalog_url=DEFAULT_CATALOG_URL)</code>","text":"<p>Fetches the current state of the repository from the deployed catalog.json. Returns a dictionary keyed by exporter name with version info.</p> Source code in <code>core/engine/state_manager.py</code> <pre><code>@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=2, max=10),\n    retry=retry_if_exception_type(requests.exceptions.RequestException),\n    reraise=True,\n)\ndef get_remote_catalog(catalog_url=DEFAULT_CATALOG_URL):\n    \"\"\"\n    Fetches the current state of the repository from the deployed catalog.json.\n    Returns a dictionary keyed by exporter name with version info.\n    \"\"\"\n    try:\n        print(f\"Fetching remote catalog from {catalog_url}...\", file=sys.stderr)\n        r = requests.get(catalog_url, timeout=10)\n        if r.status_code == 200:\n            data = r.json()\n            # Convert list to dict for easier lookup: {'node_exporter': '1.8.1', ...}\n            return {item[\"name\"]: item[\"version\"] for item in data.get(\"exporters\", [])}\n        else:\n            print(\n                f\"Warning: Remote catalog not found (Status {r.status_code}). Assuming empty state.\",\n                file=sys.stderr,\n            )\n            return {}\n    except Exception as e:\n        print(\n            f\"Warning: Could not fetch remote catalog: {e}. Assuming empty state.\",\n            file=sys.stderr,\n        )\n        return {}\n</code></pre>"},{"location":"architecture/ARCHIVED-refactoring-v2-plan/","title":"Refactoring V2 - Complete Architecture Redesign","text":"<p>Status: \u2705 Phases 1-3 Complete (Ready for Merge) Branch: <code>refactor/v2-architecture</code> Completion Date: 2026-02-13 (2 weeks) Goal: Replace fragmented patch-based fixes with a clean, atomic, scalable architecture</p>"},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#executive-summary","title":"\ud83c\udfaf Executive Summary","text":"<p>The current system suffers from: - Race conditions: Multiple jobs writing to same JSON files - Repository corruption: Metadata overwrite bugs (fixed in #45 but system remains fragile) - Code duplication: 850+ lines in release.yml, duplicated in full-build.yml - Fragmented data: <code>release_urls.json</code> + <code>build-info.json</code> + <code>catalog.json</code> + manifests - Non-atomic operations: Metadata generation can fail mid-process</p> <p>Solution: Complete redesign with atomic writes, granular catalog structure, simplified workflows.</p>"},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#phase-1-granular-catalog-architecture-week-1","title":"\ud83d\udcd0 Phase 1: Granular Catalog Architecture (Week 1)","text":""},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#current-state-problematic","title":"Current State (Problematic)","text":"<pre><code>catalog/\n\u251c\u2500\u2500 index.json (lightweight list)\n\u251c\u2500\u2500 node_exporter.json (ALL metadata in one file)\n\u251c\u2500\u2500 redis_exporter.json\n\u2514\u2500\u2500 ...\n\nIssues:\n- \u274c Multiple jobs write to same file \u2192 race conditions\n- \u274c No atomic operations\n- \u274c Difficult to track which artifact succeeded/failed\n- \u274c site_generator.py has complex aggregation logic\n</code></pre>"},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#new-state-atomic","title":"New State (Atomic)","text":"<pre><code>catalog/\n\u251c\u2500\u2500 index.json (lightweight: names + versions only)\n\u2514\u2500\u2500 &lt;exporter&gt;/\n    \u251c\u2500\u2500 rpm_amd64_el8.json      \u2190 Written by RPM job el8/amd64\n    \u251c\u2500\u2500 rpm_amd64_el9.json      \u2190 Written by RPM job el9/amd64\n    \u251c\u2500\u2500 rpm_amd64_el10.json\n    \u251c\u2500\u2500 rpm_arm64_el8.json      \u2190 Written by RPM job el8/arm64\n    \u251c\u2500\u2500 rpm_arm64_el9.json\n    \u251c\u2500\u2500 rpm_arm64_el10.json\n    \u251c\u2500\u2500 deb_amd64_ubuntu-22.04.json  \u2190 Written by DEB job ubuntu-22.04/amd64\n    \u251c\u2500\u2500 deb_amd64_ubuntu-24.04.json\n    \u251c\u2500\u2500 deb_amd64_debian-12.json\n    \u251c\u2500\u2500 deb_amd64_debian-13.json\n    \u251c\u2500\u2500 deb_arm64_ubuntu-22.04.json\n    \u251c\u2500\u2500 deb_arm64_ubuntu-24.04.json\n    \u251c\u2500\u2500 deb_arm64_debian-12.json\n    \u251c\u2500\u2500 deb_arm64_debian-13.json\n    \u251c\u2500\u2500 docker.json             \u2190 Written by Docker job\n    \u2514\u2500\u2500 metadata.json           \u2190 Aggregated by portal generator (read-only)\n</code></pre>"},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#artifact-json-schema","title":"Artifact JSON Schema","text":"<p>Example: <code>catalog/node_exporter/rpm_amd64_el9.json</code> <pre><code>{\n  \"format_version\": \"3.0\",\n  \"artifact_type\": \"rpm\",\n  \"exporter\": \"node_exporter\",\n  \"version\": \"1.10.2\",\n  \"arch\": \"amd64\",\n  \"dist\": \"el9\",\n  \"build_date\": \"2026-02-13T15:30:00Z\",\n  \"status\": \"success\",\n  \"package\": {\n    \"filename\": \"node_exporter-1.10.2-1.el9.x86_64.rpm\",\n    \"url\": \"https://github.com/SckyzO/monitoring-hub/releases/download/node_exporter-v1.10.2/node_exporter-1.10.2-1.el9.x86_64.rpm\",\n    \"sha256\": \"abc123...\",\n    \"size_bytes\": 12345678\n  },\n  \"rpm_metadata\": {\n    \"name\": \"node_exporter\",\n    \"version\": \"1.10.2\",\n    \"release\": \"1\",\n    \"arch\": \"x86_64\",\n    \"summary\": \"Prometheus exporter for hardware and OS metrics\",\n    \"license\": \"Apache-2.0\"\n  }\n}\n</code></pre></p> <p>Example: <code>catalog/node_exporter/docker.json</code> <pre><code>{\n  \"format_version\": \"3.0\",\n  \"artifact_type\": \"docker\",\n  \"exporter\": \"node_exporter\",\n  \"version\": \"1.10.2\",\n  \"build_date\": \"2026-02-13T15:45:00Z\",\n  \"status\": \"success\",\n  \"images\": [\n    {\n      \"registry\": \"ghcr.io\",\n      \"repository\": \"sckyzo/node_exporter\",\n      \"tag\": \"1.10.2\",\n      \"digest\": \"sha256:def456...\",\n      \"platforms\": [\"linux/amd64\", \"linux/arm64\"],\n      \"size_bytes\": 23456789\n    },\n    {\n      \"registry\": \"ghcr.io\",\n      \"repository\": \"sckyzo/node_exporter\",\n      \"tag\": \"latest\",\n      \"digest\": \"sha256:def456...\",\n      \"platforms\": [\"linux/amd64\", \"linux/arm64\"],\n      \"size_bytes\": 23456789\n    }\n  ]\n}\n</code></pre></p> <p>Example: <code>catalog/node_exporter/metadata.json</code> (aggregated by portal) <pre><code>{\n  \"format_version\": \"3.0\",\n  \"exporter\": \"node_exporter\",\n  \"version\": \"1.10.2\",\n  \"category\": \"System\",\n  \"description\": \"Prometheus exporter for hardware and OS metrics\",\n  \"last_updated\": \"2026-02-13T15:45:00Z\",\n  \"artifacts\": {\n    \"rpm\": {\n      \"el8\": {\"amd64\": \"success\", \"arm64\": \"success\"},\n      \"el9\": {\"amd64\": \"success\", \"arm64\": \"success\"},\n      \"el10\": {\"amd64\": \"success\", \"arm64\": \"success\"}\n    },\n    \"deb\": {\n      \"ubuntu-22.04\": {\"amd64\": \"success\", \"arm64\": \"success\"},\n      \"ubuntu-24.04\": {\"amd64\": \"success\", \"arm64\": \"success\"},\n      \"debian-12\": {\"amd64\": \"success\", \"arm64\": \"success\"},\n      \"debian-13\": {\"amd64\": \"success\", \"arm64\": \"success\"}\n    },\n    \"docker\": \"success\"\n  }\n}\n</code></pre></p>"},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#implementation-tasks","title":"Implementation Tasks","text":""},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#task-11-create-artifact-json-generator-scripts","title":"Task 1.1: Create artifact JSON generator scripts","text":"<ul> <li> <code>core/scripts/generate_artifact_metadata.py</code> - Generic artifact metadata generator</li> <li>Takes: artifact type, exporter, version, arch, dist, URLs, checksums</li> <li>Outputs: Single atomic JSON file</li> <li>Validates: Schema compliance</li> </ul>"},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#task-12-modify-releaseyml-to-write-granular-artifacts","title":"Task 1.2: Modify release.yml to write granular artifacts","text":"<ul> <li> After RPM upload: Write <code>catalog/&lt;exporter&gt;/rpm_&lt;arch&gt;_&lt;dist&gt;.json</code></li> <li> After DEB upload: Write <code>catalog/&lt;exporter&gt;/deb_&lt;arch&gt;_&lt;dist&gt;.json</code></li> <li> After Docker push: Write <code>catalog/&lt;exporter&gt;/docker.json</code></li> <li> Each job commits only ITS file to gh-pages</li> </ul>"},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#task-13-update-site_generatorpy-to-aggregate-artifacts","title":"Task 1.3: Update site_generator.py to aggregate artifacts","text":"<ul> <li> Read all <code>catalog/&lt;exporter&gt;/*.json</code> files (except metadata.json)</li> <li> Aggregate into <code>catalog/&lt;exporter&gt;/metadata.json</code></li> <li> Generate <code>catalog/index.json</code> (exporter list)</li> <li> Generate portal HTML with aggregated data</li> <li> Remove dependency on <code>release_urls.json</code> and <code>build-info.json</code></li> </ul>"},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#task-14-remove-legacy-artifacts","title":"Task 1.4: Remove legacy artifacts","text":"<ul> <li> Delete <code>release_urls.json</code> generation</li> <li> Delete <code>build-info.json</code> generation</li> <li> Update workflows to not upload these artifacts</li> <li> Clean up gh-pages (legacy files can stay for backward compat)</li> </ul>"},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#phase-2-workflow-simplification-week-2","title":"\ud83d\udd04 Phase 2: Workflow Simplification (Week 2)","text":""},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#current-state","title":"Current State","text":"<p>10 workflow files: 1. <code>scan-updates.yml</code> - CRON version checker \u2705 KEEP 2. <code>build-pr.yml</code> - PR validation \u2705 KEEP 3. <code>release.yml</code> - Build 1 exporter (850+ lines) \u26a0\ufe0f REFACTOR 4. <code>auto-release.yml</code> - Trigger release \u26a0\ufe0f FIX 5. <code>full-build.yml</code> - Build multiple exporters (764 lines) \u274c DELETE 6. <code>update-site.yml</code> - Portal update \u2753 MERGE 7. <code>regenerate-portal.yml</code> - Portal rebuild \u2753 MERGE 8. <code>deploy-docs.yml</code> - MkDocs \u2705 KEEP 9. <code>security.yml</code> - Weekly scans \u2705 KEEP 10. <code>security-rebuild.yml</code> - Monthly rebuilds \u2705 KEEP</p>"},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#new-state","title":"New State","text":"<p>8 workflow files (cleaner, no duplication):</p>"},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#21-releaseyml-optimized-single-exporter-build","title":"2.1: <code>release.yml</code> - Optimized single exporter build","text":"<pre><code># Current: 850+ lines with duplication\n# Target: ~400 lines with modular steps\n\njobs:\n  # Detect what to build based on manifest\n  detect:\n    outputs:\n      rpm_matrix: [...] # Only archs in manifest\n      deb_matrix: [...] # Only archs in manifest\n      docker_enabled: true/false\n\n  # Build RPM (parallel)\n  build-rpm:\n    strategy:\n      matrix: ${{ fromJson(needs.detect.outputs.rpm_matrix) }}\n    steps:\n      - Build RPM\n      - Upload to GitHub Releases\n      - Generate artifact JSON\n      - Commit to gh-pages catalog/&lt;exporter&gt;/rpm_&lt;arch&gt;_&lt;dist&gt;.json\n\n  # Build DEB (parallel)\n  build-deb:\n    strategy:\n      matrix: ${{ fromJson(needs.detect.outputs.deb_matrix) }}\n    steps:\n      - Build DEB\n      - Upload to GitHub Releases\n      - Generate artifact JSON\n      - Commit to gh-pages catalog/&lt;exporter&gt;/deb_&lt;arch&gt;_&lt;dist&gt;.json\n\n  # Build Docker (if enabled)\n  build-docker:\n    if: needs.detect.outputs.docker_enabled == 'true'\n    steps:\n      - Build multi-arch image\n      - Push to GHCR\n      - Generate artifact JSON\n      - Commit to gh-pages catalog/&lt;exporter&gt;/docker.json\n\n  # Generate metadata (YUM/APT) - after all builds\n  publish-metadata:\n    needs: [build-rpm, build-deb, build-docker]\n    if: always() &amp;&amp; !cancelled()\n    steps:\n      - Generate YUM metadata (cumulative, from GitHub)\n      - Generate APT metadata (cumulative, from GitHub)\n      - Sign repositories\n\n  # Update portal\n  update-portal:\n    needs: publish-metadata\n    if: always() &amp;&amp; !cancelled()\n    steps:\n      - Aggregate catalog/&lt;exporter&gt;/*.json \u2192 metadata.json\n      - Update catalog/index.json\n      - Regenerate portal HTML\n</code></pre>"},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#22-auto-releaseyml-fixed-change-detection","title":"2.2: <code>auto-release.yml</code> - Fixed change detection","text":"<pre><code># Current: Uses git diff (unreliable)\n# Target: Uses state_manager (reliable)\n\njobs:\n  detect-changes:\n    steps:\n      - name: Detect changed exporters\n        run: |\n          # Use state_manager to compare manifests vs deployed catalog\n          python3 -m core.engine.state_manager \\\n            --detect-changes \\\n            --output changed_exporters.json\n\n      - name: Trigger builds\n        run: |\n          for exporter in $(jq -r '.[]' changed_exporters.json); do\n            gh workflow run release.yml -f exporter=$exporter\n          done\n</code></pre>"},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#23-delete-full-buildyml-replace-with-simple-script","title":"2.3: Delete <code>full-build.yml</code>, replace with simple script","text":"<pre><code># New: scripts/trigger_full_build.sh\n#!/bin/bash\nfor exporter in $(ls exporters/); do\n  echo \"Triggering build for $exporter...\"\n  gh workflow run release.yml -f exporter=$exporter\n  sleep 5  # Rate limit protection\ndone\n\n# Called manually or by workflow_dispatch\n</code></pre>"},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#24-merge-update-siteyml-regenerate-portalyml-portal-updateyml","title":"2.4: Merge <code>update-site.yml</code> + <code>regenerate-portal.yml</code> \u2192 <code>portal-update.yml</code>","text":"<pre><code>name: Portal Update\n\non:\n  workflow_dispatch:\n    inputs:\n      full_rebuild:\n        description: 'Full rebuild from GitHub releases'\n        type: boolean\n        default: false\n  push:\n    paths:\n      - 'core/templates/**'\n      - 'core/engine/**'\n\njobs:\n  update:\n    steps:\n      - if: inputs.full_rebuild\n        name: Full rebuild mode\n        run: |\n          # Scan ALL GitHub releases\n          # Regenerate ALL catalog/&lt;exporter&gt;/metadata.json\n\n      - name: Incremental update\n        run: |\n          # Use existing catalog artifacts\n          # Aggregate and regenerate portal\n</code></pre>"},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#implementation-tasks_1","title":"Implementation Tasks","text":""},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#task-21-refactor-releaseyml","title":"Task 2.1: Refactor release.yml","text":"<ul> <li> Extract duplicate code into reusable bash functions</li> <li> Modularize artifact generation</li> <li> Add proper error handling</li> <li> Reduce from 850 to ~400 lines</li> </ul>"},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#task-22-fix-auto-releaseyml-with-state_manager","title":"Task 2.2: Fix auto-release.yml with state_manager","text":"<ul> <li> Implement <code>state_manager.detect_changes()</code> method</li> <li> Remove git diff logic</li> <li> Test with multiple concurrent changes</li> </ul>"},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#task-23-delete-full-buildyml","title":"Task 2.3: Delete full-build.yml","text":"<ul> <li> Create <code>scripts/trigger_full_build.sh</code></li> <li> Document usage in README</li> <li> Remove workflow file</li> </ul>"},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#task-24-merge-portal-workflows","title":"Task 2.4: Merge portal workflows","text":"<ul> <li> Create new <code>portal-update.yml</code></li> <li> Test full rebuild mode</li> <li> Test incremental mode</li> <li> Delete <code>update-site.yml</code> and <code>regenerate-portal.yml</code></li> </ul>"},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#phase-3-testing-validation-week-3","title":"\ud83e\uddea Phase 3: Testing &amp; Validation (Week 3)","text":""},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#test-strategy","title":"Test Strategy","text":""},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#31-contract-testing","title":"3.1: Contract Testing","text":"<p>Validate JSON schemas for all artifacts: <pre><code># tests/test_artifact_schemas.py\ndef test_rpm_artifact_schema():\n    \"\"\"Validate RPM artifact JSON matches schema\"\"\"\n    artifact = load_json(\"catalog/node_exporter/rpm_amd64_el9.json\")\n    validate_schema(artifact, RPM_ARTIFACT_SCHEMA)\n\ndef test_docker_artifact_schema():\n    \"\"\"Validate Docker artifact JSON matches schema\"\"\"\n    artifact = load_json(\"catalog/node_exporter/docker.json\")\n    validate_schema(artifact, DOCKER_ARTIFACT_SCHEMA)\n\ndef test_metadata_schema():\n    \"\"\"Validate aggregated metadata JSON matches schema\"\"\"\n    metadata = load_json(\"catalog/node_exporter/metadata.json\")\n    validate_schema(metadata, METADATA_SCHEMA)\n</code></pre></p>"},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#32-non-regression-testing","title":"3.2: Non-Regression Testing","text":"<p>Ensure refactoring doesn't break existing functionality: <pre><code># tests/test_non_regression.py\ndef test_portal_generation():\n    \"\"\"Portal generates successfully with new catalog structure\"\"\"\n    generate_portal()\n    assert os.path.exists(\"index.html\")\n    assert validate_html(\"index.html\")\n\ndef test_metadata_generation():\n    \"\"\"YUM/APT metadata generation still works\"\"\"\n    generate_yum_metadata()\n    assert os.path.exists(\"el9/x86_64/repodata/repomd.xml\")\n\ndef test_catalog_aggregation():\n    \"\"\"Catalog aggregation produces correct metadata\"\"\"\n    artifacts = load_artifacts(\"catalog/node_exporter/\")\n    metadata = aggregate_metadata(artifacts)\n    assert metadata[\"exporter\"] == \"node_exporter\"\n    assert len(metadata[\"artifacts\"][\"rpm\"]) &gt; 0\n</code></pre></p>"},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#33-integration-testing","title":"3.3: Integration Testing","text":"<p>Test complete workflow end-to-end: <pre><code># tests/integration/test_full_workflow.sh\n#!/bin/bash\nset -euo pipefail\n\necho \"Testing full workflow...\"\n\n# 1. Trigger build for test exporter\ngh workflow run release.yml -f exporter=node_exporter\n\n# 2. Wait for completion\ngh run watch\n\n# 3. Validate artifacts\ntest -f \"catalog/node_exporter/rpm_amd64_el9.json\"\ntest -f \"catalog/node_exporter/docker.json\"\n\n# 4. Validate portal\ncurl -s https://sckyzo.github.io/monitoring-hub/ | grep \"node_exporter\"\n\necho \"\u2705 Integration test passed\"\n</code></pre></p>"},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#implementation-tasks_2","title":"Implementation Tasks","text":""},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#task-31-add-contract-tests","title":"Task 3.1: Add contract tests","text":"<ul> <li> Define JSON schemas (jsonschema)</li> <li> Write validation tests</li> <li> Run in CI on every PR</li> </ul>"},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#task-32-add-non-regression-tests","title":"Task 3.2: Add non-regression tests","text":"<ul> <li> Portal generation tests</li> <li> Metadata generation tests</li> <li> Catalog aggregation tests</li> </ul>"},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#task-33-add-integration-tests","title":"Task 3.3: Add integration tests","text":"<ul> <li> End-to-end workflow test</li> <li> Add to CI pipeline</li> <li> Document test procedures</li> </ul>"},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#phase-4-documentation-week-3","title":"\ud83d\udcdd Phase 4: Documentation (Week 3)","text":""},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#documentation-updates","title":"Documentation Updates","text":""},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#task-41-update-architecture-docs","title":"Task 4.1: Update architecture docs","text":"<ul> <li> <code>docs/architecture/catalog-structure.md</code> - New catalog format</li> <li> <code>docs/architecture/workflows.md</code> - Simplified workflows</li> <li> <code>docs/architecture/data-flow.md</code> - Data flow diagrams</li> </ul>"},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#task-42-update-user-documentation","title":"Task 4.2: Update user documentation","text":"<ul> <li> <code>README.md</code> - Quick start with new structure</li> <li> <code>docs/user-guide/adding-exporters.md</code> - Updated manifest guide</li> <li> <code>docs/user-guide/troubleshooting.md</code> - New error scenarios</li> </ul>"},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#task-43-update-developer-documentation","title":"Task 4.3: Update developer documentation","text":"<ul> <li> <code>docs/contributing/testing.md</code> - Test procedures</li> <li> <code>docs/api-reference/catalog-api.md</code> - Catalog JSON schemas</li> <li> <code>CLAUDE.md</code> - Reference new architecture</li> </ul>"},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#migration-strategy","title":"\ud83d\ude80 Migration Strategy","text":""},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#backward-compatibility","title":"Backward Compatibility","text":"<p>During transition (both systems coexist): <pre><code># site_generator.py\nif os.path.exists(f\"catalog/{exporter}/metadata.json\"):\n    # New system: Read aggregated metadata\n    metadata = load_json(f\"catalog/{exporter}/metadata.json\")\nelse:\n    # Legacy system: Aggregate from release_urls.json\n    metadata = aggregate_legacy(exporter)\n</code></pre></p> <p>After transition (3-4 weeks): - Remove legacy code paths - Clean up old JSON files on gh-pages - Update all documentation</p>"},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#rollback-plan","title":"Rollback Plan","text":"<p>If critical issues arise: 1. Revert to previous commit on main 2. Keep refactor branch for continued work 3. Fix issues before retry</p>"},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#success-criteria","title":"\u2705 Success Criteria","text":""},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#must-have","title":"Must Have","text":"<ul> <li> No race conditions in artifact writes</li> <li> Atomic operations (1 job = 1 file)</li> <li> All 34 exporters build successfully</li> <li> Portal displays correct data</li> <li> YUM/APT repositories work correctly</li> <li> All tests pass in CI</li> </ul>"},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#nice-to-have","title":"Nice to Have","text":"<ul> <li> 50% reduction in workflow code duplication</li> <li> 90% test coverage on core modules</li> <li> Sub-5min build time for single exporter</li> <li> Comprehensive error messages</li> </ul>"},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#timeline","title":"\ud83d\udcca Timeline","text":"Week Phase Deliverables 1 Granular Catalog New JSON structure, artifact generators, site_generator refactor 2 Workflow Simplification Optimized release.yml, merged portal workflows, deleted full-build 3 Testing &amp; Docs Contract tests, integration tests, updated documentation <p>Total: 2-3 weeks for complete refactoring</p>"},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#development-notes","title":"\ud83d\udd27 Development Notes","text":""},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#key-design-decisions","title":"Key Design Decisions","text":"<ol> <li>Granular artifacts over monolithic: Each job writes its own file to eliminate race conditions</li> <li>Aggregation at read time: Portal aggregates artifacts on-demand instead of at build time</li> <li>GitHub as source of truth: Metadata generation scans GitHub releases (cumulative)</li> <li>Sequential triggers over parallel batches: Simpler, respects GitHub limits</li> <li>Test-first approach: Write tests before refactoring to catch regressions</li> </ol>"},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#trade-offs","title":"Trade-offs","text":"Decision Benefit Cost Granular files No race conditions More files to manage Aggregation at read Simpler writes Slower portal generation Sequential builds Stable, predictable Slower for batch updates Contract testing Catch schema breaks Maintenance overhead"},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#contact-support","title":"\ud83d\udcde Contact &amp; Support","text":"<p>Questions? Open an issue or discussion in the repository.</p> <p>Progress tracking: See GitHub Project board for task status.</p>"},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#completion-notes-february-2026","title":"\u2705 Completion Notes (February 2026)","text":"<p>Status: Phases 1-3 Complete Branch: <code>refactor/v2-architecture</code> (ready for merge) Completion Date: 2026-02-13</p>"},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#phase-1-granular-catalog-architecture","title":"Phase 1: Granular Catalog Architecture \u2705","text":"<p>All tasks completed:</p> <ul> <li>\u2705 Task 1.1: Created artifact generators</li> <li><code>core/scripts/generate_artifact_metadata.py</code> (290 lines)</li> <li><code>core/scripts/aggregate_catalog_metadata.py</code> (270 lines)</li> <li> <p><code>core/scripts/publish_artifact_metadata.sh</code> (160 lines)</p> </li> <li> <p>\u2705 Task 1.2: Modified release.yml for atomic writes</p> </li> <li>RPM job: Publishes <code>catalog/&lt;exporter&gt;/rpm_&lt;arch&gt;_&lt;dist&gt;.json</code></li> <li>DEB job: Publishes <code>catalog/&lt;exporter&gt;/deb_&lt;arch&gt;_&lt;dist&gt;.json</code></li> <li>Docker job: Publishes <code>catalog/&lt;exporter&gt;/docker.json</code></li> <li> <p>Removed: Legacy artifact uploads (release_urls.json, build-info.json)</p> </li> <li> <p>\u2705 Task 1.3: Created site_generator_v2.py</p> </li> <li>Reads granular catalog structure</li> <li>On-demand aggregation of metadata</li> <li>Backward compatibility with legacy format</li> <li> <p>320 lines, clean separation of concerns</p> </li> <li> <p>\u2705 Task 1.4: Updated publish-metadata job</p> </li> <li>Removed legacy artifact downloads</li> <li>Uses cumulative GitHub scanning for repo metadata</li> <li>Calls site_generator_v2 with V3 catalog support</li> </ul> <p>Result: 100% atomic operations - zero race conditions possible</p>"},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#phase-2-workflow-simplification","title":"Phase 2: Workflow Simplification \u2705","text":"<p>All tasks completed:</p> <ul> <li>\u2705 Task 2.1: Consolidated portal workflows</li> <li>Merged: <code>update-site.yml</code> + <code>regenerate-portal.yml</code> \u2192 <code>update-portal.yml</code></li> <li>Single workflow with skip-catalog option</li> <li> <p>Unified concurrency group (portal-update)</p> </li> <li> <p>\u2705 Task 2.2: Updated auto-release.yml</p> </li> <li>Replaced git diff with state_manager.py</li> <li>Version-based change detection (manifest vs catalog)</li> <li>More robust than file-based diff</li> <li> <p>Handles reverts and force pushes correctly</p> </li> <li> <p>\u2705 Task 2.3: Simplified build-pr.yml</p> </li> <li>Added detect-changes job (finds modified exporters)</li> <li>Added validate-manifests job (schema + URL validation)</li> <li>Removed unused artifact uploads</li> <li> <p>Added comprehensive summary job</p> </li> <li> <p>\u2705 Task 2.4: Updated full-build.yml</p> </li> <li>Uses site_generator_v2 instead of site_generator</li> <li>Maintains legacy artifact support during transition</li> </ul> <p>Result: Reduced workflow duplication, better change detection, clearer job structure</p>"},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#phase-3-testing-validation","title":"Phase 3: Testing &amp; Validation \u2705","text":"<p>All tasks completed:</p> <ul> <li>\u2705 Task 3.1: JSON schema validation tests</li> <li><code>tests/test_artifact_schemas.py</code> (330 lines)</li> <li>Tests for RPM, DEB, Docker artifact schemas</li> <li>Aggregated metadata schema validation</li> <li> <p>Format versioning and backward compatibility</p> </li> <li> <p>\u2705 Task 3.2: Aggregation logic tests</p> </li> <li><code>tests/test_aggregation.py</code> (420 lines)</li> <li>Tests artifact loading, aggregation, status computation</li> <li>Tests build date tracking</li> <li> <p>Full metadata aggregation workflow</p> </li> <li> <p>\u2705 Task 3.3: Portal generation tests</p> </li> <li><code>tests/test_site_generator.py</code> (180 lines)</li> <li>V3 to legacy format conversion</li> <li>Architecture mapping validation</li> <li> <p>Edge case handling</p> </li> <li> <p>\u2705 Task 3.4: Docker test infrastructure</p> </li> <li>Created <code>Dockerfile.test</code> (isolated test environment)</li> <li>Updated <code>Dockerfile.dev</code> (added rpm, dpkg-dev, pytest)</li> <li>Created <code>docker-compose.yml</code> (dev, test, test-cov services)</li> <li>All tests run in containers (no local dependencies)</li> </ul> <p>Result: 930+ lines of tests, 100% container-based testing</p>"},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#statistics","title":"Statistics","text":"<p>Files Created: 10 - Core scripts: 3 (generate_artifact_metadata.py, aggregate_catalog_metadata.py, publish_artifact_metadata.sh) - Engine: 1 (site_generator_v2.py) - Tests: 3 (test_artifact_schemas.py, test_aggregation.py, test_site_generator.py) - Workflows: 1 (update-portal.yml) - Docker: 2 (Dockerfile.test, docker-compose.yml)</p> <p>Files Modified: 6 - Workflows: 4 (release.yml, auto-release.yml, build-pr.yml, full-build.yml) - Docker: 1 (Dockerfile.dev) - Requirements: 1 (test.txt added)</p> <p>Files Deleted: 2 - Workflows: 2 (update-site.yml, regenerate-portal.yml)</p> <p>Total Commits: 11 on branch <code>refactor/v2-architecture</code></p> <p>Lines of Code: - Production code: ~1,400 lines - Test code: ~930 lines - Test coverage: Core modules covered with contract tests</p>"},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#success-metrics","title":"Success Metrics","text":"<p>\u2705 Zero race conditions: Each job writes only its own file \u2705 Atomic operations: All writes are independent and atomic \u2705 Simplified workflows: Reduced duplication, clearer structure \u2705 Comprehensive tests: Schema validation, aggregation logic, conversion \u2705 Docker-first testing: All tests run in containers \u2705 Backward compatible: Legacy clients can still read catalog</p>"},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#next-steps-phase-4","title":"Next Steps (Phase 4)","text":"<p>Phase 4 tasks remaining: - Task 4.1: Update API documentation (catalog structure, endpoints) - Task 4.2: Update workflow documentation in docs/architecture/ - Task 4.3: Update CLAUDE.md with V3 patterns - Task 4.4: Update README.md with new catalog structure - Task 4.5: Create migration guide for V3 format</p> <p>Merge Strategy: 1. Complete Phase 4 documentation updates 2. Run full CI pipeline on refactor/v2-architecture 3. Create pull request to main 4. Review and merge 5. Monitor first production deployment</p>"},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#lessons-learned","title":"Lessons Learned","text":"<p>What Worked Well: - Atomic file writes eliminated all race conditions - State manager is much more reliable than git diff - Docker-based testing ensures consistency - Contract tests catch breaking changes early - Comprehensive planning prevented scope creep</p> <p>What Could Be Improved: - Full-build.yml still uses legacy artifacts (acceptable during transition) - Portal aggregation could be cached for performance - More integration tests for end-to-end workflows</p>"},{"location":"architecture/ARCHIVED-refactoring-v2-plan/#recommendations","title":"Recommendations","text":"<p>For Future Refactoring: 1. Always use atomic operations for concurrent writes 2. Test-first approach catches regressions early 3. Document architectural decisions as you go 4. Use state managers over file-based change detection 5. Container-based testing ensures reproducibility</p> <p>Maintenance Notes: - V3 catalog format is now standard - Legacy format maintained for backward compatibility - Monitor gh-pages for any metadata issues - Consider adding performance benchmarks for aggregation</p>"},{"location":"architecture/ARCHIVED-v3-migration-guide/","title":"V3 Migration Guide","text":"<p>This guide helps you migrate from V2 (legacy) catalog format to V3 (granular) architecture.</p>"},{"location":"architecture/ARCHIVED-v3-migration-guide/#overview","title":"Overview","text":""},{"location":"architecture/ARCHIVED-v3-migration-guide/#what-changed-in-v3","title":"What Changed in V3?","text":"<p>V2 (Legacy): - Single <code>catalog.json</code> file updated by all jobs - Race conditions in parallel builds - Monolithic artifact metadata - Write-time aggregation</p> <p>V3 (Granular): - Atomic artifact files: <code>catalog/&lt;exporter&gt;/rpm_&lt;arch&gt;_&lt;dist&gt;.json</code> - No race conditions (1 job = 1 file) - Granular per-artifact metadata - Read-time aggregation</p>"},{"location":"architecture/ARCHIVED-v3-migration-guide/#why-migrate","title":"Why Migrate?","text":"<p>\u2705 Reliability: No race conditions in parallel builds \u2705 Scalability: Hundreds of parallel jobs without conflicts \u2705 Observability: Individual artifact status tracking \u2705 Performance: Faster builds (no catalog.json lock) \u2705 Maintainability: Clear separation of concerns</p>"},{"location":"architecture/ARCHIVED-v3-migration-guide/#migration-checklist","title":"Migration Checklist","text":""},{"location":"architecture/ARCHIVED-v3-migration-guide/#for-repository-maintainers","title":"For Repository Maintainers","text":"<ul> <li> Phase 1: Implement V3 catalog architecture (DONE)</li> <li> Create generate_artifact_metadata.py</li> <li> Create aggregate_catalog_metadata.py</li> <li> Create publish_artifact_metadata.sh</li> <li> <p> Create site_generator_v2.py</p> </li> <li> <p> Phase 2: Workflow simplification (DONE)</p> </li> <li> Update release.yml with atomic writes</li> <li> Update auto-release.yml with state_manager</li> <li> Consolidate update-portal.yml</li> <li> Update build-pr.yml with detect-changes</li> <li> <p> Update full-build.yml with site_generator_v2</p> </li> <li> <p> Phase 3: Testing and validation (DONE)</p> </li> <li> Add test_artifact_schemas.py</li> <li> Add test_aggregation.py</li> <li> Add test_site_generator.py</li> <li> <p> Create Docker-based test infrastructure</p> </li> <li> <p> Phase 4: Documentation updates (IN PROGRESS)</p> </li> <li> Update CLAUDE.md with V3 patterns</li> <li> Update README.md with V3 architecture</li> <li> Create catalog-v3.md API reference</li> <li> Update ci-cd.md workflow documentation</li> <li> <p> Create v3-migration-guide.md (this file)</p> </li> <li> <p> Phase 5: Deployment and monitoring</p> </li> <li> Deploy to production (merge to main)</li> <li> Monitor first V3 builds</li> <li> Verify backward compatibility</li> <li> Clean up V2 legacy code (optional)</li> </ul>"},{"location":"architecture/ARCHIVED-v3-migration-guide/#for-portal-consumers","title":"For Portal Consumers","text":"<p>If you consume the catalog programmatically, follow this guide to migrate your code.</p>"},{"location":"architecture/ARCHIVED-v3-migration-guide/#for-fork-maintainers","title":"For Fork Maintainers","text":"<p>If you maintain a fork, see the \"Migrating Your Fork\" section below.</p>"},{"location":"architecture/ARCHIVED-v3-migration-guide/#code-migration-examples","title":"Code Migration Examples","text":""},{"location":"architecture/ARCHIVED-v3-migration-guide/#python-reading-catalog","title":"Python: Reading Catalog","text":""},{"location":"architecture/ARCHIVED-v3-migration-guide/#before-v3-legacy-format","title":"Before V3 (Legacy Format)","text":"<pre><code>import requests\n\n# Read monolithic catalog.json\ncatalog_url = \"https://sckyzo.github.io/monitoring-hub/catalog.json\"\ncatalog = requests.get(catalog_url).json()\n\n# Iterate exporters\nfor exporter in catalog[\"exporters\"]:\n    name = exporter[\"name\"]\n    version = exporter[\"version\"]\n    rpm_status = exporter[\"rpm_status\"]\n\n    # Check availability\n    if \"el9\" in exporter.get(\"availability\", {}):\n        rpm_url = exporter[\"availability\"][\"el9\"][\"x86_64\"][\"path\"]\n        print(f\"{name} v{version}: {rpm_url}\")\n</code></pre>"},{"location":"architecture/ARCHIVED-v3-migration-guide/#after-v3-granular-format","title":"After V3 (Granular Format)","text":"<p>Option 1: Use aggregated metadata.json (Recommended)</p> <pre><code>import requests\n\n# Read exporter-specific metadata (aggregated)\nmetadata_url = \"https://sckyzo.github.io/monitoring-hub/catalog/node_exporter/metadata.json\"\nmetadata = requests.get(metadata_url).json()\n\n# Check format version\nif metadata[\"format_version\"] != \"3.0\":\n    raise ValueError(\"Unexpected format version\")\n\n# Access artifact data\nrpm_status = metadata[\"status\"][\"rpm\"]\nrpm_artifacts = metadata[\"artifacts\"][\"rpm\"]\n\n# Get specific artifact\nif \"el9\" in rpm_artifacts and \"amd64\" in rpm_artifacts[\"el9\"]:\n    artifact = rpm_artifacts[\"el9\"][\"amd64\"]\n    rpm_url = artifact[\"url\"]\n    print(f\"{metadata['exporter']} v{metadata['version']}: {rpm_url}\")\n</code></pre> <p>Option 2: Read granular artifacts directly (Advanced)</p> <pre><code>import requests\n\n# Read specific artifact file\nartifact_url = \"https://sckyzo.github.io/monitoring-hub/catalog/node_exporter/rpm_amd64_el9.json\"\nartifact = requests.get(artifact_url).json()\n\n# Validate format\nassert artifact[\"format_version\"] == \"3.0\"\nassert artifact[\"artifact_type\"] == \"rpm\"\n\n# Access package details\nif artifact[\"status\"] == \"success\":\n    package = artifact[\"package\"]\n    print(f\"Filename: {package['filename']}\")\n    print(f\"URL: {package['url']}\")\n    print(f\"SHA256: {package['sha256']}\")\n    print(f\"Size: {package['size_bytes']} bytes\")\n</code></pre>"},{"location":"architecture/ARCHIVED-v3-migration-guide/#javascripttypescript-portal-integration","title":"JavaScript/TypeScript: Portal Integration","text":""},{"location":"architecture/ARCHIVED-v3-migration-guide/#before-v3-legacy-format_1","title":"Before V3 (Legacy Format)","text":"<pre><code>// Fetch catalog\nconst response = await fetch('https://sckyzo.github.io/monitoring-hub/catalog.json');\nconst catalog = await response.json();\n\n// Display exporters\ncatalog.exporters.forEach(exporter =&gt; {\n  const statusBadge = exporter.rpm_status === 'success' ? '\u2705' : '\u274c';\n  console.log(`${statusBadge} ${exporter.name} v${exporter.version}`);\n\n  // Show availability\n  if (exporter.availability?.el9?.x86_64) {\n    console.log(`  RPM: ${exporter.availability.el9.x86_64.path}`);\n  }\n});\n</code></pre>"},{"location":"architecture/ARCHIVED-v3-migration-guide/#after-v3-granular-format_1","title":"After V3 (Granular Format)","text":"<pre><code>interface V3Metadata {\n  format_version: string;\n  exporter: string;\n  version: string;\n  category: string;\n  description: string;\n  last_updated: string;\n  artifacts: {\n    rpm: Record&lt;string, Record&lt;string, ArtifactInfo&gt;&gt;;\n    deb: Record&lt;string, Record&lt;string, ArtifactInfo&gt;&gt;;\n    docker: DockerInfo;\n  };\n  status: {\n    rpm: ArtifactStatus;\n    deb: ArtifactStatus;\n    docker: ArtifactStatus;\n  };\n}\n\ninterface ArtifactInfo {\n  status: ArtifactStatus;\n  url: string;\n  size_bytes: number;\n  sha256: string;\n}\n\ntype ArtifactStatus = 'success' | 'failed' | 'pending' | 'na';\n\n// Fetch exporter metadata\nasync function loadExporterMetadata(exporterName: string): Promise&lt;V3Metadata&gt; {\n  const url = `https://sckyzo.github.io/monitoring-hub/catalog/${exporterName}/metadata.json`;\n  const response = await fetch(url);\n  const metadata: V3Metadata = await response.json();\n\n  // Validate format version\n  if (metadata.format_version !== '3.0') {\n    throw new Error(`Unsupported format version: ${metadata.format_version}`);\n  }\n\n  return metadata;\n}\n\n// Display exporter\nasync function displayExporter(exporterName: string) {\n  const metadata = await loadExporterMetadata(exporterName);\n\n  const statusBadge = metadata.status.rpm === 'success' ? '\u2705' : '\u274c';\n  console.log(`${statusBadge} ${metadata.exporter} v${metadata.version}`);\n\n  // Show availability\n  const rpmArtifacts = metadata.artifacts.rpm;\n  if (rpmArtifacts.el9?.amd64) {\n    console.log(`  RPM (el9/amd64): ${rpmArtifacts.el9.amd64.url}`);\n  }\n}\n</code></pre>"},{"location":"architecture/ARCHIVED-v3-migration-guide/#bash-downloading-artifacts","title":"Bash: Downloading Artifacts","text":""},{"location":"architecture/ARCHIVED-v3-migration-guide/#before-v3-legacy-format_2","title":"Before V3 (Legacy Format)","text":"<pre><code>#!/bin/bash\nset -euo pipefail\n\nCATALOG_URL=\"https://sckyzo.github.io/monitoring-hub/catalog.json\"\nEXPORTER_NAME=\"node_exporter\"\n\n# Download and parse catalog\ncatalog=$(curl -fsSL \"$CATALOG_URL\")\n\n# Extract RPM URL (requires jq)\nrpm_url=$(echo \"$catalog\" | jq -r \\\n  \".exporters[] | select(.name == \\\"$EXPORTER_NAME\\\") | .availability.el9.x86_64.path\")\n\n# Download RPM\ncurl -fsSL -O \"$rpm_url\"\n</code></pre>"},{"location":"architecture/ARCHIVED-v3-migration-guide/#after-v3-granular-format_2","title":"After V3 (Granular Format)","text":"<pre><code>#!/bin/bash\nset -euo pipefail\n\nCATALOG_BASE=\"https://sckyzo.github.io/monitoring-hub/catalog\"\nEXPORTER_NAME=\"node_exporter\"\nARCH=\"amd64\"\nDIST=\"el9\"\n\n# Option 1: Use aggregated metadata\nmetadata_url=\"${CATALOG_BASE}/${EXPORTER_NAME}/metadata.json\"\nmetadata=$(curl -fsSL \"$metadata_url\")\n\n# Extract RPM URL\nrpm_url=$(echo \"$metadata\" | jq -r \\\n  \".artifacts.rpm.${DIST}.${ARCH}.url\")\n\n# Download RPM\ncurl -fsSL -O \"$rpm_url\"\n\n# Option 2: Use granular artifact directly\nartifact_url=\"${CATALOG_BASE}/${EXPORTER_NAME}/rpm_${ARCH}_${DIST}.json\"\nartifact=$(curl -fsSL \"$artifact_url\")\n\n# Extract package details\nrpm_url=$(echo \"$artifact\" | jq -r '.package.url')\nsha256=$(echo \"$artifact\" | jq -r '.package.sha256')\n\n# Download and verify\ncurl -fsSL -O \"$rpm_url\"\nfilename=$(basename \"$rpm_url\")\necho \"${sha256}  ${filename}\" | sha256sum -c\n</code></pre>"},{"location":"architecture/ARCHIVED-v3-migration-guide/#cicd-migration","title":"CI/CD Migration","text":""},{"location":"architecture/ARCHIVED-v3-migration-guide/#github-actions-artifact-publishing","title":"GitHub Actions: Artifact Publishing","text":""},{"location":"architecture/ARCHIVED-v3-migration-guide/#before-v3-legacy-race-conditions","title":"Before V3 (Legacy - Race Conditions!)","text":"<pre><code># \u26a0\ufe0f DANGER: Multiple jobs write to same file\n- name: Update catalog (UNSAFE)\n  run: |\n    # Clone gh-pages\n    git clone --depth=1 -b gh-pages https://github.com/user/repo.git gh-pages\n    cd gh-pages\n\n    # Update catalog.json (RACE CONDITION!)\n    jq '.exporters += [{\"name\": \"node_exporter\", ...}]' catalog.json &gt; tmp\n    mv tmp catalog.json\n\n    # Commit and push (can fail with conflicts)\n    git add catalog.json\n    git commit -m \"Update catalog\"\n    git push\n</code></pre>"},{"location":"architecture/ARCHIVED-v3-migration-guide/#after-v3-atomic-writes-safe","title":"After V3 (Atomic Writes - Safe!)","text":"<pre><code># \u2705 SAFE: Each job writes exactly 1 file\n- name: Generate artifact metadata\n  run: |\n    python3 core/scripts/generate_artifact_metadata.py \\\n      --type rpm \\\n      --exporter node_exporter \\\n      --version 1.10.2 \\\n      --arch amd64 \\\n      --dist el9 \\\n      --filename node_exporter-1.10.2-1.el9.x86_64.rpm \\\n      --url https://example.com/node_exporter.rpm \\\n      --sha256 abc123... \\\n      --size 12345678 \\\n      --status success \\\n      --output catalog/node_exporter/rpm_amd64_el9.json\n\n- name: Publish artifact metadata\n  run: |\n    bash core/scripts/publish_artifact_metadata.sh \\\n      catalog/node_exporter/rpm_amd64_el9.json\n</code></pre>"},{"location":"architecture/ARCHIVED-v3-migration-guide/#gitlab-ci-migration-example","title":"GitLab CI: Migration Example","text":""},{"location":"architecture/ARCHIVED-v3-migration-guide/#before-v3","title":"Before V3","text":"<pre><code>publish-catalog:\n  stage: deploy\n  script:\n    # Download catalog (sequential bottleneck)\n    - wget https://example.com/catalog.json\n\n    # Update catalog (not atomic)\n    - jq '.exporters += [...]' catalog.json &gt; tmp\n    - mv tmp catalog.json\n\n    # Upload (can fail with concurrent builds)\n    - rsync -avz catalog.json user@server:/var/www/\n</code></pre>"},{"location":"architecture/ARCHIVED-v3-migration-guide/#after-v3","title":"After V3","text":"<pre><code>publish-rpm-metadata:\n  stage: deploy\n  script:\n    # Generate granular artifact file\n    - python3 generate_artifact_metadata.py \\\n        --type rpm --arch amd64 --dist el9 \\\n        --output rpm_amd64_el9.json\n\n    # Upload atomic file (parallel safe)\n    - rsync -avz rpm_amd64_el9.json \\\n        user@server:/var/www/catalog/node_exporter/\n\n# Separate job for aggregation (runs after all artifacts)\naggregate-metadata:\n  stage: finalize\n  needs: [publish-rpm-metadata, publish-deb-metadata, publish-docker-metadata]\n  script:\n    - python3 aggregate_catalog_metadata.py \\\n        --exporter node_exporter \\\n        --output metadata.json\n    - rsync -avz metadata.json \\\n        user@server:/var/www/catalog/node_exporter/\n</code></pre>"},{"location":"architecture/ARCHIVED-v3-migration-guide/#migrating-your-fork","title":"Migrating Your Fork","text":"<p>If you maintain a fork of Monitoring Hub, follow these steps:</p>"},{"location":"architecture/ARCHIVED-v3-migration-guide/#step-1-merge-v3-branch","title":"Step 1: Merge V3 Branch","text":"<pre><code># Add upstream remote (if not already added)\ngit remote add upstream https://github.com/SckyzO/monitoring-hub.git\n\n# Fetch V3 branch\ngit fetch upstream refactor/v2-architecture\n\n# Merge into your main branch\ngit checkout main\ngit merge upstream/refactor/v2-architecture\n\n# Resolve conflicts (if any)\n# Common conflicts: .github/workflows/, core/scripts/, docs/\n\n# Test locally\n./devctl test\n./devctl ci\n\n# Push to your fork\ngit push origin main\n</code></pre>"},{"location":"architecture/ARCHIVED-v3-migration-guide/#step-2-update-secrets","title":"Step 2: Update Secrets","text":"<p>V3 uses the same secrets as V2, no changes required:</p> <ul> <li><code>GPG_PRIVATE_KEY</code> - GPG signing key</li> <li><code>GPG_PASSPHRASE</code> - GPG key passphrase</li> <li><code>GPG_KEY_ID</code> - GPG key ID</li> <li><code>GITHUB_TOKEN</code> - GitHub API token (automatic)</li> </ul>"},{"location":"architecture/ARCHIVED-v3-migration-guide/#step-3-update-workflow-permissions","title":"Step 3: Update Workflow Permissions","text":"<p>Ensure <code>gh-pages</code> branch has write permissions:</p> <pre><code># .github/workflows/release.yml\npermissions:\n  contents: write\n  packages: write\n</code></pre>"},{"location":"architecture/ARCHIVED-v3-migration-guide/#step-4-test-first-build","title":"Step 4: Test First Build","text":"<p>Trigger a manual build to verify V3 works:</p> <ol> <li>Go to Actions \u2192 auto-release.yml \u2192 Run workflow</li> <li>Specify a single exporter (e.g., <code>node_exporter</code>)</li> <li>Monitor build logs for atomic metadata publishing</li> <li>Verify catalog structure on gh-pages:    <pre><code>catalog/node_exporter/\n\u251c\u2500\u2500 rpm_amd64_el9.json\n\u251c\u2500\u2500 deb_amd64_ubuntu-22.04.json\n\u251c\u2500\u2500 docker.json\n\u2514\u2500\u2500 metadata.json\n</code></pre></li> </ol>"},{"location":"architecture/ARCHIVED-v3-migration-guide/#step-5-enable-v3-for-all-exporters","title":"Step 5: Enable V3 for All Exporters","text":"<p>Once verified, enable V3 for all exporters:</p> <ol> <li>Merge V3 branch to main</li> <li>Push to your fork</li> <li>V3 workflows will automatically handle all builds</li> </ol>"},{"location":"architecture/ARCHIVED-v3-migration-guide/#backward-compatibility","title":"Backward Compatibility","text":""},{"location":"architecture/ARCHIVED-v3-migration-guide/#portal-compatibility","title":"Portal Compatibility","text":"<p>V3 maintains backward compatibility with V2 consumers:</p> <pre><code># Legacy code still works!\ncatalog_url = \"https://sckyzo.github.io/monitoring-hub/catalog.json\"\ncatalog = requests.get(catalog_url).json()\n\n# V3 automatically converts granular artifacts to legacy format\nfor exporter in catalog[\"exporters\"]:\n    print(exporter[\"name\"], exporter[\"version\"])\n</code></pre> <p>How? The <code>site_generator_v2.py</code> script: 1. Reads V3 granular artifacts 2. Aggregates metadata 3. Converts to V2 legacy format 4. Generates backward-compatible <code>catalog.json</code></p>"},{"location":"architecture/ARCHIVED-v3-migration-guide/#api-compatibility-matrix","title":"API Compatibility Matrix","text":"Consumer V2 Catalog V3 Catalog Status Legacy portal (reads catalog.json) \u2705 \u2705 Fully compatible V3-aware portal (reads metadata.json) \u274c \u2705 V3 only Direct artifact consumers \u274c \u2705 V3 only Package managers (YUM/APT) \u2705 \u2705 No changes Container registries (ghcr.io) \u2705 \u2705 No changes"},{"location":"architecture/ARCHIVED-v3-migration-guide/#deprecation-timeline","title":"Deprecation Timeline","text":"<p>V2 legacy format support:</p> <ul> <li>Q1 2026: V3 release (backward compatible)</li> <li>Q2-Q3 2026: V3 stable period, V2 format maintained</li> <li>Q4 2026: V2 format deprecation warning</li> <li>Q1 2027: V2 format removal (V3 only)</li> </ul> <p>Consumers have 1 year to migrate to V3 format.</p>"},{"location":"architecture/ARCHIVED-v3-migration-guide/#troubleshooting","title":"Troubleshooting","text":""},{"location":"architecture/ARCHIVED-v3-migration-guide/#issue-race-condition-errors-in-logs","title":"Issue: Race condition errors in logs","text":"<p>Symptoms: <pre><code>Error: failed to push some refs to 'https://github.com/user/repo.git'\nhint: Updates were rejected because the tip of your current branch is behind\n</code></pre></p> <p>Cause: Multiple jobs writing to same file (V2 behavior)</p> <p>Solution: Ensure all jobs use V3 atomic writes (generate_artifact_metadata.py)</p>"},{"location":"architecture/ARCHIVED-v3-migration-guide/#issue-missing-metadatajson-files","title":"Issue: Missing metadata.json files","text":"<p>Symptoms: <pre><code>FileNotFoundError: catalog/node_exporter/metadata.json not found\n</code></pre></p> <p>Cause: Aggregation step not run</p> <p>Solution: <pre><code># Run aggregation manually\npython3 core/scripts/aggregate_catalog_metadata.py \\\n  --exporter node_exporter \\\n  --catalog-dir catalog \\\n  --manifest-path exporters/node_exporter/manifest.yaml \\\n  --output catalog/node_exporter/metadata.json\n</code></pre></p> <p>Or trigger update-portal.yml workflow to aggregate all exporters.</p>"},{"location":"architecture/ARCHIVED-v3-migration-guide/#issue-format-version-mismatch","title":"Issue: Format version mismatch","text":"<p>Symptoms: <pre><code>ValueError: Expected format_version 3.0, got 2.0\n</code></pre></p> <p>Cause: Old artifact file from V2 build</p> <p>Solution: 1. Force rebuild exporter: <code>./devctl test-exporter node_exporter</code> 2. Trigger full-build.yml workflow 3. Clean gh-pages branch and rebuild all</p>"},{"location":"architecture/ARCHIVED-v3-migration-guide/#issue-catalogjson-still-shows-old-versions","title":"Issue: Catalog.json still shows old versions","text":"<p>Symptoms: Portal displays outdated versions</p> <p>Cause: Portal cache or aggregation not run</p> <p>Solution: 1. Trigger update-portal.yml workflow 2. Clear browser cache 3. Verify metadata.json files are up to date</p>"},{"location":"architecture/ARCHIVED-v3-migration-guide/#testing-v3-locally","title":"Testing V3 Locally","text":""},{"location":"architecture/ARCHIVED-v3-migration-guide/#option-1-docker-based-testing","title":"Option 1: Docker-based testing","text":"<pre><code># Run full test suite\n./devctl test\n\n# Run with coverage\n./devctl test-cov\n\n# Test specific file\n./devctl test tests/test_artifact_schemas.py\n</code></pre>"},{"location":"architecture/ARCHIVED-v3-migration-guide/#option-2-manual-artifact-generation","title":"Option 2: Manual artifact generation","text":"<pre><code># Generate test artifact\npython3 core/scripts/generate_artifact_metadata.py \\\n  --type rpm \\\n  --exporter test_exporter \\\n  --version 1.0.0 \\\n  --arch amd64 \\\n  --dist el9 \\\n  --filename test.rpm \\\n  --url https://example.com/test.rpm \\\n  --sha256 abc123 \\\n  --size 1234 \\\n  --status success \\\n  --output /tmp/test_rpm.json\n\n# Validate format\ncat /tmp/test_rpm.json | jq .\n\n# Aggregate test catalog\nmkdir -p /tmp/test_catalog/test_exporter\ncp /tmp/test_rpm.json /tmp/test_catalog/test_exporter/\n\npython3 core/scripts/aggregate_catalog_metadata.py \\\n  --exporter test_exporter \\\n  --catalog-dir /tmp/test_catalog \\\n  --manifest-path exporters/node_exporter/manifest.yaml \\\n  --output /tmp/test_catalog/test_exporter/metadata.json\n\n# Verify aggregation\ncat /tmp/test_catalog/test_exporter/metadata.json | jq .\n</code></pre>"},{"location":"architecture/ARCHIVED-v3-migration-guide/#option-3-end-to-end-workflow-test","title":"Option 3: End-to-end workflow test","text":"<pre><code># Test single exporter build\n./devctl test-exporter node_exporter\n\n# Verify V3 artifacts generated\nls -la build/node_exporter/\n\n# Generate portal locally\npython3 core/engine/site_generator_v2.py \\\n  --catalog-dir build/catalog \\\n  --output-dir build/portal\n\n# Serve portal locally\ncd build/portal &amp;&amp; python3 -m http.server 8000\n# Open http://localhost:8000 in browser\n</code></pre>"},{"location":"architecture/ARCHIVED-v3-migration-guide/#faq","title":"FAQ","text":""},{"location":"architecture/ARCHIVED-v3-migration-guide/#q-do-i-need-to-update-my-manifestyaml-files","title":"Q: Do I need to update my manifest.yaml files?","text":"<p>A: No, manifests are unchanged. V3 only changes internal catalog structure.</p>"},{"location":"architecture/ARCHIVED-v3-migration-guide/#q-will-my-existing-packages-still-work","title":"Q: Will my existing packages still work?","text":"<p>A: Yes, package URLs and repositories are unchanged. Only catalog format changed.</p>"},{"location":"architecture/ARCHIVED-v3-migration-guide/#q-can-i-roll-back-to-v2-if-v3-has-issues","title":"Q: Can I roll back to V2 if V3 has issues?","text":"<p>A: Yes, V2 code is preserved in git history. Rollback steps: <pre><code>git revert &lt;v3-merge-commit&gt;\ngit push origin main\n</code></pre></p>"},{"location":"architecture/ARCHIVED-v3-migration-guide/#q-how-do-i-migrate-my-custom-portal","title":"Q: How do I migrate my custom portal?","text":"<p>A: Update your portal code to read V3 metadata.json instead of catalog.json. See \"Code Migration Examples\" above.</p>"},{"location":"architecture/ARCHIVED-v3-migration-guide/#q-are-there-performance-improvements-in-v3","title":"Q: Are there performance improvements in V3?","text":"<p>A: Yes: - Faster builds: No catalog.json lock contention - Better parallelism: Hundreds of jobs without conflicts - Incremental updates: Only changed artifacts regenerated - Efficient caching: Granular artifacts can be cached independently</p>"},{"location":"architecture/ARCHIVED-v3-migration-guide/#q-what-if-i-only-want-to-use-v3-for-some-exporters","title":"Q: What if I only want to use V3 for some exporters?","text":"<p>A: V3 is all-or-nothing. Once migrated, all exporters use V3 format. However, you can rebuild individual exporters independently.</p>"},{"location":"architecture/ARCHIVED-v3-migration-guide/#resources","title":"Resources","text":"<ul> <li>V3 Catalog API Reference</li> <li>CI/CD Workflow Documentation</li> <li>Refactoring V2 Plan</li> <li>State Management</li> </ul>"},{"location":"architecture/ARCHIVED-v3-migration-guide/#support","title":"Support","text":"<p>If you encounter issues during migration:</p> <ol> <li>Check Troubleshooting section above</li> <li>Review refactoring-v2-plan.md for implementation details</li> <li>Open an issue on GitHub: https://github.com/SckyzO/monitoring-hub/issues</li> <li>Include:</li> <li>Error messages</li> <li>Workflow logs</li> <li>Exporter name</li> <li>Build environment (fork/main repo)</li> </ol>"},{"location":"architecture/build-pipeline/","title":"Build Pipeline","text":"<p>Detailed documentation of the build pipeline.</p>"},{"location":"architecture/build-pipeline/#pipeline-stages","title":"Pipeline Stages","text":""},{"location":"architecture/build-pipeline/#1-state-management","title":"1. State Management","text":"<p>Compares local manifests against deployed catalog.json to determine what needs rebuilding.</p>"},{"location":"architecture/build-pipeline/#2-artifact-generation","title":"2. Artifact Generation","text":"<p>Builder downloads upstream binaries, extracts them, and renders Jinja2 templates.</p>"},{"location":"architecture/build-pipeline/#3-parallel-builds","title":"3. Parallel Builds","text":"<ul> <li>RPM: Built for el8, el9, el10 \u00d7 amd64, arm64</li> <li>Docker: Built as multi-arch images</li> </ul>"},{"location":"architecture/build-pipeline/#4-validation","title":"4. Validation","text":"<ul> <li>Port checks for Docker containers</li> <li>Command validation for binaries</li> <li>Schema validation for manifests</li> </ul>"},{"location":"architecture/build-pipeline/#5-publication","title":"5. Publication","text":"<ul> <li>RPMs pushed to GitHub Pages YUM repository</li> <li>Docker images pushed to GHCR</li> <li>Catalog updated</li> </ul>"},{"location":"architecture/build-pipeline/#incremental-builds","title":"Incremental Builds","text":"<p>Only changed exporters are rebuilt, saving time and resources.</p>"},{"location":"architecture/ci-cd/","title":"CI/CD Workflows","text":"<p>GitHub Actions workflows that power the factory with unified build architecture and V3 granular catalog.</p>"},{"location":"architecture/ci-cd/#unified-workflow-architecture","title":"Unified Workflow Architecture","text":"<p>The V3 architecture uses a single unified build workflow with atomic writes and state-based change detection:</p> <pre><code>1. scan-updates.yml (CRON daily 8:00 UTC)\n   \u2193 Detects new versions\n   \u2193 Creates PRs\n\n2. build-pr.yml (on PR)\n   \u2193 Validates manifest + canary build\n   \u2193 Uses detect-changes job\n\n3. (merge to main)\n   \u2193\n   auto-release.yml (on push/manual)\n   \u2193 Uses state_manager for change detection\n   \u2193 Triggers build.yml\n\n4. build.yml (unified workflow)\n   \u2193 Builds ALL artifacts (RPM + DEB + Docker)\n   \u2193 Uploads to GitHub Releases\n   \u2193 Atomic writes: 1 job = 1 file\n   \u2193 Publishes metadata to gh-pages\n   \u2193 Generates portal\n\nOR\n\nsecurity-rebuild.yml (CRON Monday 2:00 UTC)\n   \u2193 Detects new UBI9 base image\n   \u2193 Triggers build.yml with force_rebuild=true\n   \u2193 Rebuilds ALL exporters\n</code></pre>"},{"location":"architecture/ci-cd/#main-workflows","title":"Main Workflows","text":""},{"location":"architecture/ci-cd/#buildyml-unified-build-workflow","title":"build.yml (Unified Build Workflow)","text":"<p>Purpose: Main build pipeline that handles everything (RPM, DEB, Docker, metadata, portal)</p> <p>Triggers: - workflow_call from auto-release.yml (changed exporters only) - workflow_call from security-rebuild.yml (all exporters) - Manual workflow_dispatch</p> <p>Input: - <code>exporters</code>: JSON array of exporter names (e.g., <code>[\"node_exporter\", \"redis_exporter\"]</code>) - Empty array <code>[]</code> = build ALL exporters - Empty string <code>\"\"</code> (default) = auto-detect changed exporters with state_manager</p> <p>Key Features: - Unified workflow: Single workflow handles all build scenarios (1 exporter, N exporters, ALL exporters) - Atomic metadata publishing: Each job writes exactly 1 JSON file - No race conditions: Parallel jobs safe (rpm_amd64_el9 + rpm_arm64_el9 write different files) - Granular artifacts: <code>catalog/&lt;exporter&gt;/rpm_&lt;arch&gt;_&lt;dist&gt;.json</code> - Matrix strategy: Builds all arch/dist combinations in parallel - Integrated portal generation: Portal updated after all builds complete</p> <p>Jobs:</p> <ol> <li>discover: Generate build matrix from input</li> <li>Parses <code>exporters</code> input parameter</li> <li>Auto-detects changed exporters if input is empty (using state_manager)</li> <li>Generates matrix for RPM, DEB, and Docker jobs</li> <li> <p>Outputs: <code>rpm_matrix</code>, <code>deb_matrix</code>, <code>docker_matrix</code></p> </li> <li> <p>build-rpm: Build RPM for specific arch/dist combination (parallel)</p> </li> <li>Downloads binary from upstream</li> <li>Renders spec file from Jinja2 template</li> <li>Builds RPM in Docker (almalinux:9 for el8/el9/el10)</li> <li>Signs with GPG</li> <li>Uploads to GitHub Releases</li> <li>Publishes atomic metadata: <code>generate_artifact_metadata.py</code> + <code>publish_artifact_metadata.sh</code></li> <li> <p>Output: <code>catalog/&lt;exporter&gt;/rpm_&lt;arch&gt;_&lt;dist&gt;.json</code></p> </li> <li> <p>build-deb: Build DEB for specific arch/dist combination (parallel)</p> </li> <li>Downloads binary from upstream</li> <li>Renders control files from Jinja2 templates</li> <li>Builds DEB in Docker (debian:12 for universal package)</li> <li>Signs with GPG</li> <li>Uploads to GitHub Releases</li> <li>Publishes atomic metadata: <code>generate_artifact_metadata.py</code> + <code>publish_artifact_metadata.sh</code></li> <li> <p>Output: <code>catalog/&lt;exporter&gt;/deb_&lt;arch&gt;_&lt;dist&gt;.json</code></p> </li> <li> <p>build-docker: Build multi-arch Docker image (parallel)</p> </li> <li>Uses buildx for multi-platform builds (linux/amd64, linux/arm64)</li> <li>Pushes to ghcr.io</li> <li>Scans with Trivy (security)</li> <li>Publishes atomic metadata: <code>generate_artifact_metadata.py</code> + <code>publish_artifact_metadata.sh</code></li> <li> <p>Output: <code>catalog/&lt;exporter&gt;/docker.json</code></p> </li> <li> <p>aggregate-security: Aggregate Trivy scan results (sequential)</p> </li> <li>Collects all security scan results</li> <li>Generates security-stats.json</li> <li> <p>Uploads to gh-pages</p> </li> <li> <p>publish-metadata-portal: Generate portal (sequential, after all builds)</p> </li> <li>Downloads ALL metadata artifacts</li> <li>Organizes into <code>catalog/&lt;exporter&gt;/*.json</code> structure</li> <li>Aggregates metadata using <code>aggregate_catalog_metadata.py</code></li> <li>Generates portal with <code>site_generator_v2.py</code></li> <li>Pushes to gh-pages (catalog/ + index.html)</li> </ol> <p>Example Atomic Write: <pre><code>- name: \ud83d\udce6 Publish RPM metadata (amd64/el9)\n  run: |\n    python3 core/scripts/generate_artifact_metadata.py \\\n      --type rpm --arch amd64 --dist el9 \\\n      --exporter node_exporter --version 1.10.2 \\\n      --filename node_exporter-1.10.2-1.el9.x86_64.rpm \\\n      --sha256 $SHA256 --size $SIZE \\\n      --output artifacts/rpm-node_exporter-amd64-el9/node_exporter/rpm_amd64_el9.json\n\n    bash core/scripts/publish_artifact_metadata.sh \\\n      artifacts/rpm-node_exporter-amd64-el9\n</code></pre></p> <p>Matrix Strategy: <pre><code>strategy:\n  matrix:\n    exporter: [\"node_exporter\", \"redis_exporter\", ...]\n    arch: [\"amd64\", \"arm64\"]\n    dist: [\"el8\", \"el9\", \"el10\"]  # For RPM\n    # OR\n    dist: [\"debian-12\"]            # For DEB (universal package)\n</code></pre></p> <p>Why Unified? - \u2705 Simpler mental model (1 main workflow vs 5 chained workflows) - \u2705 Faster debugging (everything in one workflow run) - \u2705 No concurrency bottlenecks - \u2705 No race conditions - \u2705 Atomic operation (all or nothing) - \u2705 Consistent behavior across all scenarios</p>"},{"location":"architecture/ci-cd/#auto-releaseyml","title":"auto-release.yml","text":"<p>Purpose: Detect changed exporters and trigger build.yml</p> <p>Triggers: - push to main (exporters/*) - Manual workflow_dispatch</p> <p>Key Features: - State-based detection: Uses state_manager to compare manifests with deployed catalog - Smart filtering: Only builds exporters that actually changed - Force rebuild: Manual trigger with empty input builds ALL exporters</p> <p>Jobs:</p> <ol> <li>detect-changes: Detect changed exporters</li> <li>If manual trigger with specific exporters: Use provided list</li> <li>If manual trigger with empty input: Set FORCE_REBUILD=true</li> <li>If automatic (push): Use state_manager for change detection</li> <li> <p>Output: JSON array of exporter names</p> </li> <li> <p>trigger-releases: Trigger build.yml</p> </li> <li>Calls build.yml with <code>exporters</code> parameter</li> <li>Passes force_rebuild flag if applicable</li> </ol> <p>Example: <pre><code># User edits exporters/node_exporter/manifest.yaml\ngit commit -m \"feat: update node_exporter to v1.11.0\"\ngit push origin main\n\n# auto-release.yml detects 1 changed exporter\n# Triggers: build.yml --raw-field exporters='[\"node_exporter\"]'\n</code></pre></p>"},{"location":"architecture/ci-cd/#security-rebuildyml","title":"security-rebuild.yml","text":"<p>Purpose: Rebuild all exporters when security updates are available</p> <p>Triggers: - CRON: Monday at 2:00 UTC - Manual workflow_dispatch</p> <p>Key Features: - UBI9 detection: Checks if new ubi9-minimal base image is available - Force rebuild: Rebuilds ALL exporters regardless of state_manager - Security-first: Ensures all containers use latest secure base image</p> <p>Jobs:</p> <ol> <li>check-ubi9-update: Check for new UBI9 base image</li> <li>Pulls latest ubi9-minimal digest</li> <li>Compares with previous build</li> <li> <p>Outputs: <code>rebuild_needed=true/false</code></p> </li> <li> <p>trigger-rebuild: Trigger build.yml with force_rebuild</p> </li> <li>Calls build.yml with empty exporters array (= ALL)</li> <li>Sets force_rebuild=true to ignore state_manager</li> </ol>"},{"location":"architecture/ci-cd/#build-pryml","title":"build-pr.yml","text":"<p>Purpose: Validate PRs before merge</p> <p>Triggers: - pull_request (exporters/*)</p> <p>Key Features: - Fast validation: Only validates changed exporters - Canary build: Builds node_exporter as smoke test - Comprehensive checks: Linting + schema validation + build test</p> <p>Jobs:</p> <ol> <li>detect-changes: Detect which exporters changed in PR</li> <li>Uses git diff to identify modified manifests</li> <li> <p>Output: JSON array of changed exporters</p> </li> <li> <p>validate-manifests: Validate YAML syntax and schema</p> </li> <li>Runs yamllint on all manifests</li> <li> <p>Validates with marshmallow schema</p> </li> <li> <p>lint-python: Run ruff linter on Python code</p> </li> <li> <p>lint-css: Run stylelint on portal templates</p> </li> <li> <p>canary-build: Build node_exporter as smoke test</p> </li> <li>Tests that build pipeline is functional</li> <li>Catches breaking changes early</li> </ol>"},{"location":"architecture/ci-cd/#scan-updatesyml","title":"scan-updates.yml","text":"<p>Purpose: Automated version watcher</p> <p>Triggers: - CRON: Daily at 8:00 UTC - Manual workflow_dispatch</p> <p>Key Features: - GitHub API integration: Checks latest releases for all exporters - Version comparison: Uses <code>packaging.version</code> for semantic versioning - Automated PRs: Creates PR when new version detected</p> <p>Jobs:</p> <ol> <li>scan-versions: Scan all exporters for updates</li> <li>Reads all manifests</li> <li>Queries GitHub API for latest releases</li> <li>Compares versions</li> <li>Creates PR if update available</li> </ol>"},{"location":"architecture/ci-cd/#legacy-workflows-disabled","title":"Legacy Workflows (Disabled)","text":"<p>The following workflows have been disabled (renamed to <code>.disabled</code>) as they are now integrated into <code>build.yml</code>:</p> <ul> <li><code>build-legacy.yml.disabled</code> - Old single-exporter build</li> <li><code>full-build.yml.disabled</code> - Batch build (logic merged into build.yml)</li> <li><code>upload-releases.yml.disabled</code> - Upload step (now integrated)</li> <li><code>catalog-update.yml.disabled</code> - Catalog update (now integrated)</li> <li><code>update-portal.yml.disabled</code> - Portal update (now integrated)</li> </ul> <p>Why they were removed: - Fragile workflow chains (build \u2192 upload \u2192 catalog \u2192 portal) - Concurrency bottlenecks (upload-releases had concurrency:1) - Race conditions (catalog-update could be triggered 34\u00d7 in parallel) - Architecture confusion (build.yml for 1, full-build.yml for N)</p> <p>Solution: One unified build.yml that does everything atomically.</p>"},{"location":"architecture/ci-cd/#workflow-comparison","title":"Workflow Comparison","text":""},{"location":"architecture/ci-cd/#before-fragmented","title":"Before (Fragmented)","text":"<pre><code>security-rebuild.yml\n  \u2514\u2500&gt; Triggers 34\u00d7 build.yml in parallel\n        \u2514\u2500&gt; Each triggers upload-releases.yml (concurrency:1)\n              \u2514\u2500&gt; 33 triggers lost/cancelled \u274c\n                    \u2514\u2500&gt; Packages built but catalog never updated\n</code></pre>"},{"location":"architecture/ci-cd/#after-unified","title":"After (Unified)","text":"<pre><code>security-rebuild.yml\n  \u2514\u2500&gt; Triggers 1\u00d7 build.yml with exporters=[]\n        \u2514\u2500&gt; Builds ALL 34 exporters in one workflow run\n              \u2514\u2500&gt; Uploads in parallel (no concurrency limit)\n                    \u2514\u2500&gt; Catalog + portal updated atomically \u2705\n</code></pre>"},{"location":"architecture/ci-cd/#state-management","title":"State Management","text":"<p>All change detection uses <code>core/engine/state_manager.py</code>:</p> <p>How it works: 1. Fetches <code>catalog.json</code> from gh-pages (production state) 2. Compares with local manifests (git state) 3. Detects version changes 4. Outputs JSON array of changed exporters</p> <p>Override: - Set <code>FORCE_REBUILD=true</code> to rebuild everything - Set <code>TARGET_EXPORTER=name</code> to build specific exporter</p>"},{"location":"architecture/ci-cd/#metadata-publishing-v3","title":"Metadata Publishing (V3)","text":"<p>Each build job publishes exactly 1 JSON file:</p> <pre><code># RPM job (el9/amd64) publishes:\ncatalog/node_exporter/rpm_amd64_el9.json\n\n# RPM job (el9/arm64) publishes:\ncatalog/node_exporter/rpm_arm64_el9.json\n\n# DEB job (debian-12/amd64) publishes:\ncatalog/node_exporter/deb_amd64_debian-12.json\n\n# Docker job publishes:\ncatalog/node_exporter/docker.json\n</code></pre> <p>No race conditions: Each job writes to a different file.</p> <p>Aggregation: Portal generation aggregates these files into <code>metadata.json</code> on-demand.</p>"},{"location":"architecture/ci-cd/#security-scanning","title":"Security Scanning","text":"<p>Security scans run in parallel with builds:</p> <p>Docker images: - Trivy scan with SARIF output - Results uploaded to GitHub Security tab - Critical/High vulnerabilities block release</p> <p>Python code: - Bandit: Security linter for Python - pip-audit: Check dependencies for CVEs - Results in security.yml workflow</p> <p>Frequency: - Daily: security.yml (scheduled) - On PR: build-pr.yml (gating) - On release: build.yml (embedded)</p>"},{"location":"architecture/ci-cd/#see-also","title":"See Also","text":"<ul> <li>Unified Workflow Architecture - Complete migration details</li> <li>State Manager Documentation - Change detection logic</li> <li>V3 Catalog Architecture - Granular metadata structure</li> </ul>"},{"location":"architecture/overview/","title":"Architecture Overview","text":"<p>Monitoring Hub is built as a Software Factory with automated build pipelines.</p>"},{"location":"architecture/overview/#high-level-architecture","title":"High-Level Architecture","text":"<pre><code>graph TD\n    A[YAML Manifests] --&gt; B[State Manager]\n    B --&gt; C{Changed?}\n    C --&gt;|Yes| D[Builder]\n    C --&gt;|No| E[Skip]\n    D --&gt; F[RPM Builder]\n    D --&gt; G[Docker Builder]\n    F --&gt; H[YUM Repo]\n    G --&gt; I[GHCR]\n    J[Watcher] --&gt;|New Version| K[PR]\n    K --&gt;|CI Pass| L[Auto-merge]</code></pre>"},{"location":"architecture/overview/#core-components","title":"Core Components","text":"<ul> <li>Builder (<code>core/engine/builder.py</code>): Downloads binaries and generates build artifacts</li> <li>Schema (<code>core/engine/schema.py</code>): Validates manifest YAML files</li> <li>State Manager (<code>core/engine/state_manager.py</code>): Tracks changes and triggers rebuilds</li> <li>Site Generator (<code>core/engine/site_generator.py</code>): Generates the portal</li> <li>Watcher (<code>core/engine/watcher.py</code>): Monitors upstream for updates</li> </ul>"},{"location":"architecture/overview/#build-flow","title":"Build Flow","text":"<ol> <li>Discovery: State Manager compares local manifests with deployed catalog</li> <li>Generation: Builder downloads binaries and renders templates</li> <li>Build: CI builds RPM and Docker artifacts in parallel</li> <li>Validation: Tests ensure packages work correctly</li> <li>Distribution: Artifacts published to YUM repo and GHCR</li> </ol> <p>See Build Pipeline for details.</p>"},{"location":"architecture/state-management/","title":"State Management","text":"<p>How Monitoring Hub tracks and manages exporter versions.</p>"},{"location":"architecture/state-management/#catalog-system","title":"Catalog System","text":"<p>The catalog uses a fragmented structure for efficient incremental publishing:</p>"},{"location":"architecture/state-management/#structure","title":"Structure","text":"<pre><code>/catalog\n  \u251c\u2500\u2500 index.json           # Lightweight index (4KB)\n  \u251c\u2500\u2500 node_exporter.json   # Per-exporter details\n  \u251c\u2500\u2500 prometheus.json\n  \u251c\u2500\u2500 alertmanager.json\n  \u2514\u2500\u2500 ...\n/catalog.json              # Legacy full catalog (217KB, deprecated)\n</code></pre>"},{"location":"architecture/state-management/#file-purposes","title":"File Purposes","text":"<ol> <li><code>catalog/index.json</code> (Recommended)</li> <li>Lightweight version lookup (4KB)</li> <li>Contains: <code>name</code>, <code>version</code>, <code>category</code>, <code>last_updated</code></li> <li>Used by State Manager for change detection</li> <li> <p>54x smaller than legacy format</p> </li> <li> <p><code>catalog/{exporter}.json</code></p> </li> <li>Complete metadata for each exporter</li> <li>Availability tracking (RPM/DEB/Docker)</li> <li>Build status and dates</li> <li> <p>Enables incremental updates per exporter</p> </li> <li> <p><code>catalog.json</code> (Legacy)</p> </li> <li>Full catalog in single file</li> <li>Maintained for backward compatibility</li> <li>Contains deprecation notice</li> <li>Will be removed in future version</li> </ol>"},{"location":"architecture/state-management/#configuration","title":"Configuration","text":"<p>State Manager uses the lightweight index by default:</p> <pre><code># core/config/settings.py\nDEFAULT_CATALOG_URL = \"https://sckyzo.github.io/monitoring-hub/catalog/index.json\"\n</code></pre> <p>Override with environment variable:</p> <pre><code>CATALOG_URL=https://custom.domain/catalog.json python -m core.engine.state_manager\n</code></pre>"},{"location":"architecture/state-management/#change-detection","title":"Change Detection","text":"<p>State Manager compares:</p> <ul> <li>Local manifest versions (from <code>exporters/*/manifest.yaml</code>)</li> <li>Deployed catalog versions (from <code>catalog/index.json</code>)</li> <li>Force rebuild flags (environment variable)</li> </ul>"},{"location":"architecture/state-management/#detection-logic","title":"Detection Logic","text":"<pre><code>def needs_rebuild(exporter_name, local_version, remote_version):\n    # 1. Check force rebuild\n    if FORCE_REBUILD:\n        return True\n\n    # 2. New exporter (not in remote catalog)\n    if remote_version is None:\n        return True\n\n    # 3. Version changed\n    if local_version != remote_version:\n        return True\n\n    return False\n</code></pre>"},{"location":"architecture/state-management/#rebuild-matrix","title":"Rebuild Matrix","text":"<p>The state manager outputs a JSON array of exporters requiring rebuild:</p> <pre><code>[\"node_exporter\", \"prometheus\", \"alertmanager\"]\n</code></pre> <p>This array feeds into GitHub Actions matrix strategy for parallel builds.</p>"},{"location":"architecture/state-management/#version-comparison","title":"Version Comparison","text":"<p>Version format handling:</p> <ul> <li>Strips <code>v</code> prefix for normalization (<code>v1.0.0</code> \u2192 <code>1.0.0</code>)</li> <li>Supports semantic versioning (MAJOR.MINOR.PATCH)</li> <li>Handles custom version schemes (dates, git SHAs)</li> <li>Case-insensitive comparison</li> </ul>"},{"location":"architecture/state-management/#examples","title":"Examples","text":"<pre><code># Normalized comparison\n\"v1.0.0\" == \"1.0.0\"  # True (v prefix stripped)\n\"1.0.0\" != \"1.0.1\"   # True (version changed)\n\"2.0.0\" != \"1.9.9\"   # True (major bump)\n</code></pre>"},{"location":"architecture/state-management/#incremental-publishing","title":"Incremental Publishing","text":"<p>The fragmented catalog enables efficient incremental updates:</p> <ol> <li>Build completes for <code>node_exporter</code></li> <li>Generate <code>catalog/node_exporter.json</code> with new data</li> <li>Regenerate <code>catalog/index.json</code> with updated version</li> <li>Publish only changed files to gh-pages</li> <li>Preserve unchanged exporter files</li> </ol>"},{"location":"architecture/state-management/#benefits","title":"Benefits","text":"<ul> <li>Reduced bandwidth: Only upload changed exporters</li> <li>Faster CI/CD: Skip unchanged catalog entries</li> <li>Atomic updates: Each exporter independently versioned</li> <li>Parallel builds: Multiple exporters can publish simultaneously</li> </ul>"},{"location":"architecture/unified-workflow/","title":"Unified Workflow Architecture","text":"<p>Date: 2026-02-16 Status: Active (migrated from fragmented architecture)</p>"},{"location":"architecture/unified-workflow/#overview","title":"Overview","text":"<p>Monitoring Hub uses a unified build workflow architecture with a single main workflow (<code>build.yml</code>) that handles all build scenarios.</p>"},{"location":"architecture/unified-workflow/#architecture-diagram","title":"Architecture Diagram","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                 TRIGGERS (Automatic &amp; Manual)                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nscan-updates.yml (CRON 8h UTC)\n  \u2193 Creates PR with version update\nUser merges PR\n  \u2193 Triggers auto-release.yml\n\nauto-release.yml\n  \u251c\u2500 Detects changed manifests (state_manager)\n  \u2514\u2500&gt; build.yml (changed exporters only)\n\nsecurity-rebuild.yml (CRON Monday 2h UTC)\n  \u251c\u2500 Detects new UBI9 base image\n  \u2514\u2500&gt; build.yml (ALL exporters)\n\nManual: workflow_dispatch\n  \u2514\u2500&gt; build.yml (custom list or ALL)\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502               UNIFIED BUILD WORKFLOW (build.yml)             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nINPUT: JSON array of exporters\n  - [\"node_exporter\"]           \u2192 Build 1 exporter\n  - [\"node_exporter\", \"redis\"]  \u2192 Build 2 exporters\n  - []                          \u2192 Build ALL exporters (empty=all)\n  - \"\" (default)                \u2192 Auto-detect (state_manager)\n\nJOBS (Parallel):\n  \u251c\u2500 discover: Generate build matrix\n  \u2502\n  \u251c\u2500 build-docker (34\u00d7 exporters \u00d7 2 arches)\n  \u2502  \u251c\u2500 Build multi-arch image\n  \u2502  \u251c\u2500 Push to GHCR\n  \u2502  \u251c\u2500 Upload to GitHub Releases\n  \u2502  \u2514\u2500 Generate docker.json (V3 metadata)\n  \u2502\n  \u251c\u2500 build-rpm (~200 jobs: 34 \u00d7 2 arches \u00d7 3 dists)\n  \u2502  \u251c\u2500 Build RPM in AlmaLinux container\n  \u2502  \u251c\u2500 Sign with GPG\n  \u2502  \u251c\u2500 Upload to GitHub Releases\n  \u2502  \u2514\u2500 Generate rpm_&lt;arch&gt;_&lt;dist&gt;.json (V3 metadata)\n  \u2502\n  \u251c\u2500 build-deb (~68 jobs: 34 \u00d7 2 arches \u00d7 1 dist)\n  \u2502  \u251c\u2500 Build DEB in Debian 12 container\n  \u2502  \u251c\u2500 Sign with GPG\n  \u2502  \u251c\u2500 Upload to GitHub Releases\n  \u2502  \u2514\u2500 Generate deb_&lt;arch&gt;_&lt;dist&gt;.json (V3 metadata)\n  \u2502\n  \u251c\u2500 aggregate-security\n  \u2502  \u2514\u2500 Aggregate Trivy scan results\n  \u2502\n  \u2514\u2500 publish-metadata-portal (Sequential, after all builds)\n     \u251c\u2500 Download ALL metadata artifacts\n     \u251c\u2500 Organize into catalog/&lt;exporter&gt;/*.json\n     \u251c\u2500 Generate portal with site_generator_v2.py\n     \u2514\u2500 Push to gh-pages (catalog/ + index.html)\n\nOUTPUT:\n  \u2705 Packages on GitHub Releases\n  \u2705 Docker images on GHCR\n  \u2705 Catalog V3 on gh-pages\n  \u2705 Portal updated on gh-pages\n</code></pre>"},{"location":"architecture/unified-workflow/#workflow-list","title":"Workflow List","text":"Workflow Trigger Purpose build.yml \u2b50 workflow_dispatch, auto-release, security-rebuild Main unified build workflow auto-release.yml push to exporters/ Detects manifest changes, triggers build.yml scan-updates.yml CRON daily 8h UTC Scans for new versions, creates PRs security-rebuild.yml CRON Monday 2h UTC Rebuilds all for security updates build-pr.yml pull_request Validates PRs (lint + schema + canary) deploy-docs.yml push to docs/ Deploys MkDocs documentation"},{"location":"architecture/unified-workflow/#key-principles","title":"Key Principles","text":""},{"location":"architecture/unified-workflow/#1-single-responsibility","title":"1. Single Responsibility","text":"<ul> <li>One workflow does it all: build.yml handles building, uploading, catalog, and portal</li> <li>No fragile workflow chains (no build \u2192 upload \u2192 catalog \u2192 portal)</li> <li>No concurrency bottlenecks</li> </ul>"},{"location":"architecture/unified-workflow/#2-scalability","title":"2. Scalability","text":"<ul> <li>Handles 1 exporter as efficiently as 34 exporters</li> <li>Parallel builds (100+ jobs running concurrently)</li> <li>Single aggregation job at the end (no race conditions)</li> </ul>"},{"location":"architecture/unified-workflow/#3-maintainability","title":"3. Maintainability","text":"<ul> <li>Only 6 workflows instead of 12</li> <li>Clear responsibility for each workflow</li> <li>Easy to understand and debug</li> </ul>"},{"location":"architecture/unified-workflow/#4-consistency","title":"4. Consistency","text":"<ul> <li>Same workflow for all scenarios (auto, security, manual)</li> <li>Consistent inputs (JSON array of exporters)</li> <li>Consistent outputs (Releases + Catalog + Portal)</li> </ul>"},{"location":"architecture/unified-workflow/#migration-from-legacy-architecture","title":"Migration from Legacy Architecture","text":""},{"location":"architecture/unified-workflow/#what-was-removed","title":"What Was Removed","text":"<p>The following workflows were disabled (renamed to <code>.disabled</code>) as they are now integrated into <code>build.yml</code>:</p> <ul> <li><code>build-legacy.yml.disabled</code> - Old single-exporter build</li> <li><code>full-build.yml.disabled</code> - Batch build (logic merged into build.yml)</li> <li><code>upload-releases.yml.disabled</code> - Upload step (now integrated)</li> <li><code>catalog-update.yml.disabled</code> - Catalog update (now integrated)</li> <li><code>update-portal.yml.disabled</code> - Portal update (now integrated)</li> </ul>"},{"location":"architecture/unified-workflow/#why-they-were-removed","title":"Why They Were Removed","text":"<p>Problem 1: Fragile Workflow Chain <pre><code>build.yml \u2192 upload-releases.yml \u2192 catalog-update.yml \u2192 update-portal.yml\n   \u2193              \u2193                      \u2193                    \u2193\n If any fails, entire chain breaks\n</code></pre></p> <p>Problem 2: Concurrency Chaos <pre><code>security-rebuild triggers 34\u00d7 build.yml in parallel\n  \u2192 Each triggers upload-releases.yml\n    \u2192 But upload-releases has concurrency: 1\n      \u2192 33 triggers lost/cancelled\n        \u2192 Packages built but catalog never updated \u274c\n</code></pre></p> <p>Problem 3: Architecture Confusion - build.yml for 1 exporter - full-build.yml for N exporters - Why two workflows for the same task?</p>"},{"location":"architecture/unified-workflow/#solution-unified-architecture","title":"Solution: Unified Architecture","text":"<p>One workflow to rule them all: <pre><code>build.yml handles:\n  \u2705 1 exporter\n  \u2705 N exporters\n  \u2705 ALL exporters\n  \u2705 Auto-detected exporters\n</code></pre></p> <p>No chains, no concurrency issues: <pre><code>build.yml does everything in one workflow run:\n  \u251c\u2500 Build (parallel)\n  \u251c\u2500 Upload (parallel)\n  \u2514\u2500 Catalog + Portal (sequential after all)\n</code></pre></p>"},{"location":"architecture/unified-workflow/#usage-examples","title":"Usage Examples","text":""},{"location":"architecture/unified-workflow/#scenario-1-user-updates-1-manifest","title":"Scenario 1: User Updates 1 Manifest","text":"<pre><code># User edits exporters/node_exporter/manifest.yaml\ngit commit -m \"feat: update node_exporter to v1.11.0\"\ngit push origin main\n</code></pre> <p>Flow: <pre><code>push \u2192 auto-release.yml\n    \u251c\u2500 state_manager detects 1 changed exporter\n    \u2514\u2500&gt; build.yml --raw-field exporters='[\"node_exporter\"]'\n        \u2514\u2500&gt; Builds only node_exporter\n            \u2514\u2500&gt; Updates catalog + portal \u2705\n</code></pre></p>"},{"location":"architecture/unified-workflow/#scenario-2-security-rebuild-all-exporters","title":"Scenario 2: Security Rebuild (All Exporters)","text":"<pre><code># Monday 2h UTC (automatic CRON)\nsecurity-rebuild.yml\n  \u251c\u2500 Detects new UBI9 base image\n  \u2514\u2500&gt; build.yml --raw-field exporters='[]'  # Empty = ALL\n      \u2514\u2500&gt; Builds ALL 34 exporters\n          \u2514\u2500&gt; Updates catalog + portal \u2705\n</code></pre>"},{"location":"architecture/unified-workflow/#scenario-3-manual-rebuild-after-refactoring","title":"Scenario 3: Manual Rebuild After Refactoring","text":"<pre><code># User manually triggers build.yml in GitHub Actions UI\n# Input: exporters = [] (empty)\n</code></pre> <p>Flow: <pre><code>Manual trigger \u2192 build.yml (empty input = ALL)\n    \u2514\u2500&gt; Builds ALL 34 exporters from scratch\n        \u2514\u2500&gt; Publishes everything \u2705\n</code></pre></p>"},{"location":"architecture/unified-workflow/#scenario-4-test-specific-exporters","title":"Scenario 4: Test Specific Exporters","text":"<pre><code># User manually triggers build.yml in GitHub Actions UI\n# Input: exporters = [\"node_exporter\", \"redis_exporter\", \"prometheus\"]\n</code></pre> <p>Flow: <pre><code>Manual trigger \u2192 build.yml\n    \u2514\u2500&gt; Builds only these 3 exporters\n        \u2514\u2500&gt; Updates catalog + portal \u2705\n</code></pre></p>"},{"location":"architecture/unified-workflow/#rollback-plan","title":"Rollback Plan","text":"<p>If critical issues arise with the unified architecture:</p> <pre><code># Restore legacy workflows\ncd .github/workflows/\nmv build-legacy.yml.disabled build-single.yml\nmv full-build.yml.disabled full-build.yml\nmv upload-releases.yml.disabled upload-releases.yml\nmv catalog-update.yml.disabled catalog-update.yml\nmv update-portal.yml.disabled update-portal.yml\n\n# Revert auto-release and security-rebuild\ngit restore auto-release.yml security-rebuild.yml\n\n# Disable unified build.yml\nmv build.yml build-unified.yml.disabled\n</code></pre>"},{"location":"architecture/unified-workflow/#testing-checklist","title":"Testing Checklist","text":"<p>Before declaring the migration complete, verify:</p> <ul> <li> build.yml triggered by auto-release works</li> <li> build.yml triggered by security-rebuild works</li> <li> build.yml manual trigger with 1 exporter works</li> <li> build.yml manual trigger with 3 exporters works</li> <li> build.yml manual trigger with empty (ALL) works</li> <li> Catalog.json published correctly on gh-pages</li> <li> Portal (index.html) updated correctly</li> <li> No concurrency issues</li> <li> No race conditions in metadata aggregation</li> </ul>"},{"location":"architecture/unified-workflow/#benefits","title":"Benefits","text":""},{"location":"architecture/unified-workflow/#operational","title":"Operational","text":"<p>\u2705 Simpler mental model (1 main workflow vs 5 chained workflows) \u2705 Faster debugging (everything in one workflow run) \u2705 No concurrency bottlenecks \u2705 No race conditions</p>"},{"location":"architecture/unified-workflow/#development","title":"Development","text":"<p>\u2705 Easier to maintain (less code duplication) \u2705 Easier to test (single workflow to validate) \u2705 Easier to extend (add features in one place)</p>"},{"location":"architecture/unified-workflow/#reliability","title":"Reliability","text":"<p>\u2705 No fragile chains (1 workflow = atomic operation) \u2705 Consistent behavior across all scenarios \u2705 Better error handling (fail early in single workflow)</p>"},{"location":"architecture/unified-workflow/#see-also","title":"See Also","text":"<ul> <li>V3 Granular Catalog Architecture</li> <li>CI/CD Workflows</li> <li>State Manager Documentation</li> </ul>"},{"location":"contributing/development/","title":"Development Setup","text":"<p>Get your development environment ready using our Docker-first workflow.</p>"},{"location":"contributing/development/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker - The only requirement!</li> <li>Git - For version control</li> </ul> <p>Optional: - Python 3.9+ (only for local development without Docker)</p>"},{"location":"contributing/development/#quick-start-recommended","title":"Quick Start (Recommended)","text":"<p>The simplest way to start is with our <code>./devctl</code> CLI:</p> <pre><code># Clone repository\ngit clone https://github.com/SckyzO/monitoring-hub.git\ncd monitoring-hub\ngit checkout -b feature/my-feature\n\n# Build development Docker image\n./devctl build\n\n# Open interactive shell\n./devctl shell\n\n# Run tests\n./devctl test\n\n# Run linter\n./devctl lint\n</code></pre> <p>That's it! No Python installation needed.</p>"},{"location":"contributing/development/#alternative-local-python-setup","title":"Alternative: Local Python Setup","text":"<p>If you prefer developing without Docker:</p> <pre><code># Create virtual environment\npython -m venv .venv\nsource .venv/bin/activate\n\n# Install dependencies\nmake install\n\n# This installs:\n# - requirements/base.txt (engine dependencies)\n# - requirements/dev.txt (ruff, pytest, mypy)\n# - requirements/docs.txt (mkdocs)\n# - pre-commit hooks\n</code></pre>"},{"location":"contributing/development/#development-workflow","title":"Development Workflow","text":""},{"location":"contributing/development/#using-devctl-recommended","title":"Using devctl (Recommended)","text":"<p>All development tasks are available through <code>./devctl</code>:</p> <pre><code># Code quality\n./devctl test              # Run tests\n./devctl test-cov          # Run tests with coverage\n./devctl lint              # Check Python linting\n./devctl lint-fix          # Auto-fix linting issues\n./devctl lint-css          # Check CSS linting\n./devctl lint-yaml         # Check YAML linting\n./devctl format            # Format code\n./devctl format-check      # Check formatting without changes\n./devctl type-check        # Run type checker\n./devctl ci                # Run all CI checks (lint + format + type + tests)\n\n# Working with exporters\n./devctl create-exporter           # Create new exporter\n./devctl list-exporters            # List all exporters\n./devctl build-exporter &lt;name&gt;     # Build specific exporter\n./devctl test-exporter &lt;name&gt;      # Test build exporter\n\n# Documentation\n./devctl generate-portal   # Generate web portal\n./devctl docs-build        # Build MkDocs docs\n./devctl docs-serve        # Serve docs at localhost:8000\n\n# Utilities\n./devctl shell             # Open interactive shell\n./devctl python &lt;cmd&gt;      # Run Python command\n./devctl help              # Show all commands\n</code></pre>"},{"location":"contributing/development/#using-make","title":"Using Make","text":"<p>For convenience, most <code>devctl</code> commands are aliased in the Makefile:</p> <pre><code>make test              # Same as ./devctl test\nmake lint              # Same as ./devctl lint\nmake format            # Same as ./devctl format\nmake ci                # Same as ./devctl ci\n</code></pre>"},{"location":"contributing/development/#local-python-workflow-advanced","title":"Local Python Workflow (Advanced)","text":"<p>If you have Python installed locally and want faster iteration:</p> <pre><code>make local-test        # Run tests locally\nmake local-lint        # Check linting locally\nmake local-format      # Format code locally\nmake pre-commit        # Run pre-commit hooks\n</code></pre> <p>Note: Local commands require <code>make install</code> first to set up the virtual environment.</p>"},{"location":"contributing/development/#testing-exporters-locally","title":"Testing Exporters Locally","text":""},{"location":"contributing/development/#quick-test","title":"Quick Test","text":"<pre><code># Test build an exporter (RPM + Docker + validation)\n./devctl test-exporter node_exporter\n\n# With specific options\n./devctl test-exporter node_exporter --arch arm64 --el10\n</code></pre>"},{"location":"contributing/development/#build-artifacts-only","title":"Build Artifacts Only","text":"<pre><code># Build just the artifacts (no RPM/Docker)\n./devctl build-exporter node_exporter\n\n# Output will be in: build/node_exporter/\n</code></pre>"},{"location":"contributing/development/#validate-manifest","title":"Validate Manifest","text":"<pre><code># Open shell and validate manifest\n./devctl shell\n\n# Inside container:\npython3 -m core.engine.builder \\\n  --manifest exporters/my_exporter/manifest.yaml \\\n  --arch amd64 \\\n  --output-dir /tmp/test\n</code></pre> <p>See Testing Guide for more details.</p>"},{"location":"contributing/pull-requests/","title":"Pull Requests","text":"<p>Guidelines for submitting pull requests.</p>"},{"location":"contributing/pull-requests/#before-you-submit","title":"Before You Submit","text":"<ol> <li>\u2705 Tests pass locally (<code>make test</code>)</li> <li>\u2705 Linting passes (<code>make lint</code>)</li> <li>\u2705 Documentation updated</li> <li>\u2705 Commit messages follow conventions</li> </ol>"},{"location":"contributing/pull-requests/#pr-process","title":"PR Process","text":"<ol> <li>Create Branch: <code>git checkout -b feature/my-feature</code></li> <li>Make Changes: Follow coding standards</li> <li>Test Locally: Run test suite</li> <li>Commit: Use conventional commits</li> <li>Push: <code>git push origin feature/my-feature</code></li> <li>Create PR: On GitHub</li> </ol>"},{"location":"contributing/pull-requests/#pr-template","title":"PR Template","text":"<pre><code>## Description\nBrief description of changes\n\n## Type of Change\n- [ ] Bug fix\n- [ ] New feature\n- [ ] Breaking change\n- [ ] Documentation update\n\n## Testing\n- [ ] Tests pass locally\n- [ ] New tests added\n- [ ] Manual testing completed\n\n## Checklist\n- [ ] Code follows style guidelines\n- [ ] Documentation updated\n- [ ] No breaking changes (or documented)\n</code></pre>"},{"location":"contributing/pull-requests/#ci-checks","title":"CI Checks","text":"<p>All PRs must pass:</p> <ul> <li>\u2705 Linting (ruff)</li> <li>\u2705 Tests (pytest)</li> <li>\u2705 Build validation</li> <li>\u2705 Manifest validation</li> </ul>"},{"location":"contributing/pull-requests/#review-process","title":"Review Process","text":"<p>Maintainers will review and may request changes. Once approved, PRs are merged automatically.</p>"},{"location":"contributing/security/","title":"Security Guidelines","text":"<p>Security best practices for contributing to Monitoring Hub.</p>"},{"location":"contributing/security/#code-security-principles","title":"Code Security Principles","text":""},{"location":"contributing/security/#input-validation","title":"Input Validation","text":"<p>Always validate external input:</p> <pre><code># Good: Strict schema validation\nfrom core.engine.schema import ManifestSchema\nschema = ManifestSchema()\nvalidated_data = schema.load(user_input)\n\n# Bad: Trusting user input\ndata = yaml.safe_load(untrusted_file)  # No validation\n</code></pre> <p>Manifest validation prevents: - Injection attacks through malformed YAML - Unexpected data types causing runtime errors - Missing required fields</p>"},{"location":"contributing/security/#network-operations","title":"Network Operations","text":"<p>Always use timeouts:</p> <pre><code># Good: Timeout prevents hanging\nresponse = requests.get(url, timeout=30)\n\n# Bad: No timeout\nresponse = requests.get(url)  # Can hang indefinitely\n</code></pre> <p>Implement retry logic with exponential backoff:</p> <pre><code>from tenacity import retry, stop_after_attempt, wait_exponential\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=2, max=10)\n)\ndef download_file(url):\n    response = requests.get(url, timeout=30)\n    response.raise_for_status()\n    return response.content\n</code></pre>"},{"location":"contributing/security/#template-security","title":"Template Security","text":"<p>Enable Jinja2 autoescape:</p> <pre><code># Good: Autoescape prevents XSS\nfrom jinja2 import Environment, FileSystemLoader, select_autoescape\n\nenv = Environment(\n    loader=FileSystemLoader(template_dir),\n    autoescape=select_autoescape(['html', 'xml', 'j2'])\n)\n\n# Bad: No autoescape\nenv = Environment(loader=FileSystemLoader(template_dir))\n</code></pre>"},{"location":"contributing/security/#exception-handling","title":"Exception Handling","text":"<p>Always chain exceptions for better debugging:</p> <pre><code># Good: Exception chaining preserves context\ntry:\n    dangerous_operation()\nexcept SpecificError as err:\n    logger.error(f\"Operation failed: {err}\")\n    raise click.Abort() from err\n\n# Bad: Losing exception context\nexcept:\n    raise click.Abort()\n</code></pre> <p>Avoid bare except clauses:</p> <pre><code># Good: Specific exception handling\ntry:\n    risky_operation()\nexcept (ValueError, KeyError) as err:\n    handle_error(err)\n\n# Bad: Catches everything including KeyboardInterrupt\ntry:\n    risky_operation()\nexcept:\n    pass\n</code></pre>"},{"location":"contributing/security/#dependency-security","title":"Dependency Security","text":""},{"location":"contributing/security/#managing-dependencies","title":"Managing Dependencies","text":"<ul> <li>Pin versions: Use exact versions in <code>requirements.txt</code></li> <li>Regular updates: Review Dependabot PRs weekly</li> <li>Vulnerability scanning: Check <code>pip-audit</code> reports</li> <li>Minimal dependencies: Only add necessary packages</li> </ul>"},{"location":"contributing/security/#before-adding-dependencies","title":"Before Adding Dependencies","text":"<ol> <li>Check package reputation (GitHub stars, downloads, maintainers)</li> <li>Review security advisories on PyPI Advisory Database</li> <li>Verify package signatures when available</li> <li>Check for known vulnerabilities with <code>pip-audit</code></li> </ol>"},{"location":"contributing/security/#container-security","title":"Container Security","text":""},{"location":"contributing/security/#dockerfile-best-practices","title":"Dockerfile Best Practices","text":"<pre><code># Use minimal base images\nFROM registry.access.redhat.com/ubi9/ubi-minimal\n\n# Run as non-root user\nRUN useradd -r -u 1000 exporter\nUSER exporter\n\n# Use read-only root filesystem\nVOLUME [\"/var/lib/exporter\"]\n</code></pre>"},{"location":"contributing/security/#image-scanning","title":"Image Scanning","text":"<p>Before pushing images:</p> <pre><code># Scan with Trivy\ntrivy image monitoring-hub/exporter:latest\n\n# Check for high/critical vulnerabilities\ntrivy image --severity HIGH,CRITICAL monitoring-hub/exporter:latest\n</code></pre>"},{"location":"contributing/security/#secret-management","title":"Secret Management","text":"<p>Never commit secrets:</p> <ul> <li>API tokens</li> <li>Private keys</li> <li>Passwords</li> <li>Connection strings</li> </ul> <p>Use environment variables:</p> <pre><code># Good: From environment\ntoken = os.environ.get('GITHUB_TOKEN')\n\n# Bad: Hardcoded\ntoken = \"ghp_xxxxxxxxxxxx\"\n</code></pre> <p>Add sensitive files to <code>.gitignore</code>:</p> <pre><code>secrets/\n.env\n*.key\n*.pem\ncredentials.json\n</code></pre>"},{"location":"contributing/security/#security-testing","title":"Security Testing","text":""},{"location":"contributing/security/#automated-ci-security-scanning","title":"Automated CI Security Scanning","text":"<p>The <code>security.yml</code> workflow runs on every pull request and push to main:</p> <p>Bandit (Python Security Scanner) - Scans Python code for common security issues - Checks for SQL injection, hardcoded passwords, insecure functions - Reports findings as GitHub annotations</p> <p>pip-audit (Dependency Vulnerability Scanner) - Scans Python dependencies for known CVEs - Checks against the OSV vulnerability database - Fails CI on high/critical vulnerabilities</p> <p>Trivy (Container Security Scanner) - Scans container images for vulnerabilities - Uploads SARIF reports to GitHub Security tab - Requires <code>security-events: write</code> permission - Integrates with GitHub Advanced Security</p> <p>View security findings: - Go to the Security tab in GitHub - Click Code scanning alerts - Review Trivy vulnerability reports</p>"},{"location":"contributing/security/#pre-commit-checks","title":"Pre-Commit Checks","text":"<p>Install pre-commit hooks:</p> <pre><code>make install\n</code></pre> <p>This runs: - <code>bandit</code>: Security vulnerability scanner - <code>ruff</code>: Linting with security rules - <code>mypy</code>: Type checking</p>"},{"location":"contributing/security/#manual-security-checks","title":"Manual Security Checks","text":"<pre><code># Run security scanner\nmake security\n\n# Check for vulnerable dependencies\npip-audit -r requirements/base.txt\n\n# Scan code for secrets\ngitleaks detect --no-git\n\n# Scan container image with Trivy\ntrivy image --severity HIGH,CRITICAL monitoring-hub/exporter:latest\n</code></pre>"},{"location":"contributing/security/#reporting-security-issues","title":"Reporting Security Issues","text":"<p>Do not open public issues for security vulnerabilities.</p> <p>Follow our Security Policy for responsible disclosure.</p>"},{"location":"contributing/security/#security-checklist-for-contributors","title":"Security Checklist for Contributors","text":"<p>Before submitting a PR:</p> <ul> <li> All network calls have timeouts</li> <li> Input validation on external data</li> <li> No hardcoded secrets or credentials</li> <li> Jinja2 templates use autoescape</li> <li> Exceptions are properly chained</li> <li> No bare <code>except:</code> clauses</li> <li> Dependencies are pinned and reviewed</li> <li> Security tests pass (<code>make security</code>)</li> <li> No new Bandit warnings introduced</li> </ul>"},{"location":"contributing/security/#resources","title":"Resources","text":"<ul> <li>OWASP Top 10</li> <li>Python Security Best Practices</li> <li>Bandit Documentation</li> <li>pip-audit</li> </ul>"},{"location":"contributing/security/#questions","title":"Questions?","text":"<p>For security questions or concerns:</p> <ul> <li>Open a GitHub Discussion</li> <li>Contact maintainers privately for sensitive issues</li> <li>See SECURITY.md for vulnerability reporting</li> </ul>"},{"location":"contributing/standards/","title":"Coding Standards","text":"<p>Code quality standards for Monitoring Hub.</p>"},{"location":"contributing/standards/#commit-messages","title":"Commit Messages","text":"<p>Use Conventional Commits format:</p> <ul> <li><code>feat(exporters): add new exporter</code></li> <li><code>fix(builder): correct archive extraction</code></li> <li><code>docs: update installation guide</code></li> <li><code>test: add builder unit tests</code></li> <li><code>chore(deps): update dependencies</code></li> </ul>"},{"location":"contributing/standards/#code-style","title":"Code Style","text":"<ul> <li>Python: Follow PEP 8, enforced by ruff</li> <li>Formatting: Automated with ruff format</li> <li>Type Hints: Encouraged but not required</li> <li>Docstrings: Google style</li> </ul>"},{"location":"contributing/standards/#testing","title":"Testing","text":"<ul> <li>Write tests for new functionality</li> <li>Maintain or improve coverage</li> <li>Use pytest fixtures for common setup</li> </ul>"},{"location":"contributing/standards/#documentation","title":"Documentation","text":"<ul> <li>Update docs for user-facing changes</li> <li>Include code examples</li> <li>Keep manifest.reference.yaml in sync</li> </ul>"},{"location":"contributing/testing/","title":"Testing Guide","text":"<p>How to write and run tests for Monitoring Hub.</p>"},{"location":"contributing/testing/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\nmake test\n\n# Run with coverage\nmake test-cov\n\n# Run specific test file\npytest core/tests/test_builder.py\n\n# Run specific test\npytest core/tests/test_builder.py::test_load_valid_manifest\n</code></pre>"},{"location":"contributing/testing/#writing-tests","title":"Writing Tests","text":""},{"location":"contributing/testing/#unit-tests","title":"Unit Tests","text":"<pre><code>def test_my_function():\n    result = my_function(\"input\")\n    assert result == \"expected\"\n</code></pre>"},{"location":"contributing/testing/#using-fixtures","title":"Using Fixtures","text":"<pre><code>def test_with_manifest(sample_manifest):\n    assert sample_manifest['name'] == 'test_exporter'\n</code></pre>"},{"location":"contributing/testing/#test-structure","title":"Test Structure","text":"<ul> <li><code>core/tests/</code> - Test files</li> <li><code>core/tests/fixtures/</code> - Test data</li> <li><code>core/tests/conftest.py</code> - Shared fixtures</li> </ul> <p>See conftest.py for available fixtures.</p>"},{"location":"contributing/testing/#code-quality-checks","title":"Code Quality Checks","text":""},{"location":"contributing/testing/#python-linting","title":"Python Linting","text":"<p>Using <code>ruff</code> for fast linting and formatting:</p> <pre><code># Check linting\nmake lint\n\n# Auto-fix issues\nmake lint-fix\n\n# Format code\nmake format\n\n# Type checking\nmake type-check\n</code></pre>"},{"location":"contributing/testing/#css-linting","title":"CSS Linting","text":"<p>Using <code>stylelint</code> for CSS quality in portal and templates:</p> <pre><code># Check CSS linting\nmake docker-lint-css\n\n# Auto-fix CSS issues\nmake docker-lint-fix\n</code></pre> <p>Configuration files: - <code>.stylelintrc.json</code> - Stylelint rules (standard + order plugin) - <code>.stylelintignore</code> - Ignored files (generated files, external assets)</p> <p>Stylelint validates: - Standalone CSS files - Embedded <code>&lt;style&gt;</code> blocks in Jinja2 templates - Portal CSS (<code>core/templates/index.html.j2</code>)</p>"},{"location":"contributing/testing/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>Pre-commit hooks automatically run before each commit:</p> <pre><code># Install hooks\npre-commit install\n\n# Run manually\nmake pre-commit\n\n# Update hook versions\npre-commit autoupdate\n</code></pre> <p>Hooks enforce: - Trailing whitespace removal - YAML validation - Python linting (ruff) - Type checking (mypy) - File size limits</p>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>Choose your preferred installation method based on your environment.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>For YUM/DNF: Enterprise Linux 8, 9, or 10 (RHEL, CentOS Stream, AlmaLinux, Rocky Linux)</li> <li>For APT: Ubuntu 22.04/24.04 or Debian 12/13</li> <li>For Docker: Docker or Podman installed</li> <li>Architectures: x86_64 (amd64) or aarch64 (arm64)</li> </ul>"},{"location":"getting-started/installation/#yum-repository","title":"YUM Repository","text":""},{"location":"getting-started/installation/#add-gpg-key","title":"Add GPG Key","text":"<p>First, import the repository GPG key to verify package signatures:</p> <pre><code>sudo rpm --import https://sckyzo.github.io/monitoring-hub/RPM-GPG-KEY-monitoring-hub\n</code></pre>"},{"location":"getting-started/installation/#configure-repository","title":"Configure Repository","text":"EL9 (Recommended)EL10EL8 <pre><code>sudo dnf config-manager --add-repo https://sckyzo.github.io/monitoring-hub/el9/$(arch)/\n</code></pre> <pre><code>sudo dnf config-manager --add-repo https://sckyzo.github.io/monitoring-hub/el10/$(arch)/\n</code></pre> <pre><code>sudo dnf config-manager --add-repo https://sckyzo.github.io/monitoring-hub/el8/$(arch)/\n</code></pre>"},{"location":"getting-started/installation/#install-exporter","title":"Install Exporter","text":"<pre><code># Install any exporter\nsudo dnf install node_exporter\n\n# Enable and start service\nsudo systemctl enable --now node_exporter\n\n# Check status\nsudo systemctl status node_exporter\n</code></pre>"},{"location":"getting-started/installation/#list-available-packages","title":"List Available Packages","text":"<pre><code>dnf search monitoring-hub\n# or\ndnf list available | grep exporter\n</code></pre>"},{"location":"getting-started/installation/#apt-repository","title":"APT Repository","text":""},{"location":"getting-started/installation/#add-gpg-key_1","title":"Add GPG Key","text":"<p>First, add the repository GPG key to verify package signatures:</p> <pre><code>curl -fsSL https://sckyzo.github.io/monitoring-hub/apt/monitoring-hub.asc | \\\n  sudo gpg --dearmor -o /usr/share/keyrings/monitoring-hub.gpg\n</code></pre>"},{"location":"getting-started/installation/#configure-repository_1","title":"Configure Repository","text":"Ubuntu 24.04 (Noble)Ubuntu 22.04 (Jammy)Debian 13 (Trixie)Debian 12 (Bookworm) <pre><code>echo \"deb [signed-by=/usr/share/keyrings/monitoring-hub.gpg] \\\n  https://sckyzo.github.io/monitoring-hub/apt noble main\" | \\\n  sudo tee /etc/apt/sources.list.d/monitoring-hub.list\n</code></pre> <pre><code>echo \"deb [signed-by=/usr/share/keyrings/monitoring-hub.gpg] \\\n  https://sckyzo.github.io/monitoring-hub/apt jammy main\" | \\\n  sudo tee /etc/apt/sources.list.d/monitoring-hub.list\n</code></pre> <pre><code>echo \"deb [signed-by=/usr/share/keyrings/monitoring-hub.gpg] \\\n  https://sckyzo.github.io/monitoring-hub/apt trixie main\" | \\\n  sudo tee /etc/apt/sources.list.d/monitoring-hub.list\n</code></pre> <pre><code>echo \"deb [signed-by=/usr/share/keyrings/monitoring-hub.gpg] \\\n  https://sckyzo.github.io/monitoring-hub/apt bookworm main\" | \\\n  sudo tee /etc/apt/sources.list.d/monitoring-hub.list\n</code></pre>"},{"location":"getting-started/installation/#install-exporter_1","title":"Install Exporter","text":"<pre><code># Update package lists\nsudo apt update\n\n# Install any exporter\nsudo apt install node-exporter\n\n# Enable and start service\nsudo systemctl enable --now node-exporter\n\n# Check status\nsudo systemctl status node-exporter\n</code></pre> <p>Package Naming</p> <p>DEB packages use dashes instead of underscores (e.g., <code>node-exporter</code> instead of <code>node_exporter</code>).</p>"},{"location":"getting-started/installation/#list-available-packages_1","title":"List Available Packages","text":"<pre><code>apt-cache search monitoring-hub\n# or\napt list | grep exporter\n</code></pre>"},{"location":"getting-started/installation/#docker-podman","title":"Docker / Podman","text":""},{"location":"getting-started/installation/#pull-image","title":"Pull Image","text":"<pre><code># Using Docker\ndocker pull ghcr.io/sckyzo/monitoring-hub/node_exporter:latest\n\n# Using Podman\npodman pull ghcr.io/sckyzo/monitoring-hub/node_exporter:latest\n</code></pre>"},{"location":"getting-started/installation/#run-container","title":"Run Container","text":"<pre><code>docker run -d \\\n  --name node_exporter \\\n  -p 9100:9100 \\\n  --restart unless-stopped \\\n  ghcr.io/sckyzo/monitoring-hub/node_exporter:latest\n</code></pre>"},{"location":"getting-started/installation/#docker-compose","title":"Docker Compose","text":"<pre><code>version: '3.8'\n\nservices:\n  node_exporter:\n    image: ghcr.io/sckyzo/monitoring-hub/node_exporter:latest\n    container_name: node_exporter\n    ports:\n      - \"9100:9100\"\n    restart: unless-stopped\n</code></pre>"},{"location":"getting-started/installation/#verification","title":"Verification","text":"<p>Test that the exporter is running:</p> <pre><code># Check metrics endpoint\ncurl http://localhost:9100/metrics\n\n# Or\nwget -qO- http://localhost:9100/metrics\n</code></pre> <p>You should see Prometheus metrics output.</p>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start Guide - Get started with your first exporter</li> <li>User Guide - Learn how to add exporters</li> <li>Troubleshooting - Common issues and solutions</li> </ul>"},{"location":"getting-started/quick-start/","title":"Quick Start","text":"<p>Get up and running with Monitoring Hub in just a few minutes.</p>"},{"location":"getting-started/quick-start/#for-users-installing-exporters","title":"For Users (Installing Exporters)","text":""},{"location":"getting-started/quick-start/#1-configure-repository","title":"1. Configure Repository","text":"<pre><code>sudo dnf config-manager --add-repo https://sckyzo.github.io/monitoring-hub/el9/$(arch)/\n</code></pre>"},{"location":"getting-started/quick-start/#2-install-exporter","title":"2. Install Exporter","text":"<pre><code>sudo dnf install node_exporter\n</code></pre>"},{"location":"getting-started/quick-start/#3-start-service","title":"3. Start Service","text":"<pre><code>sudo systemctl enable --now node_exporter\n</code></pre>"},{"location":"getting-started/quick-start/#4-verify","title":"4. Verify","text":"<pre><code>curl http://localhost:9100/metrics\n</code></pre> <p>Done! The exporter is running and exposing metrics.</p>"},{"location":"getting-started/quick-start/#for-contributors-adding-exporters","title":"For Contributors (Adding Exporters)","text":""},{"location":"getting-started/quick-start/#1-clone-repository","title":"1. Clone Repository","text":"<pre><code>git clone https://github.com/SckyzO/monitoring-hub.git\ncd monitoring-hub\n</code></pre>"},{"location":"getting-started/quick-start/#2-build-development-environment","title":"2. Build Development Environment","text":"<p>No Python installation required - just Docker!</p> <pre><code>./devctl build\n</code></pre> <p>This creates a Docker image with all development tools pre-installed.</p>"},{"location":"getting-started/quick-start/#3-create-exporter","title":"3. Create Exporter","text":"<pre><code>./devctl create-exporter\n</code></pre> <p>Follow the interactive prompts:</p> <ul> <li>Name: <code>my_exporter</code></li> <li>Repo: <code>owner/my_exporter</code></li> <li>Category: Select appropriate category</li> <li>Description: Short description</li> </ul>"},{"location":"getting-started/quick-start/#4-test-locally","title":"4. Test Locally","text":"<pre><code>./devctl test-exporter my_exporter\n</code></pre> <p>This will build RPM + Docker image and run validation tests.</p>"},{"location":"getting-started/quick-start/#5-commit-and-push","title":"5. Commit and Push","text":"<pre><code>git checkout -b feature/add-my-exporter\ngit add exporters/my_exporter/\ngit commit -m \"feat(exporters): add my_exporter\"\ngit push origin feature/add-my-exporter\n</code></pre>"},{"location":"getting-started/quick-start/#6-create-pull-request","title":"6. Create Pull Request","text":"<p>Open a PR on GitHub. CI will automatically:</p> <ul> <li>Validate the manifest</li> <li>Build RPM packages</li> <li>Build Docker images</li> <li>Run validation tests</li> </ul> <p>Once approved and merged, your exporter will be automatically deployed!</p>"},{"location":"getting-started/quick-start/#whats-next","title":"What's Next?","text":"<ul> <li>Users: Installation Guide for more installation options</li> <li>Contributors: Adding Exporters for detailed guide</li> <li>Developers: Development Setup for setting up dev environment</li> </ul>"},{"location":"getting-started/requirements/","title":"Requirements","text":""},{"location":"getting-started/requirements/#for-using-exporters","title":"For Using Exporters","text":""},{"location":"getting-started/requirements/#system-requirements","title":"System Requirements","text":"<ul> <li>OS: Enterprise Linux 8, 9, or 10 (RHEL, AlmaLinux, Rocky Linux, CentOS Stream)</li> <li>Architecture: x86_64 or aarch64 (ARM64)</li> <li>Disk Space: ~50MB per exporter</li> <li>Memory: Varies by exporter (typically 10-50MB)</li> </ul>"},{"location":"getting-started/requirements/#runtime-requirements","title":"Runtime Requirements","text":"<ul> <li>systemd: For managing services</li> <li>Network: Internet access for DNF repository (or local mirror)</li> </ul>"},{"location":"getting-started/requirements/#for-development","title":"For Development","text":""},{"location":"getting-started/requirements/#required-software","title":"Required Software","text":"<ul> <li>Python: 3.9 or higher</li> <li>Docker: 20.10+ or Podman 3.0+</li> <li>Git: 2.0+</li> </ul>"},{"location":"getting-started/requirements/#python-dependencies","title":"Python Dependencies","text":"<p>Install via <code>requirements.txt</code>:</p> <pre><code>pip install -r requirements/base.txt\n</code></pre> <p>Dependencies include:</p> <ul> <li><code>PyYAML</code> - YAML parsing</li> <li><code>Jinja2</code> - Template rendering</li> <li><code>marshmallow</code> - Schema validation</li> <li><code>requests</code> - HTTP client</li> <li><code>click</code> - CLI framework</li> </ul>"},{"location":"getting-started/requirements/#development-dependencies","title":"Development Dependencies","text":"<p>For running tests and linting:</p> <pre><code>pip install -r requirements/dev.txt\n</code></pre> <p>Includes:</p> <ul> <li><code>pytest</code> - Testing framework</li> <li><code>ruff</code> - Linting and formatting</li> <li><code>mypy</code> - Type checking</li> <li><code>pre-commit</code> - Git hooks</li> </ul>"},{"location":"getting-started/requirements/#documentation-dependencies","title":"Documentation Dependencies","text":"<p>For building docs locally:</p> <pre><code>pip install -r requirements/docs.txt\n</code></pre>"},{"location":"getting-started/requirements/#for-building-exporters","title":"For Building Exporters","text":""},{"location":"getting-started/requirements/#build-environment","title":"Build Environment","text":"<ul> <li>Docker/Podman: Required for RPM builds</li> <li>rpmbuild: Containerized (no host installation needed)</li> <li>Multi-arch: Requires QEMU for cross-compilation (handled by Docker)</li> </ul>"},{"location":"getting-started/requirements/#disk-space","title":"Disk Space","text":"<ul> <li>Per build: ~500MB temporary space</li> <li>Cache: ~2GB for Docker images</li> </ul>"},{"location":"getting-started/requirements/#network-requirements","title":"Network Requirements","text":""},{"location":"getting-started/requirements/#for-cicd","title":"For CI/CD","text":"<ul> <li>Access to <code>github.com</code> (releases, API)</li> <li>Access to <code>ghcr.io</code> (container registry)</li> <li>Access to GitHub Pages (publishing)</li> </ul>"},{"location":"getting-started/requirements/#for-local-development","title":"For Local Development","text":"<ul> <li>Internet access for downloading upstream binaries</li> <li>Optional: GitHub CLI (<code>gh</code>) for enhanced features</li> </ul>"},{"location":"getting-started/requirements/#optional-tools","title":"Optional Tools","text":"<ul> <li>gh CLI: Enhanced GitHub integration</li> <li>make: For using Makefile commands</li> <li>jq: JSON processing in scripts</li> <li>curl/wget: Testing endpoints</li> </ul>"},{"location":"getting-started/requirements/#browser-requirements-portal","title":"Browser Requirements (Portal)","text":"<ul> <li>Modern browsers: Chrome 90+, Firefox 88+, Safari 14+</li> <li>JavaScript: Required</li> <li>Responsive: Works on mobile devices</li> </ul>"},{"location":"user-guide/adding-exporters/","title":"Adding Exporters","text":"<p>Learn how to add a new Prometheus exporter to the Monitoring Hub.</p>"},{"location":"user-guide/adding-exporters/#overview","title":"Overview","text":"<p>Adding a new exporter is a simple process that takes less than 5 minutes. The factory handles all the complexity of building, packaging, and distributing your exporter.</p>"},{"location":"user-guide/adding-exporters/#step-1-create-exporter-scaffold","title":"Step 1: Create Exporter Scaffold","text":"<p>Use the Docker-first creator tool:</p> <pre><code>./devctl create-exporter\n</code></pre> <p>Or using Make: <pre><code>make create-exporter\n</code></pre></p> <p>You'll be prompted for:</p> <ul> <li>Exporter Name: Technical name (e.g., <code>redis_exporter</code>)</li> <li>GitHub Repository: Upstream repo (e.g., <code>oliver006/redis_exporter</code>)</li> <li>Category: Choose from System, Database, Web, Network, etc.</li> <li>Description: Short one-liner description</li> </ul> <p>The script will automatically:</p> <ul> <li>Fetch latest version from GitHub</li> <li>Detect supported architectures</li> <li>Create <code>exporters/my_exporter/</code> directory</li> <li>Generate <code>manifest.yaml</code> with sensible defaults</li> <li>Create <code>README.md</code> template</li> <li>Create <code>assets/</code> directory for config files</li> </ul> <p>Note: No Python installation required - everything runs in Docker!</p>"},{"location":"user-guide/adding-exporters/#step-2-review-and-customize-manifest","title":"Step 2: Review and Customize Manifest","text":"<p>Open <code>exporters/my_exporter/manifest.yaml</code> and review:</p>"},{"location":"user-guide/adding-exporters/#basic-information","title":"Basic Information","text":"<pre><code>name: my_exporter\ndescription: Exports metrics for My Service\ncategory: Database\nversion: v1.2.3\n# Optional: License will be auto-detected from GitHub if not specified\n# license: Apache-2.0\n</code></pre> <p>License Auto-Detection: The builder automatically queries the GitHub API to detect the upstream project's license (SPDX format). If detection fails or the field is omitted, it defaults to <code>Apache-2.0</code>. Common values include: - <code>MIT</code> - <code>Apache-2.0</code> - <code>GPL-3.0</code> - <code>BSD-3-Clause</code> - <code>MPL-2.0</code></p>"},{"location":"user-guide/adding-exporters/#upstream-configuration","title":"Upstream Configuration","text":"<pre><code>upstream:\n  type: github\n  repo: owner/my_exporter\n  strategy: latest_release\n</code></pre>"},{"location":"user-guide/adding-exporters/#custom-archive-naming","title":"Custom Archive Naming","text":"<p>If the upstream release artifacts don't follow standard naming conventions, you can define custom patterns:</p> <p>Option 1: String Pattern (Most Common) <pre><code>upstream:\n  archive_name: \"{name}-{clean_version}-linux-{upstream_linux_arch}.tar.gz\"\n</code></pre></p> <p>Available variables: - <code>{name}</code>, <code>{version}</code>, <code>{clean_version}</code>: Project identifiers - <code>{arch}</code>: Standard (amd64, arm64) - <code>{rpm_arch}</code>: RPM convention (x86_64, aarch64) - <code>{deb_arch}</code>: DEB convention (amd64, arm64) - <code>{upstream_linux_arch}</code>: Mixed convention (x86_64, arm64)</p> <p>Option 2: Dict Format (Per-Architecture Patterns) <pre><code>upstream:\n  archive_name:\n    amd64: \"my_exporter-v{clean_version}-x86_64-special.tar.gz\"\n    arm64: \"my_exporter-v{clean_version}-arm64-custom.tar.gz\"\n</code></pre></p> <p>Use the dict format when upstream uses completely different naming per architecture.</p> <p>Real-World Example: NATS Exporter <pre><code># NATS uses mixed convention: x86_64 for amd64, arm64 for arm64\nupstream:\n  archive_name: \"prometheus-nats-exporter-v{clean_version}-linux-{upstream_linux_arch}.tar.gz\"\n  # Resolves to:\n  # - amd64: prometheus-nats-exporter-v0.19.1-linux-x86_64.tar.gz\n  # - arm64: prometheus-nats-exporter-v0.19.1-linux-arm64.tar.gz\n</code></pre></p>"},{"location":"user-guide/adding-exporters/#build-configuration","title":"Build Configuration","text":"<pre><code>build:\n  method: binary_repack\n  binary_name: my_exporter\n  archs:\n    - amd64\n    - arm64\n  # Optional: additional binaries from the archive\n  extra_binaries: []\n  # Optional: extra files to download\n  extra_sources: []\n</code></pre>"},{"location":"user-guide/adding-exporters/#rpm-artifacts","title":"RPM Artifacts","text":"<pre><code>artifacts:\n  rpm:\n    enabled: true\n    summary: My Service Prometheus Exporter\n    targets: [el8, el9, el10]\n    systemd:\n      enabled: true\n      # Optional: command-line arguments\n      arguments: [\"--config.file=/etc/my_exporter/config.yml\"]\n    # Optional: config files\n    extra_files:\n      - source: assets/config.yml\n        dest: /etc/my_exporter/config.yml\n        mode: \"0640\"\n        config: true\n    # Optional: data directories\n    directories:\n      - path: /var/lib/my_exporter\n        owner: my_exporter\n        group: my_exporter\n    # Optional: system user\n    system_user: my_exporter\n    # Optional: dependencies\n    dependencies: []\n</code></pre>"},{"location":"user-guide/adding-exporters/#docker-artifacts","title":"Docker Artifacts","text":"<pre><code>artifacts:\n  docker:\n    enabled: true\n    base_image: registry.access.redhat.com/ubi9/ubi-minimal:latest\n    entrypoint: [\"/usr/bin/my_exporter\"]\n    cmd: []\n    validation:\n      enabled: true\n      port: 9121\n</code></pre>"},{"location":"user-guide/adding-exporters/#step-3-add-configuration-files-optional","title":"Step 3: Add Configuration Files (Optional)","text":"<p>If your exporter needs configuration files:</p> <ol> <li>Create files in <code>exporters/my_exporter/assets/</code></li> <li>Reference them in the manifest's <code>extra_files</code> section</li> </ol> <p>Example:</p> <pre><code># Create config file\ncat &gt; exporters/my_exporter/assets/config.yml &lt;&lt; 'EOF'\nlisten_address: \":9121\"\nlog_level: info\nEOF\n</code></pre>"},{"location":"user-guide/adding-exporters/#step-4-test-locally","title":"Step 4: Test Locally","text":"<p>Run the comprehensive test using Docker:</p> <pre><code>./devctl test-exporter my_exporter\n</code></pre> <p>Or using Make: <pre><code>make test-exporter EXPORTER=my_exporter\n</code></pre></p> <p>This will:</p> <ul> <li>Generate build artifacts</li> <li>Build RPM for EL9/x86_64</li> <li>Build Docker image</li> <li>Run validation tests</li> </ul> <p>For specific options:</p> <pre><code># Test ARM64 build\n./devctl test-exporter my_exporter --arch arm64\n\n# Test specific EL version\n./devctl test-exporter my_exporter --el10\n\n# Enable smoke tests\n./devctl test-exporter my_exporter --smoke\n</code></pre> <p>Quick artifact generation only: <pre><code>./devctl build-exporter my_exporter\n# Output: build/my_exporter/\n</code></pre></p>"},{"location":"user-guide/adding-exporters/#step-5-create-pull-request","title":"Step 5: Create Pull Request","text":"<p>Once local testing passes:</p> <pre><code># Create feature branch\ngit checkout -b feature/add-my-exporter\n\n# Add files\ngit add exporters/my_exporter/\n\n# Commit using conventional commits\ngit commit -m \"feat(exporters): add my_exporter support\n\n- Add manifest for my_exporter v1.2.3\n- Include default configuration\n- Support amd64 and arm64 architectures\"\n\n# Push branch\ngit push origin feature/add-my-exporter\n</code></pre> <p>Create a Pull Request on GitHub. CI will automatically:</p> <p>\u2705 Validate manifest schema \u2705 Build RPM packages (all distros + archs) \u2705 Build Docker images (multi-arch) \u2705 Run validation tests \u2705 Update catalog</p>"},{"location":"user-guide/adding-exporters/#advanced-custom-templates","title":"Advanced: Custom Templates","text":"<p>For complex packaging needs, you can override the default templates.</p>"},{"location":"user-guide/adding-exporters/#custom-rpm-spec","title":"Custom RPM Spec","text":"<p>Create <code>exporters/my_exporter/templates/my_exporter.spec.j2</code>:</p> <pre><code>%define debug_package %{nil}\n\nName: {{ name }}\nVersion: {{ version }}\n# ... custom spec logic ...\n\n%post\n# Custom post-install logic\nif [ $1 -eq 1 ]; then\n    # First install\n    /usr/bin/my_exporter --init-db\nfi\n</code></pre>"},{"location":"user-guide/adding-exporters/#custom-dockerfile","title":"Custom Dockerfile","text":"<p>Create <code>exporters/my_exporter/templates/Dockerfile.j2</code>:</p> <pre><code>FROM {{ artifacts.docker.base_image }}\n\n# Install dependencies\nRUN microdnf install -y ca-certificates curl &amp;&amp; \\\n    microdnf clean all\n\n# Copy binary\nCOPY {{ build.binary_name }} /usr/bin/{{ name }}\n\n# Custom setup\nRUN /usr/bin/{{ name }} --version\n\nENTRYPOINT {{ artifacts.docker.entrypoint | tojson }}\n</code></pre>"},{"location":"user-guide/adding-exporters/#common-patterns","title":"Common Patterns","text":""},{"location":"user-guide/adding-exporters/#simple-exporter-no-config","title":"Simple Exporter (No Config)","text":"<p>Minimal manifest with just the binary:</p> <pre><code>artifacts:\n  rpm:\n    enabled: true\n    systemd:\n      enabled: true\n  docker:\n    enabled: true\n    entrypoint: [\"/usr/bin/my_exporter\"]\n</code></pre>"},{"location":"user-guide/adding-exporters/#exporter-with-config-file","title":"Exporter with Config File","text":"<p>Include configuration management:</p> <pre><code>artifacts:\n  rpm:\n    system_user: my_exporter\n    extra_files:\n      - source: assets/config.yml\n        dest: /etc/my_exporter/config.yml\n        mode: \"0640\"\n        config: true\n    systemd:\n      arguments: [\"--config.file=/etc/my_exporter/config.yml\"]\n</code></pre>"},{"location":"user-guide/adding-exporters/#multi-binary-package","title":"Multi-Binary Package","text":"<p>Include helper tools:</p> <pre><code>build:\n  binary_name: main_tool\n  extra_binaries:\n    - helper_tool\n    - cli_tool\n</code></pre>"},{"location":"user-guide/adding-exporters/#troubleshooting","title":"Troubleshooting","text":"<p>See Troubleshooting Guide for common issues.</p>"},{"location":"user-guide/adding-exporters/#next-steps","title":"Next Steps","text":"<ul> <li>Manifest Reference - Complete manifest documentation</li> <li>Local Testing - Advanced testing techniques</li> <li>Contributing Guide - Development best practices</li> </ul>"},{"location":"user-guide/local-testing/","title":"Local Testing","text":"<p>Learn how to test exporters locally before submitting PRs using our Docker-first workflow.</p>"},{"location":"user-guide/local-testing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker - The only requirement!</li> </ul>"},{"location":"user-guide/local-testing/#quick-test-recommended","title":"Quick Test (Recommended)","text":"<p>Test an exporter with full RPM + Docker build + validation:</p> <pre><code>./devctl test-exporter my_exporter\n</code></pre> <p>Or using Make: <pre><code>make test-exporter EXPORTER=my_exporter\n</code></pre></p> <p>This automatically: - \u2705 Validates manifest schema - \u2705 Generates build artifacts - \u2705 Builds RPM for EL9/x86_64 - \u2705 Builds Docker image - \u2705 Runs smoke tests (if configured)</p>"},{"location":"user-guide/local-testing/#advanced-options","title":"Advanced Options","text":""},{"location":"user-guide/local-testing/#test-specific-architecture","title":"Test Specific Architecture","text":"<pre><code>./devctl test-exporter my_exporter --arch arm64\n</code></pre> <p>Supported architectures: <code>amd64</code>, <code>arm64</code></p>"},{"location":"user-guide/local-testing/#test-specific-el-version","title":"Test Specific EL Version","text":"<pre><code>./devctl test-exporter my_exporter --el8   # Enterprise Linux 8\n./devctl test-exporter my_exporter --el9   # Enterprise Linux 9 (default)\n./devctl test-exporter my_exporter --el10  # Enterprise Linux 10\n</code></pre>"},{"location":"user-guide/local-testing/#test-deb-builds","title":"Test DEB Builds","text":"<p>Test Debian/Ubuntu package build:</p> <pre><code>./devctl test-deb my_exporter                    # Ubuntu 22.04 / amd64 (default)\n./devctl test-deb my_exporter ubuntu-24.04       # Ubuntu 24.04\n./devctl test-deb my_exporter debian-12          # Debian 12\n./devctl test-deb my_exporter debian-13 arm64    # Debian 13 / arm64\n</code></pre> <p>Supported distributions: - <code>ubuntu-22.04</code>, <code>ubuntu-24.04</code> - <code>debian-12</code>, <code>debian-13</code></p> <p>Supported architectures: <code>amd64</code>, <code>arm64</code></p>"},{"location":"user-guide/local-testing/#enable-smoke-tests","title":"Enable Smoke Tests","text":"<pre><code>./devctl test-exporter my_exporter --smoke\n</code></pre> <p>Smoke tests verify: - Container starts successfully - Metrics endpoint responds with HTTP 200 - Binary version command executes</p>"},{"location":"user-guide/local-testing/#build-docker-image-only","title":"Build Docker Image Only","text":"<pre><code>./devctl test-exporter my_exporter --docker\n</code></pre>"},{"location":"user-guide/local-testing/#build-artifacts-only","title":"Build Artifacts Only","text":"<p>If you just need to generate artifacts without building packages:</p> <pre><code>./devctl build-exporter my_exporter\n</code></pre> <p>Output directory: <code>build/my_exporter/</code></p> <p>Generated files: - <code>my_exporter.spec</code> - RPM spec file - <code>Dockerfile</code> - Container build file - Binaries and assets</p>"},{"location":"user-guide/local-testing/#manual-build-steps-advanced","title":"Manual Build Steps (Advanced)","text":"<p>For debugging, you can run each step inside the development container:</p>"},{"location":"user-guide/local-testing/#1-open-development-shell","title":"1. Open Development Shell","text":"<pre><code>./devctl shell\n</code></pre>"},{"location":"user-guide/local-testing/#2-generate-artifacts","title":"2. Generate Artifacts","text":"<pre><code>python3 -m core.engine.builder \\\n  --manifest exporters/my_exporter/manifest.yaml \\\n  --arch amd64 \\\n  --output-dir build/my_exporter\n</code></pre>"},{"location":"user-guide/local-testing/#3-build-rpm","title":"3. Build RPM","text":"<pre><code>./core/scripts/build_rpm.sh \\\n  build/my_exporter/my_exporter.spec \\\n  build/my_exporter/rpms \\\n  amd64 \\\n  almalinux:9\n</code></pre>"},{"location":"user-guide/local-testing/#4-build-docker-image","title":"4. Build Docker Image","text":"<pre><code>docker build -t monitoring-hub/my_exporter:local build/my_exporter\n</code></pre>"},{"location":"user-guide/local-testing/#5-test-container","title":"5. Test Container","text":"<pre><code># Start container\ndocker run -d -p 9100:9100 --name test_exporter \\\n  monitoring-hub/my_exporter:local\n\n# Check metrics\ncurl http://localhost:9100/metrics\n\n# Cleanup\ndocker stop test_exporter &amp;&amp; docker rm test_exporter\n</code></pre>"},{"location":"user-guide/local-testing/#url-validation","title":"URL Validation","text":"<p>Validate GitHub release URLs before building:</p> <pre><code># Validate all exporters\nmake validate-urls\n\n# Validate specific exporter\nmake validate-url EXPORTER=node_exporter\n\n# With verbose output (show successful URLs)\n./devctl validate-urls --verbose\n\n# Test specific architectures only\n./devctl validate-urls --arch amd64\n\n# Fail on errors (useful for CI)\n./devctl validate-urls --fail-on-error\n</code></pre> <p>This checks that GitHub release assets are accessible and correctly named: - \u2705 Constructs expected URLs from manifest configuration - \u2705 Tests HTTP accessibility (HEAD requests) - \u2705 Reports success/failed/partial results - \u26a0\ufe0f Partial = some architectures missing (often arm64 unavailable upstream)</p> <p>When to use: - Before building new exporters - After updating versions in manifests - When upstream releases change naming patterns - To debug 404 download errors</p>"},{"location":"user-guide/local-testing/#validation-checklist","title":"Validation Checklist","text":"<p>The test script automatically validates:</p> <ul> <li>\u2705 Manifest schema is valid (marshmallow validation)</li> <li>\u2705 Binary downloads successfully from upstream</li> <li>\u2705 Archive extraction works correctly</li> <li>\u2705 RPM spec renders without errors</li> <li>\u2705 RPM builds successfully</li> <li>\u2705 Docker image builds</li> <li>\u2705 Container starts (if smoke tests enabled)</li> <li>\u2705 Metrics endpoint responds (if validation.port configured)</li> <li>\u2705 Version command succeeds (if validation.command configured)</li> </ul>"},{"location":"user-guide/local-testing/#quick-commands-reference","title":"Quick Commands Reference","text":"<pre><code># List all exporters\n./devctl list-exporters\n\n# Build artifacts only\n./devctl build-exporter &lt;name&gt;\n\n# Full test with defaults\n./devctl test-exporter &lt;name&gt;\n\n# Test with all options\n./devctl test-exporter &lt;name&gt; --arch arm64 --el10 --docker --smoke\n\n# Open interactive shell\n./devctl shell\n\n# Run Python command\n./devctl python -m core.engine.builder --help\n</code></pre>"},{"location":"user-guide/local-testing/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/local-testing/#image-not-found","title":"Image Not Found","text":"<p>If you see \"Development image not found\", build it first:</p> <pre><code>./devctl build\n</code></pre>"},{"location":"user-guide/local-testing/#permission-denied-on-rpm-build","title":"Permission Denied on RPM Build","text":"<p>Ensure Docker daemon is running and your user has permissions:</p> <pre><code>docker ps\n</code></pre>"},{"location":"user-guide/local-testing/#manifest-validation-errors","title":"Manifest Validation Errors","text":"<p>Check your manifest against the reference:</p> <pre><code># View reference manifest\ncat manifest.reference.yaml\n\n# Validate specific manifest\n./devctl shell\npython3 -m core.engine.schema\n</code></pre> <p>See Troubleshooting Guide for more common issues.</p>"},{"location":"user-guide/local-testing/#next-steps","title":"Next Steps","text":"<ul> <li>Adding Exporters - Complete guide</li> <li>Manifest Reference - Schema documentation</li> <li>Development Guide - Contributing workflow</li> </ul>"},{"location":"user-guide/manifest-reference/","title":"Manifest Reference","text":"<p>Complete reference for the exporter manifest YAML format.</p>"},{"location":"user-guide/manifest-reference/#quick-example","title":"Quick Example","text":"<pre><code>name: my_exporter\ndescription: Exports metrics for My Service\ncategory: Database\nversion: v1.2.3\n\nupstream:\n  type: github\n  repo: owner/my_exporter\n  strategy: latest_release\n\nbuild:\n  method: binary_repack\n  binary_name: my_exporter\n  archs: [amd64, arm64]\n\nartifacts:\n  rpm:\n    enabled: true\n    systemd:\n      enabled: true\n  docker:\n    enabled: true\n    entrypoint: [\"/usr/bin/my_exporter\"]\n    validation:\n      port: 9100\n</code></pre>"},{"location":"user-guide/manifest-reference/#complete-reference","title":"Complete Reference","text":"<p>For the full manifest schema with all available options, see the manifest.reference.yaml file in the repository.</p>"},{"location":"user-guide/manifest-reference/#field-descriptions","title":"Field Descriptions","text":""},{"location":"user-guide/manifest-reference/#identity","title":"Identity","text":"<ul> <li><code>name</code> (required): Technical name (e.g., <code>node_exporter</code>)</li> <li><code>description</code> (required): Short description</li> <li><code>category</code> (required): System, Database, Web, Network, etc.</li> <li><code>version</code> (required): Upstream version (e.g., <code>v1.2.3</code>)</li> </ul>"},{"location":"user-guide/manifest-reference/#upstream","title":"Upstream","text":"<ul> <li><code>type</code>: Always <code>github</code></li> <li><code>repo</code> (required): GitHub repository (e.g., <code>prometheus/node_exporter</code>)</li> <li><code>strategy</code>: <code>latest_release</code> (default) or <code>pinned</code></li> <li><code>archive_name</code>: Custom archive name pattern (optional)</li> </ul>"},{"location":"user-guide/manifest-reference/#archive-name-patterns","title":"Archive Name Patterns","text":"<p>The <code>archive_name</code> field supports two formats for handling non-standard upstream naming:</p> <p>1. String Pattern (Most Common)</p> <p>Use template variables for simple patterns:</p> <pre><code>archive_name: \"{name}-{clean_version}.linux-{arch}.tar.gz\"\n</code></pre> <p>Available variables: - <code>{name}</code>: Exporter name - <code>{version}</code>: Raw version (e.g., v1.0.0) - <code>{clean_version}</code>: Version without 'v' prefix (e.g., 1.0.0) - <code>{arch}</code>: Standard arch (amd64, arm64) - <code>{rpm_arch}</code>: RPM convention (x86_64, aarch64) - <code>{deb_arch}</code>: DEB convention (amd64, arm64) - <code>{upstream_linux_arch}</code>: Mixed convention (x86_64, arm64)</p> <p>2. Dict Format (Per-Architecture Patterns)</p> <p>Use when upstream has completely different naming per architecture:</p> <pre><code>archive_name:\n  amd64: \"project-v{clean_version}-x86_64-special.tar.gz\"\n  arm64: \"project-v{clean_version}-arm64-custom.tar.gz\"\n</code></pre> <p>This is useful when: - Upstream uses inconsistent naming across architectures - Each architecture has a unique file structure - Standard variables don't provide enough flexibility</p>"},{"location":"user-guide/manifest-reference/#build","title":"Build","text":"<ul> <li><code>method</code>: <code>binary_repack</code> or <code>source_build</code></li> <li><code>binary_name</code> (required): Main binary name</li> <li><code>archs</code>: List of architectures (<code>amd64</code>, <code>arm64</code>)</li> <li><code>extra_binaries</code>: Additional binaries to extract</li> <li><code>extra_sources</code>: External files to download</li> </ul>"},{"location":"user-guide/manifest-reference/#artifacts-rpm","title":"Artifacts - RPM","text":"<ul> <li><code>enabled</code>: Enable RPM generation</li> <li><code>summary</code>: Package summary</li> <li><code>targets</code>: EL versions (<code>el8</code>, <code>el9</code>, <code>el10</code>)</li> <li><code>systemd.enabled</code>: Create systemd service</li> <li><code>systemd.arguments</code>: Command-line args</li> <li><code>system_user</code>: Create system user</li> <li><code>extra_files</code>: Config files to include</li> <li><code>directories</code>: Data directories to create</li> <li><code>dependencies</code>: Package dependencies</li> </ul>"},{"location":"user-guide/manifest-reference/#artifacts-docker","title":"Artifacts - Docker","text":"<ul> <li><code>enabled</code>: Enable Docker image</li> <li><code>base_image</code>: Base container image</li> <li><code>entrypoint</code>: Container entrypoint</li> <li><code>cmd</code>: Container command</li> <li><code>validation.enabled</code>: Enable port validation</li> <li><code>validation.port</code>: Port to check</li> </ul> <p>See Adding Exporters for practical examples.</p>"},{"location":"user-guide/package-repositories/","title":"Package Repositories","text":"<p>Monitoring Hub provides multiple distribution channels for installing exporters on different platforms.</p>"},{"location":"user-guide/package-repositories/#distribution-overview","title":"Distribution Overview","text":"Platform Package Format Distributions Architectures Red Hat RPM EL8, EL9, EL10 x86_64, aarch64 Debian/Ubuntu DEB Ubuntu 22.04/24.04, Debian 12/13 amd64, arm64 Containers OCI All amd64, arm64"},{"location":"user-guide/package-repositories/#yum-repository-rpm","title":"YUM Repository (RPM)","text":""},{"location":"user-guide/package-repositories/#supported-distributions","title":"Supported Distributions","text":"<ul> <li>RHEL 8, 9, 10</li> <li>CentOS Stream 8, 9, 10</li> <li>AlmaLinux 8, 9, 10</li> <li>Rocky Linux 8, 9, 10</li> </ul>"},{"location":"user-guide/package-repositories/#installation","title":"Installation","text":"<pre><code># Import GPG key for package verification\nsudo rpm --import https://sckyzo.github.io/monitoring-hub/RPM-GPG-KEY-monitoring-hub\n\n# Add repository (replace el9 with your version)\nsudo dnf config-manager --add-repo https://sckyzo.github.io/monitoring-hub/el9/$(arch)/\n\n# Install exporter\nsudo dnf install node_exporter\n\n# Enable and start service\nsudo systemctl enable --now node_exporter\n</code></pre>"},{"location":"user-guide/package-repositories/#repository-structure","title":"Repository Structure","text":"<pre><code>https://sckyzo.github.io/monitoring-hub/\n\u251c\u2500\u2500 el8/\n\u2502   \u251c\u2500\u2500 x86_64/\n\u2502   \u2502   \u251c\u2500\u2500 repodata/\n\u2502   \u2502   \u2514\u2500\u2500 *.rpm\n\u2502   \u2514\u2500\u2500 aarch64/\n\u251c\u2500\u2500 el9/\n\u2502   \u251c\u2500\u2500 x86_64/\n\u2502   \u2514\u2500\u2500 aarch64/\n\u2514\u2500\u2500 el10/\n    \u251c\u2500\u2500 x86_64/\n    \u2514\u2500\u2500 aarch64/\n</code></pre>"},{"location":"user-guide/package-repositories/#manual-rpm-installation","title":"Manual RPM Installation","text":"<p>Download directly from GitHub Releases:</p> <pre><code># Find the latest release\ncurl -s https://api.github.com/repos/SckyzO/monitoring-hub/releases | \\\n  jq -r '.[0].assets[] | select(.name | contains(\"node_exporter\")) | .browser_download_url'\n\n# Install directly\nsudo dnf install https://github.com/SckyzO/monitoring-hub/releases/download/node_exporter-v1.8.0/node_exporter-1.8.0-1.el9.x86_64.rpm\n</code></pre>"},{"location":"user-guide/package-repositories/#apt-repository-deb","title":"APT Repository (DEB)","text":""},{"location":"user-guide/package-repositories/#supported-distributions_1","title":"Supported Distributions","text":"<ul> <li>Ubuntu 22.04 (Jammy Jellyfish)</li> <li>Ubuntu 24.04 (Noble Numbat)</li> <li>Debian 12 (Bookworm)</li> <li>Debian 13 (Trixie)</li> </ul>"},{"location":"user-guide/package-repositories/#installation_1","title":"Installation","text":""},{"location":"user-guide/package-repositories/#1-add-gpg-key","title":"1. Add GPG Key","text":"<pre><code>curl -fsSL https://sckyzo.github.io/monitoring-hub/apt/monitoring-hub.asc | \\\n  sudo gpg --dearmor -o /usr/share/keyrings/monitoring-hub.gpg\n</code></pre>"},{"location":"user-guide/package-repositories/#2-add-repository","title":"2. Add Repository","text":"<p>Choose your distribution codename:</p> <pre><code># Ubuntu 24.04 (Noble)\necho \"deb [signed-by=/usr/share/keyrings/monitoring-hub.gpg] \\\n  https://sckyzo.github.io/monitoring-hub/apt noble main\" | \\\n  sudo tee /etc/apt/sources.list.d/monitoring-hub.list\n\n# Ubuntu 22.04 (Jammy)\necho \"deb [signed-by=/usr/share/keyrings/monitoring-hub.gpg] \\\n  https://sckyzo.github.io/monitoring-hub/apt jammy main\" | \\\n  sudo tee /etc/apt/sources.list.d/monitoring-hub.list\n\n# Debian 13 (Trixie)\necho \"deb [signed-by=/usr/share/keyrings/monitoring-hub.gpg] \\\n  https://sckyzo.github.io/monitoring-hub/apt trixie main\" | \\\n  sudo tee /etc/apt/sources.list.d/monitoring-hub.list\n\n# Debian 12 (Bookworm)\necho \"deb [signed-by=/usr/share/keyrings/monitoring-hub.gpg] \\\n  https://sckyzo.github.io/monitoring-hub/apt bookworm main\" | \\\n  sudo tee /etc/apt/sources.list.d/monitoring-hub.list\n</code></pre>"},{"location":"user-guide/package-repositories/#3-install-exporter","title":"3. Install Exporter","text":"<pre><code>sudo apt update\nsudo apt install node-exporter\nsudo systemctl enable --now node-exporter\n</code></pre> <p>Package Naming Convention</p> <p>DEB packages use dashes instead of underscores:</p> <ul> <li>RPM: <code>node_exporter</code></li> <li>DEB: <code>node-exporter</code></li> </ul>"},{"location":"user-guide/package-repositories/#repository-structure_1","title":"Repository Structure","text":"<pre><code>https://sckyzo.github.io/monitoring-hub/apt/\n\u251c\u2500\u2500 monitoring-hub.asc         # GPG public key\n\u251c\u2500\u2500 pool/main/                 # Package pool\n\u2502   \u251c\u2500\u2500 node-exporter_1.8.0_amd64.deb\n\u2502   \u2514\u2500\u2500 blackbox-exporter_0.28.0_arm64.deb\n\u2514\u2500\u2500 dists/\n    \u251c\u2500\u2500 jammy/                 # Ubuntu 22.04\n    \u2502   \u251c\u2500\u2500 InRelease\n    \u2502   \u251c\u2500\u2500 Release\n    \u2502   \u2514\u2500\u2500 main/\n    \u2502       \u251c\u2500\u2500 binary-amd64/\n    \u2502       \u2502   \u251c\u2500\u2500 Packages\n    \u2502       \u2502   \u2514\u2500\u2500 Packages.gz\n    \u2502       \u2514\u2500\u2500 binary-arm64/\n    \u251c\u2500\u2500 noble/                 # Ubuntu 24.04\n    \u251c\u2500\u2500 bookworm/              # Debian 12\n    \u2514\u2500\u2500 trixie/                # Debian 13\n</code></pre>"},{"location":"user-guide/package-repositories/#manual-deb-installation","title":"Manual DEB Installation","text":"<p>Download directly from GitHub Releases:</p> <pre><code># Find the latest release\ncurl -s https://api.github.com/repos/SckyzO/monitoring-hub/releases | \\\n  jq -r '.[0].assets[] | select(.name | contains(\"node-exporter\")) | .browser_download_url'\n\n# Install directly\nwget https://github.com/SckyzO/monitoring-hub/releases/download/node_exporter-v1.8.0/node-exporter_1.8.0-1_amd64.deb\nsudo dpkg -i node-exporter_1.8.0-1_amd64.deb\nsudo apt-get install -f  # Fix dependencies if needed\n</code></pre>"},{"location":"user-guide/package-repositories/#container-registry-oci","title":"Container Registry (OCI)","text":""},{"location":"user-guide/package-repositories/#pull-image","title":"Pull Image","text":"<pre><code># Latest version\ndocker pull ghcr.io/sckyzo/monitoring-hub/node_exporter:latest\n\n# Specific version\ndocker pull ghcr.io/sckyzo/monitoring-hub/node_exporter:1.8.0\n\n# Specific architecture\ndocker pull --platform linux/amd64 ghcr.io/sckyzo/monitoring-hub/node_exporter:latest\ndocker pull --platform linux/arm64 ghcr.io/sckyzo/monitoring-hub/node_exporter:latest\n</code></pre>"},{"location":"user-guide/package-repositories/#run-container","title":"Run Container","text":"<pre><code>docker run -d \\\n  --name node_exporter \\\n  --restart unless-stopped \\\n  -p 9100:9100 \\\n  ghcr.io/sckyzo/monitoring-hub/node_exporter:latest\n</code></pre>"},{"location":"user-guide/package-repositories/#docker-compose","title":"Docker Compose","text":"<pre><code>version: '3.8'\n\nservices:\n  node_exporter:\n    image: ghcr.io/sckyzo/monitoring-hub/node_exporter:latest\n    container_name: node_exporter\n    restart: unless-stopped\n    ports:\n      - \"9100:9100\"\n    networks:\n      - monitoring\n\n  blackbox_exporter:\n    image: ghcr.io/sckyzo/monitoring-hub/blackbox_exporter:latest\n    container_name: blackbox_exporter\n    restart: unless-stopped\n    ports:\n      - \"9115:9115\"\n    volumes:\n      - ./blackbox.yml:/etc/blackbox_exporter/config.yml:ro\n    networks:\n      - monitoring\n\nnetworks:\n  monitoring:\n    driver: bridge\n</code></pre>"},{"location":"user-guide/package-repositories/#multi-arch-support","title":"Multi-Arch Support","text":"<p>All container images are published as multi-arch manifests supporting both amd64 and arm64:</p> <pre><code># Inspect manifest\ndocker manifest inspect ghcr.io/sckyzo/monitoring-hub/node_exporter:latest\n\n# Docker automatically pulls the correct architecture\ndocker pull ghcr.io/sckyzo/monitoring-hub/node_exporter:latest\n</code></pre>"},{"location":"user-guide/package-repositories/#version-pinning","title":"Version Pinning","text":""},{"location":"user-guide/package-repositories/#rpm","title":"RPM","text":"<pre><code># List available versions\ndnf list --showduplicates node_exporter\n\n# Install specific version\nsudo dnf install node_exporter-1.8.0\n</code></pre>"},{"location":"user-guide/package-repositories/#deb","title":"DEB","text":"<pre><code># List available versions\napt-cache madison node-exporter\n\n# Install specific version\nsudo apt install node-exporter=1.8.0-1\n</code></pre>"},{"location":"user-guide/package-repositories/#container","title":"Container","text":"<pre><code># Always use version tags in production\ndocker pull ghcr.io/sckyzo/monitoring-hub/node_exporter:1.8.0\n\n# Avoid :latest in production\n</code></pre>"},{"location":"user-guide/package-repositories/#repository-metadata","title":"Repository Metadata","text":"<p>All repositories are updated automatically on every release. The catalog is available at:</p> <pre><code>https://sckyzo.github.io/monitoring-hub/catalog.json\n</code></pre> <p>This JSON file contains:</p> <ul> <li>Exporter metadata (name, version, description)</li> <li>Package availability per distribution and architecture</li> <li>Build dates and statuses</li> <li>Download URLs</li> </ul>"},{"location":"user-guide/package-repositories/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/package-repositories/#yum-gpg-check-failed","title":"YUM: GPG Check Failed","text":"<pre><code># Temporarily disable GPG check\nsudo dnf install --nogpgcheck node_exporter\n\n# Or disable GPG check permanently (not recommended)\necho \"gpgcheck=0\" | sudo tee -a /etc/yum.repos.d/monitoring-hub.repo\n</code></pre>"},{"location":"user-guide/package-repositories/#apt-gpg-error","title":"APT: GPG Error","text":"<pre><code># Re-add the GPG key\ncurl -fsSL https://sckyzo.github.io/monitoring-hub/apt/monitoring-hub.asc | \\\n  sudo gpg --dearmor -o /usr/share/keyrings/monitoring-hub.gpg\n\n# Update package lists\nsudo apt update\n</code></pre>"},{"location":"user-guide/package-repositories/#package-not-found","title":"Package Not Found","text":"<p>Check the Portal to see if the package is available for your distribution and architecture.</p>"},{"location":"user-guide/package-repositories/#container-pull-failed","title":"Container Pull Failed","text":"<pre><code># Check if you're authenticated (for private registries)\ndocker login ghcr.io\n\n# Verify image exists\ncurl -s https://ghcr.io/v2/sckyzo/monitoring-hub/node_exporter/tags/list | jq\n</code></pre>"},{"location":"user-guide/package-repositories/#next-steps","title":"Next Steps","text":"<ul> <li>Adding Exporters - Learn how to add new exporters</li> <li>Manifest Reference - Understand the manifest schema</li> <li>Troubleshooting - Common issues and solutions</li> </ul>"},{"location":"user-guide/troubleshooting/","title":"Troubleshooting","text":"<p>Common issues and solutions when working with Monitoring Hub.</p>"},{"location":"user-guide/troubleshooting/#installation-issues","title":"Installation Issues","text":""},{"location":"user-guide/troubleshooting/#dnf-repository-not-found","title":"DNF Repository Not Found","text":"<p>Problem: <code>Error: Failed to download metadata for repo</code></p> <p>Solution:</p> <pre><code># Clear DNF cache\nsudo dnf clean all\n\n# Reconfigure repo\nsudo dnf config-manager --add-repo https://sckyzo.github.io/monitoring-hub/el9/$(arch)/\n</code></pre>"},{"location":"user-guide/troubleshooting/#gpg-signature-verification-failed","title":"GPG Signature Verification Failed","text":"<p>Problem: Packages fail signature verification</p> <p>Solution: Packages are not signed. Disable GPG check:</p> <pre><code>sudo dnf install --nogpgcheck node_exporter\n</code></pre>"},{"location":"user-guide/troubleshooting/#build-issues","title":"Build Issues","text":""},{"location":"user-guide/troubleshooting/#binary-not-found-in-archive","title":"Binary Not Found in Archive","text":"<p>Problem: <code>Warning: Binary 'my_exporter' not found</code></p> <p>Solution: Check the archive structure and update <code>archive_name</code> pattern:</p> <pre><code># Check actual archive structure\ncurl -L https://github.com/owner/repo/releases/download/v1.0.0/archive.tar.gz | tar -tzf - | head\n</code></pre>"},{"location":"user-guide/troubleshooting/#version-mismatch","title":"Version Mismatch","text":"<p>Problem: Wrong version is downloaded</p> <p>Solution: Update manifest version and ensure it matches upstream tag format (with or without <code>v</code> prefix).</p>"},{"location":"user-guide/troubleshooting/#architecture-not-available","title":"Architecture Not Available","text":"<p>Problem: Build fails for ARM64</p> <p>Solution: Check if upstream provides ARM64 binaries. Update manifest <code>archs</code> if needed.</p>"},{"location":"user-guide/troubleshooting/#runtime-issues","title":"Runtime Issues","text":""},{"location":"user-guide/troubleshooting/#service-wont-start","title":"Service Won't Start","text":"<p>Problem: <code>systemctl start my_exporter</code> fails</p> <p>Solution:</p> <pre><code># Check service status\nsudo systemctl status my_exporter\n\n# Check logs\nsudo journalctl -u my_exporter -n 50\n\n# Test binary manually\nsudo -u my_exporter /usr/bin/my_exporter --help\n</code></pre>"},{"location":"user-guide/troubleshooting/#port-already-in-use","title":"Port Already in Use","text":"<p>Problem: Exporter can't bind to port</p> <p>Solution:</p> <pre><code># Find what's using the port\nsudo lsof -i :9100\n\n# Change port in config or systemd override\nsudo systemctl edit my_exporter\n</code></pre>"},{"location":"user-guide/troubleshooting/#permission-issues","title":"Permission Issues","text":"<p>Problem: Exporter can't read/write files</p> <p>Solution:</p> <pre><code># Check file permissions\nls -la /etc/my_exporter/\nls -la /var/lib/my_exporter/\n\n# Fix ownership\nsudo chown -R my_exporter:my_exporter /var/lib/my_exporter/\n</code></pre>"},{"location":"user-guide/troubleshooting/#docker-issues","title":"Docker Issues","text":""},{"location":"user-guide/troubleshooting/#image-pull-failed","title":"Image Pull Failed","text":"<p>Problem: Can't pull from GHCR</p> <p>Solution:</p> <pre><code># Ensure you're authenticated\ndocker login ghcr.io\n\n# Or use public access (no auth needed for public images)\ndocker pull ghcr.io/sckyzo/monitoring-hub/node_exporter:latest\n</code></pre>"},{"location":"user-guide/troubleshooting/#container-exits-immediately","title":"Container Exits Immediately","text":"<p>Problem: Container starts then stops</p> <p>Solution:</p> <pre><code># Check container logs\ndocker logs my_exporter\n\n# Run interactively for debugging\ndocker run -it --entrypoint /bin/sh ghcr.io/sckyzo/monitoring-hub/my_exporter:latest\n</code></pre>"},{"location":"user-guide/troubleshooting/#development-issues","title":"Development Issues","text":""},{"location":"user-guide/troubleshooting/#tests-failing","title":"Tests Failing","text":"<p>Problem: <code>pytest</code> fails</p> <p>Solution:</p> <pre><code># Ensure PYTHONPATH is set\nexport PYTHONPATH=$(pwd)\n\n# Run with verbose output\npytest -vv\n\n# Run specific test\npytest core/tests/test_builder.py::test_load_valid_manifest\n</code></pre>"},{"location":"user-guide/troubleshooting/#pre-commit-hooks-failing","title":"Pre-commit Hooks Failing","text":"<p>Problem: <code>pre-commit run</code> fails</p> <p>Solution:</p> <pre><code># Update hooks\npre-commit autoupdate\n\n# Run on all files\npre-commit run --all-files\n\n# Skip hooks if needed (not recommended)\ngit commit --no-verify\n</code></pre>"},{"location":"user-guide/troubleshooting/#getting-help","title":"Getting Help","text":"<p>If your issue isn't covered here:</p> <ol> <li>Check GitHub Issues</li> <li>Review GitHub Actions logs</li> <li>Open a new issue with details</li> </ol>"}]}